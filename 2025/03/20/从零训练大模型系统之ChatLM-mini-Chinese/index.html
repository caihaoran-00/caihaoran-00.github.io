<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言前面看了Qwen2.5 VL的技术报告，但感觉还是没有个具象的认识，遂打算从零训练个大模型，一般认为1B及以上参数量才是大模型，由于我只有一张4090，而且仅是为了对全流程有一个具象的认识，遂选择ChatLM-mini-Chinese模型，该模型仅有0.2 B参数量，本文记录我的实操过程，尽量记录详细并给出自己的思考。 项目地址：https:&#x2F;&#x2F;github.com&#x2F;charent&#x2F;ChatL">
<meta property="og:type" content="article">
<meta property="og:title" content="从零训练大模型系列之ChatLM-mini-Chinese">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言前面看了Qwen2.5 VL的技术报告，但感觉还是没有个具象的认识，遂打算从零训练个大模型，一般认为1B及以上参数量才是大模型，由于我只有一张4090，而且仅是为了对全流程有一个具象的认识，遂选择ChatLM-mini-Chinese模型，该模型仅有0.2 B参数量，本文记录我的实操过程，尽量记录详细并给出自己的思考。 项目地址：https:&#x2F;&#x2F;github.com&#x2F;charent&#x2F;ChatL">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/fa7def4c95ae9b52114c9913be5af16.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/33f81f22284ae303577bc3f96d8f8ce.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/9f1e7ddf11ec364329f95d8a4946541.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/02936efbc49baa2164c16c2041a6482.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/581aa414b4e49cd72090ca0f8047842.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/56990b8a32e9c9d383bd24529cbb991.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/5adfc70a140c401c06ac7fa4f7aa95f.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/79528face960a3e59d52c9934c5caf3.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/018ade7a8ad04e38063ce00ac67844a.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/96ec2e98b0f075e87284581846a2a11.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/ff07f320532d665cb0891e865eb005a.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/aa9445b8e38fdec2f0f2167921cb7f3.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/758a746fde78ede686731711c97f781.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250321202529850.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/b47fb1d38c4863fbb8fbf699b330640.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/6a6bcabed9b7bb65acae8b98a741880.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/06e8c7567031d208f2dda383b1eb9f0.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/c0fe0ead2ee03428a0c2b49943b68a4.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/2f7c0d35b027b359fecfd6f7a185996.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/4d0a28c8a61a6cf72a8bd04e7e8c45a.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250325101716479.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/2d9dd25e5279a6444be89dd6ae9db4e.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/42e7638999c83740bea73557c23c9f2.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/af3b357acf806a9fd91bf83ee450432.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326110851718.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/c3a38a8e6ce3e077de06a9411d56881.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250325193813523.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/6323398b3316d8f21637256586767e8.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/4c827d26cc2f531350a0c46f16fe5ea.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/d42712e2f723e6b3d9fd87ede8e40ef.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326142917174.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/276b78ff698e642d99babdbf9ad95e1.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/99b30048901aa654d190e49846588d6.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326152851163.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/04733e147854cb62395463940ce8326.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/11142dca3a425570c53ff7d93c52ba2.png">
<meta property="article:published_time" content="2025-03-20T02:37:10.000Z">
<meta property="article:modified_time" content="2025-04-17T10:06:58.116Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="SFT">
<meta property="article:tag" content="ChatLM-mini-Chinese">
<meta property="article:tag" content="Pre-train">
<meta property="article:tag" content="DPO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/fa7def4c95ae9b52114c9913be5af16.png">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/","path":"2025/03/20/从零训练大模型系统之ChatLM-mini-Chinese/","title":"从零训练大模型系列之ChatLM-mini-Chinese"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从零训练大模型系列之ChatLM-mini-Chinese | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"><span class="nav-number">2.</span> <span class="nav-text">环境准备</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tokenizer%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">Tokenizer训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">3.1.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">3.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95"><span class="nav-number">3.3.</span> <span class="nav-text">简单测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">4.</span> <span class="nav-text">预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD"><span class="nav-number">4.1.</span> <span class="nav-text">预训练数据集下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B8%85%E6%B4%97%E6%95%B0%E6%8D%AE"><span class="nav-number">4.2.</span> <span class="nav-text">清洗数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%88%E5%8D%95%E6%9C%BA%E5%8D%95%E5%8D%A1%EF%BC%89"><span class="nav-number">4.3.</span> <span class="nav-text">预训练（单机单卡）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%95%E9%AA%8C%E4%B8%80%E4%B8%8B"><span class="nav-number">4.4.</span> <span class="nav-text">试验一下</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SFT%E5%BE%AE%E8%B0%83"><span class="nav-number">5.</span> <span class="nav-text">SFT微调</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">5.1.</span> <span class="nav-text">下载数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B8%85%E6%B4%97%E6%95%B0%E6%8D%AE-1"><span class="nav-number">5.2.</span> <span class="nav-text">清洗数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%95%E9%AA%8C%E4%B8%80%E4%B8%8B-1"><span class="nav-number">5.3.</span> <span class="nav-text">试验一下</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RLHF-DPO"><span class="nav-number">6.</span> <span class="nav-text">RLHF -&gt; DPO</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD%E5%8F%8A%E5%88%B6%E4%BD%9C"><span class="nav-number">6.1.</span> <span class="nav-text">数据集下载及制作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DPO%E8%AE%AD%E7%BB%83"><span class="nav-number">6.2.</span> <span class="nav-text">DPO训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%95%E9%AA%8C%E4%B8%80%E4%B8%8B-2"><span class="nav-number">6.3.</span> <span class="nav-text">试验一下</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83-%E4%B8%89%E5%85%83%E7%BB%84%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96"><span class="nav-number">7.</span> <span class="nav-text">下游任务微调-三元组信息抽取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">8.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#token-type"><span class="nav-number">8.1.</span> <span class="nav-text">token_type</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dpo%E6%97%A5%E5%BF%97"><span class="nav-number">8.2.</span> <span class="nav-text">dpo日志</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">9.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">90</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="从零训练大模型系列之ChatLM-mini-Chinese | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从零训练大模型系列之ChatLM-mini-Chinese
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-20 10:37:10" itemprop="dateCreated datePublished" datetime="2025-03-20T10:37:10+08:00">2025-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-17 18:06:58" itemprop="dateModified" datetime="2025-04-17T18:06:58+08:00">2025-04-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/code/" itemprop="url" rel="index"><span itemprop="name">code</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面看了Qwen2.5 VL的技术报告，但感觉还是没有个具象的认识，遂打算从零训练个大模型，一般认为1B及以上参数量才是大模型，由于我只有一张4090，而且仅是为了对全流程有一个具象的认识，遂选择ChatLM-mini-Chinese模型，该模型仅有0.2 B参数量，本文记录我的实操过程，尽量记录详细并给出自己的思考。</p>
<p><strong>项目地址：<a target="_blank" rel="noopener" href="https://github.com/charent/ChatLM-mini-Chinese">https://github.com/charent/ChatLM-mini-Chinese</a></strong></p>
<span id="more"></span>

<hr>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth <span class="number">1</span> https://github.com/charent/ChatLM-mini-Chinese.git</span><br><span class="line"></span><br><span class="line">cd ChatLM-mini-Chinese</span><br><span class="line"></span><br><span class="line">conda create -n chatlm_mini_chinese -y python=<span class="number">3.10</span></span><br><span class="line"></span><br><span class="line">conda activate chatlm_mini_chinese</span><br><span class="line"></span><br><span class="line">pip install -r ./requirements.txt</span><br><span class="line"></span><br><span class="line">(可选，查看torch是否支持cuda, 我的输出是<span class="number">2.11</span>+cu121就是支持cuda)：</span><br><span class="line">python -c <span class="string">&quot;import torch;print(torch.__version__)&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="Tokenizer训练"><a href="#Tokenizer训练" class="headerlink" title="Tokenizer训练"></a>Tokenizer训练</h2><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p><a target="_blank" rel="noopener" href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles-multistream.xml.bz2">点击这里</a>下载维基百科中文语料（3.2G），下载完成后，回到桌面（去你管理程序库的地方）打开个终端，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/attardi/wikiextractor</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> wikiextractor</span><br></pre></td></tr></table></figure>

<p>将下载的维基百科数据<code>zhwiki-latest-pages-articles-multistream.xml.bz2</code>放在该文件夹下，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m wikiextractor.WikiExtractor -b 1000M -o zhwik zhwiki-latest-pages-articles-multistream.xml.bz2 </span><br></pre></td></tr></table></figure>

<p>耗时：14:28~15:18</p>
<p>或者：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m wikiextractor.WikiExtractor -b 1000M -o zhwik1 zhwiki-latest-pages-articles-multistream.xml.bz2 --no-templates</span><br></pre></td></tr></table></figure>

<p>耗时：15:20~15:26</p>
<p><strong>不同点：</strong></p>
<p>不加<code>--no-templates</code></p>
<p>&lt;doc id&#x3D;”13” url&#x3D;”<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki?curid=13">https://zh.wikipedia.org/wiki?curid=13</a>“ title&#x3D;”数学”&gt;<br>数学</p>
<p>&amp;lt;templatestyles src&#x3D;”Ambox&#x2F;style.css” &#x2F;&amp;gt;<br>&amp;lt;templatestyles src&#x3D;”Ambox&#x2F;style.css” &#x2F;&amp;gt;<br>数学是研究數量、结构…</p>
<p>参考书目.<br>&amp;lt;templatestyles src&#x3D;”Template:ReflistH&#x2F;styles.css” &#x2F;&amp;gt;</p>
<p>&lt;&#x2F;doc&gt;</p>
<p>加<code>--no-templates</code></p>
<p>&lt;doc id&#x3D;”13” url&#x3D;”<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki?curid=13">https://zh.wikipedia.org/wiki?curid=13</a>“ title&#x3D;”数学”&gt;<br>数学</p>
<p>数学是研究數量、结构…</p>
<p>&lt;&#x2F;doc&gt;</p>
<p>进入到你的提取好的wiki文件的文件夹，这些文件<code>wiki_00、wiki_01、wiki_02</code>内容是繁体的，要先转化为繁体，使用opencc，终端运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install opencc</span><br><span class="line"></span><br><span class="line">opencc -i wiki_00 -o wiki0.txt -c t2s.json</span><br></pre></td></tr></table></figure>

<p>等待片刻完成后，将<code>wiki0.txt</code>复制到<code>ChatLM-mini-Chinese/data</code>文件夹下，打开<code>train_tokenizer.py</code>，将151行改成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropus_file = PROJECT_ROOT + <span class="string">&#x27;/data/wiki0.txt&#x27;</span></span><br></pre></td></tr></table></figure>

<p>这是我未运行训练命令时的内存占用：</p>
<p>![2a06d299a89475b93bed10e77a714db](..&#x2F;..&#x2F;..&#x2F;..&#x2F;WeChat Files&#x2F;wxid_noknx8x7zbmf22&#x2F;FileStorage&#x2F;Temp&#x2F;2a06d299a89475b93bed10e77a714db.png)</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>好的，在你的conda环境下开炮试下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_tokenizer.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/fa7def4c95ae9b52114c9913be5af16.png" class="" title="fa7def4c95ae9b52114c9913be5af16">

<p>不行，运行不起来，把能关的都关上，只留三个终端（此时显存占用3.2G左右）：</p>
<ul>
<li>显存<code>watch -n 1 gpustat</code></li>
<li>内存<code>htop</code></li>
<li>运行训练代码<code>python train_tokenizer.py</code></li>
</ul>
<p>好的，运行起来了，但是也卡住了😅，口头描述一下吧：</p>
<ul>
<li>显存占用未增加</li>
<li>内存占用30 G</li>
<li>运行五分钟后还是被强制停止了</li>
</ul>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/33f81f22284ae303577bc3f96d8f8ce.png" class="" title="33f81f22284ae303577bc3f96d8f8ce">

<p>再想想办法，把Swp缓冲区释放了试试（但是发现虽然Swp清零了，但是这部分转移到Mem了）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> swapoff -a</span><br><span class="line"><span class="built_in">sudo</span> swapon -a</span><br></pre></td></tr></table></figure>

<p>开炮！好的，Mem从<code>5.20G</code>一直升高，升高到<code>28G</code>的样子升高<code>Swp</code>，<code>Swp</code>升满后，<code>Mem</code>升高至30.4G，就又卡住了(16:42)，等一会吧，(17:07)又被嘎了，耗时25分钟，没法了，得改下代码咯（不！就不改！），我要扩充Swp的容量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">free -m  <span class="comment"># 查看当前系统的swap大小</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> swapoff -a <span class="comment"># 关闭swap</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/swapfile bs=1G count=32  <span class="comment"># 创建新的swap文件，bs*count=swap大小，这里直接设置32G</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> 600 /swapfile <span class="comment"># 设置权限</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> mkswap /swapfile  <span class="comment"># 设置swap</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sudo</span> swapon /swapfile  <span class="comment"># 启用swap</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;/swapfile none swap sw 0 0&quot;</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> -a /etc/fstab  <span class="comment"># 使Swap文件永久生效（重启试了，确实有效）</span></span><br></pre></td></tr></table></figure>

<p>然后，再：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train_tokenizer.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/9f1e7ddf11ec364329f95d8a4946541.png" class="" title="9f1e7ddf11ec364329f95d8a4946541">

<h3 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h3><p>成了！并且也很快，反正不要半小时，简单的体验下这是个什么：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你的 tokenizer 存放路径</span></span><br><span class="line">tokenizer_path = <span class="string">&quot;/home/chr/桌面/ChatLM-mini-Chinese/model_save/hf_tokenizer&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.直接加载测试Tokenizer</span></span><br><span class="line"><span class="comment"># 测试分词效果</span></span><br><span class="line">text = <span class="string">&quot;你好，欢迎使用 Tokenizer！&quot;</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原始文本:&quot;</span>, text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分词结果:&quot;</span>, tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Token ID:&quot;</span>, ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.使用tokenizer.encoder()和tokenizer.decoder()</span></span><br><span class="line">encoded = tokenizer.encode(text)</span><br><span class="line">decoded = tokenizer.decode(encoded)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;编码结果:&quot;</span>, encoded)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;解码结果:&quot;</span>, decoded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.批量测试不同类型文本</span></span><br><span class="line">test_cases = [</span><br><span class="line">    <span class="string">&quot;你好，我叫小明。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;AI 发展迅猛。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;今天的天气怎么样？&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ChatGPT 是一个大型语言模型。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;BERT, GPT-3 和 LLaMA 是 NLP 领域的重要模型。&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> test_cases:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;原始文本: <span class="subst">&#123;text&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;分词结果: <span class="subst">&#123;tokenizer.tokenize(text)&#125;</span>\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.统计词表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词表大小:&quot;</span>, tokenizer.vocab_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.统计不同文本的平均Token长度</span></span><br><span class="line">texts = [<span class="string">&quot;今天天气很好，我们去公园玩吧！&quot;</span>,</span><br><span class="line">         <span class="string">&quot;Transformer 是一种基于自注意力机制的神经网络模型。&quot;</span>,</span><br><span class="line">         <span class="string">&quot;BERT 和 GPT 是 NLP 领域的重要突破。&quot;</span>,</span><br><span class="line">         <span class="string">&quot;自然语言处理（NLP）是人工智能的一个分支。&quot;</span>,</span><br><span class="line">         <span class="string">&quot;这是一条很短的句子。&quot;</span>]</span><br><span class="line"></span><br><span class="line">lengths = [<span class="built_in">len</span>(tokenizer.tokenize(text)) <span class="keyword">for</span> text <span class="keyword">in</span> texts]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;平均 Token 长度:&quot;</span>, <span class="built_in">sum</span>(lengths) / <span class="built_in">len</span>(lengths))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;单个文本 Token 数:&quot;</span>, lengths)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/home/<span class="built_in">chr</span>/anaconda3/envs/chatlm_mini_chinese/<span class="built_in">bin</span>/python /home/<span class="built_in">chr</span>/桌面/ChatLM-mini-Chinese/val_tokenizer.py </span><br><span class="line">原始文本: 你好，欢迎使用 Tokenizer！</span><br><span class="line">分词结果: [<span class="string">&#x27;▁你&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;▁,&#x27;</span>, <span class="string">&#x27;▁&#x27;</span>, <span class="string">&#x27;欢迎&#x27;</span>, <span class="string">&#x27;使用&#x27;</span>, <span class="string">&#x27;▁T&#x27;</span>, <span class="string">&#x27;ok&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;iz&#x27;</span>, <span class="string">&#x27;er&#x27;</span>, <span class="string">&#x27;▁!&#x27;</span>]</span><br><span class="line">Token ID: [<span class="number">21418</span>, <span class="number">6386</span>, <span class="number">19161</span>, <span class="number">2986</span>, <span class="number">20848</span>, <span class="number">19253</span>, <span class="number">19318</span>, <span class="number">21700</span>, <span class="number">19311</span>, <span class="number">24092</span>, <span class="number">19248</span>, <span class="number">19703</span>]</span><br><span class="line">编码结果: [<span class="number">21418</span>, <span class="number">6386</span>, <span class="number">19161</span>, <span class="number">2986</span>, <span class="number">20848</span>, <span class="number">19253</span>, <span class="number">19318</span>, <span class="number">21700</span>, <span class="number">19311</span>, <span class="number">24092</span>, <span class="number">19248</span>, <span class="number">19703</span>]</span><br><span class="line">解码结果: 你好, 欢迎使用 Tokenizer!</span><br><span class="line">原始文本: 你好，我叫小明。</span><br><span class="line">分词结果: [<span class="string">&#x27;▁你&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;▁,&#x27;</span>, <span class="string">&#x27;▁我&#x27;</span>, <span class="string">&#x27;叫&#x27;</span>, <span class="string">&#x27;小&#x27;</span>, <span class="string">&#x27;明&#x27;</span>, <span class="string">&#x27;▁。&#x27;</span>]</span><br><span class="line"></span><br><span class="line">原始文本: AI 发展迅猛。</span><br><span class="line">分词结果: [<span class="string">&#x27;▁A&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;▁发展&#x27;</span>, <span class="string">&#x27;迅&#x27;</span>, <span class="string">&#x27;猛&#x27;</span>, <span class="string">&#x27;▁。&#x27;</span>]</span><br><span class="line"></span><br><span class="line">原始文本: 今天的天气怎么样？</span><br><span class="line">分词结果: [<span class="string">&#x27;▁今天&#x27;</span>, <span class="string">&#x27;的天&#x27;</span>, <span class="string">&#x27;气&#x27;</span>, <span class="string">&#x27;怎么&#x27;</span>, <span class="string">&#x27;样&#x27;</span>, <span class="string">&#x27;▁?&#x27;</span>]</span><br><span class="line"></span><br><span class="line">原始文本: ChatGPT 是一个大型语言模型。</span><br><span class="line">分词结果: [<span class="string">&#x27;▁Ch&#x27;</span>, <span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;GP&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;▁是一个&#x27;</span>, <span class="string">&#x27;大型&#x27;</span>, <span class="string">&#x27;语言&#x27;</span>, <span class="string">&#x27;模型&#x27;</span>, <span class="string">&#x27;▁。&#x27;</span>]</span><br><span class="line"></span><br><span class="line">原始文本: BERT, GPT-<span class="number">3</span> 和 LLaMA 是 NLP 领域的重要模型。</span><br><span class="line">分词结果: [<span class="string">&#x27;▁B&#x27;</span>, <span class="string">&#x27;ER&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;▁,&#x27;</span>, <span class="string">&#x27;▁G&#x27;</span>, <span class="string">&#x27;P&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;▁-&#x27;</span>, <span class="string">&#x27;▁3&#x27;</span>, <span class="string">&#x27;▁和&#x27;</span>, <span class="string">&#x27;▁L&#x27;</span>, <span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;MA&#x27;</span>, <span class="string">&#x27;▁是&#x27;</span>, <span class="string">&#x27;▁N&#x27;</span>, <span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;P&#x27;</span>, <span class="string">&#x27;▁&#x27;</span>, <span class="string">&#x27;领域&#x27;</span>, <span class="string">&#x27;的重要&#x27;</span>, <span class="string">&#x27;模型&#x27;</span>, <span class="string">&#x27;▁。&#x27;</span>]</span><br><span class="line"></span><br><span class="line">词表大小: <span class="number">40960</span></span><br><span class="line">平均 Token 长度: <span class="number">11.8</span></span><br><span class="line">单个文本 Token 数: [<span class="number">10</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">14</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><h3 id="预训练数据集下载"><a href="#预训练数据集下载" class="headerlink" title="预训练数据集下载"></a>预训练数据集下载</h3><ol>
<li><p><a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v">社区问答json版(webtext2019zh) 大规模高质量数据集</a></p>
<p>将下载完成的<code>webtext2019zh.zip</code>放到<code>data/raw_data</code>下（没有就创建），并运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip webtext2019zh.zip</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意，解压得到三个文件<code>web_text_zh_testa.json</code>（我没写错）、<code>web_text_zh_train.json</code>和<code>web_text_zh_valid.json</code>，需要把<code>web_text_zh_testa.json</code>改为<code>web_text_zh_test.json</code>。</p>
</blockquote>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/terrychanorg/baike-qa2019?resource=download&amp;select=baike_qa_train.json">baike_qa2019百科问答类</a></p>
<p>操作与1类似，不再赘述</p>
</li>
<li><p>中国医药领域问答数据集</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入data/raw_data目录</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Toyhom/Chinese-medical-dialogue-data.git</span><br></pre></td></tr></table></figure>

<blockquote>
<p>克隆完成后，将文件夹名称改为chinese_medical_dialogue_datasets</p>
</blockquote>
</li>
<li><p>知乎问答数据</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也是在data/raw_data目录</span></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/wangrui6/Zhihu-KOL</span><br></pre></td></tr></table></figure>

<blockquote>
<p>克隆完成后，将文件夹名称改为zhihu-kol。</p>
</blockquote>
</li>
</ol>
<h3 id="清洗数据"><a href="#清洗数据" class="headerlink" title="清洗数据"></a>清洗数据</h3><p>清洗数据的脚本位于<code>utils/raw_data_process.py</code>，在运行之前先略微配置下代码，将<code>process_web_text</code>下面的代码全注释掉：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/02936efbc49baa2164c16c2041a6482.png" class="" title="02936efbc49baa2164c16c2041a6482">

<p>conda环境下运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python raw_data_process.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/581aa414b4e49cd72090ca0f8047842.png" class="" title="581aa414b4e49cd72090ca0f8047842">

<p>而且<code>logs/raw_data_process.log</code>中也会有调试信息：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/56990b8a32e9c9d383bd24529cbb991.png" class="" title="56990b8a32e9c9d383bd24529cbb991">

<p>好的，继续，注释掉<code>process_web_text</code>，打开<code>process_bake_qa</code>，<code>process_chinese_medical_datasets</code>，<code>process_zhihu_kol_dataset</code>：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/5adfc70a140c401c06ac7fa4f7aa95f.png" class="" title="5adfc70a140c401c06ac7fa4f7aa95f">

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/79528face960a3e59d52c9934c5caf3.png" class="" title="79528face960a3e59d52c9934c5caf3">

<p>还差一些步骤（先把原来步骤注释掉），如图：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/018ade7a8ad04e38063ce00ac67844a.png" class="" title="018ade7a8ad04e38063ce00ac67844a">

<p>然后继续：<code>python raw_data_process.py</code>，经过漫长的等待：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(chatlm_mini_chinese) chr@chr:~/桌面/ChatLM-mini-Chinese/utils$ python raw_data_process.py </span><br><span class="line">process file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_data/my_web_text_zh.parquet</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:58</span><br><span class="line">process file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_data/my_baike_qa.parquet</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:32</span><br><span class="line">process file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_data/zhihu_kol.parquet</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:34</span><br><span class="line">process file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_data/my_chinese_medical_dialogue.parquet</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:15</span><br><span class="line">[2025-03-21 16:08:24.638] [INFO]: merge into file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_dataset.parquet, 全部数据共5718789行，清洗后剩余5718644行</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 2:34:53</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:02:30</span><br><span class="line">[2025-03-21 18:45:51.019] [INFO]: merge into file: /home/chr/桌面/ChatLM-mini-Chinese/data/my_dataset_no_dulpticates.parquet, 全部数据共5718644行，文档去重后剩余5519322行</span><br><span class="line">start shuffle...</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:02:34</span><br><span class="line">[2025-03-21 18:50:53.355] [INFO]: [[<span class="string">&#x27;file_name&#x27;</span>, <span class="string">&#x27;count&#x27;</span>], [<span class="string">&#x27;my_dataset.parquet&#x27;</span>, 5718644], [<span class="string">&#x27;汇总&#x27;</span>, 5718644]]</span><br><span class="line">┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓</span><br><span class="line">┃ file_name          ┃ count┃</span><br><span class="line">┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩</span><br><span class="line">│ my_dataset.parquet │ 5718644 │</span><br><span class="line">├────────────────────┼─────────┤</span><br><span class="line">│ 汇总               │ 5718644 │</span><br><span class="line">└────────────────────┴─────────┘</span><br><span class="line">Working... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:08</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/96ec2e98b0f075e87284581846a2a11.png" class="" title="96ec2e98b0f075e87284581846a2a11">

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/ff07f320532d665cb0891e865eb005a.png" class="" title="ff07f320532d665cb0891e865eb005a">

<p>但是呢，进度在这里不动了，而且图已经画出来了（画图是最后一步），</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/aa9445b8e38fdec2f0f2167921cb7f3.png" class="" title="aa9445b8e38fdec2f0f2167921cb7f3">

<p>而且内存占用也没释放，点回车也没用😅，怎么办怎么办，不管了，应该是运行完了，直接ctrl+c给关掉。</p>
<h3 id="预训练（单机单卡）"><a href="#预训练（单机单卡）" class="headerlink" title="预训练（单机单卡）"></a>预训练（单机单卡）</h3><p>好的，开炮！</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/758a746fde78ede686731711c97f781.png" class="" title="758a746fde78ede686731711c97f781">

<p>哦，说在<code>model_save</code>文件夹下找不到训练好的<code>Tokenizer</code>，前面明明是保存到<code>model_save/hf_tokenizer</code>下了，在<code>model_save</code>下哪能找到，打开<code>config.py</code>，箭头位置改一下：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250321202529850.png" class="" title="image-20250321202529850">

<p>开炮！</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/b47fb1d38c4863fbb8fbf699b330640.png" class="" title="b47fb1d38c4863fbb8fbf699b330640">

<p>显存、内存占用：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/6a6bcabed9b7bb65acae8b98a741880.png" class="" title="6a6bcabed9b7bb65acae8b98a741880">

<p>好的，等待着吧，现在是25-3-21（20:28），好的，运行完了：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/06e8c7567031d208f2dda383b1eb9f0.png" class="" title="06e8c7567031d208f2dda383b1eb9f0">

<p>看样子用了2 days + 10h，看loss也确实是降低的，去除警告后的完整信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">(chatlm_mini_chinese) chr@chr:~/桌面/ChatLM-mini-Chinese$ accelerate launch ./train.py train</span><br><span class="line">The following values were not passed to `accelerate launch` and had defaults used instead:</span><br><span class="line">	`--num_processes` was <span class="built_in">set</span> to a value of `1`</span><br><span class="line">	`--num_machines` was <span class="built_in">set</span> to a value of `1`</span><br><span class="line">	`--mixed_precision` was <span class="built_in">set</span> to a value of `<span class="string">&#x27;no&#x27;</span>`</span><br><span class="line">	`--dynamo_backend` was <span class="built_in">set</span> to a value of `<span class="string">&#x27;no&#x27;</span>`</span><br><span class="line">To avoid this warning pass <span class="keyword">in</span> values <span class="keyword">for</span> each of the problematic parameters or run `accelerate config`.</span><br><span class="line">[2025-03-21 20:19:54.158] [INFO]: cpu memory available: 24.39 GB, disk space available: 668.51 GB, keep dataset <span class="keyword">in</span> memory: False.</span><br><span class="line">[2025-03-21 20:19:54.158] [INFO]: operation: train, keep training: False, loading datasets ...</span><br><span class="line">[2025-03-21 20:20:04.510] [INFO]: using device: cuda </span><br><span class="line">[2025-03-21 20:20:05.680] [INFO]: train dataset size: 5203576, steps per epoch:325223; validation dataset size: 13994, steps per validation: 874; datalodater num_workers: 0.</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:0, avg_loss:4.247095609019069, cur_bleu4:0.0486591236416422, best_bleu4:0.0486591236416422, best_epoch:0</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:1, avg_loss:3.805832653275791, cur_bleu4:0.06714899049061171, best_bleu4:0.06714899049061171, best_epoch:1</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:2, avg_loss:3.713093634462342, cur_bleu4:0.03836117001107061, best_bleu4:0.06714899049061171, best_epoch:1</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:3, avg_loss:3.658557256075041, cur_bleu4:0.024710511352158505, best_bleu4:0.06714899049061171, best_epoch:1</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:4, avg_loss:3.6181896459202454, cur_bleu4:0.08591752119996698, best_bleu4:0.08591752119996698, best_epoch:4</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:5, avg_loss:3.5855124745777927, cur_bleu4:0.04782050916056753, best_bleu4:0.08591752119996698, best_epoch:4</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:6, avg_loss:3.5662105754453, cur_bleu4:0.09046048314026331, best_bleu4:0.09046048314026331, best_epoch:6</span><br><span class="line"></span><br><span class="line">epoch <span class="built_in">log</span>: epoch:7, avg_loss:3.5456458627353107, cur_bleu4:0.040374996576144455, best_bleu4:0.09046048314026331, best_epoch:6</span><br><span class="line">epoch:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━  88% -:--:-- 2 days, 10:03:54 epoch: 7/8, avg_loss: 3.566211, best_epoch: 6, best_bleu: 0.09046048314026331</span><br><span class="line">steps:     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 6:24:13          step: 325223/325223, loss: 3.221098   </span><br></pre></td></tr></table></figure>

<p>好的，教程的下一步是SFT微调，但是我想这一步先试试效果。</p>
<hr>
<h3 id="试验一下"><a href="#试验一下" class="headerlink" title="试验一下"></a>试验一下</h3><p>打开model_save文件夹，你会看到：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/c0fe0ead2ee03428a0c2b49943b68a4.png" class="" title="c0fe0ead2ee03428a0c2b49943b68a4">

<ul>
<li><p>将<code>hf_tokenizer</code>文件夹下的<code>special_tokens_map.json</code>、<code>tokenizer.json</code>和<code>tokenizer_config.json</code>复制到<code>model_save目录下</code></p>
</li>
<li><p>将<code>train_latest_state</code>文件夹下的<code>model.safetensors</code>复制到<code>model_save</code>目录下</p>
</li>
<li><p>运行下面命令下载一些配置文件和模型文件（用前辈的配置文件）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth <span class="number">1</span> https://www.modelscope.cn/charent/ChatLM-mini-Chinese.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>将上面克隆得到的文件夹中的<code>config.json</code>复制到<code>model_save</code>目录下，并将该文件的最后一行的<code>vocab_size</code>改40961</p>
</li>
</ul>
<p>这时候文件夹下是这样的：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/2f7c0d35b027b359fecfd6f7a185996.png" class="" title="2f7c0d35b027b359fecfd6f7a185996">

<p>运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python cli_demo.py</span><br></pre></td></tr></table></figure>

<p>我的简单测试结果：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/4d0a28c8a61a6cf72a8bd04e7e8c45a.png" class="" title="4d0a28c8a61a6cf72a8bd04e7e8c45a">

<p>好的，这时候是胡言乱语的（我也不确认<code>config.json</code>能不能那样改，但不改运行不起来，先这样吧，继续往下走）。</p>
<hr>
<h2 id="SFT微调"><a href="#SFT微调" class="headerlink" title="SFT微调"></a>SFT微调</h2><h3 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h3><p>去<code>data/raw_data</code>文件夹下创建<code>bell_open_source</code>文件夹，然后运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/BelleGroup/train_0.5M_CN</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/BelleGroup/train_2M_CN</span><br></pre></td></tr></table></figure>

<p>克隆完成后，将各文件夹下的<code>json</code>文件复制到<code>bell_open_source</code>文件夹下</p>
<h3 id="清洗数据-1"><a href="#清洗数据-1" class="headerlink" title="清洗数据"></a>清洗数据</h3><p>回到清洗数据的脚本：<code>utils/raw_data_process.py</code>，&#96;if _<em>name</em>_ &#x3D;&#x3D; ‘_<em>main</em>_’下面的部分除了开头部分和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">process_belle_knowledge_enhanced_dataset_for_finetune(max_len=<span class="number">320</span>, group_cnt=<span class="number">50000</span>)</span><br></pre></td></tr></table></figure>

<p>外，均注释掉，再按下图所示位置改一下：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250325101716479.png" class="" title="image-20250325101716479">

<p>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python raw_data_process.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/2d9dd25e5279a6444be89dd6ae9db4e.png" class="" title="2d9dd25e5279a6444be89dd6ae9db4e">

<p>再将&#96;if _<em>name</em>_ &#x3D;&#x3D; ‘_<em>main</em>_’下面的部分除了开头部分和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parquet_to_json()</span><br></pre></td></tr></table></figure>

<p>外均注释掉（生成sft.json数据供sft微调），运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python raw_data_process.py</span><br></pre></td></tr></table></figure>

<p>好的，现在sft数据有了，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line">pip install tensorboard</span><br><span class="line"></span><br><span class="line">python sft_train.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/42e7638999c83740bea73557c23c9f2.png" class="" title="42e7638999c83740bea73557c23c9f2">

<p>好的，开始SFT了（11:00），完事了，用时7h+44 min：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/af3b357acf806a9fd91bf83ee450432.png" class="" title="af3b357acf806a9fd91bf83ee450432">

<h3 id="试验一下-1"><a href="#试验一下-1" class="headerlink" title="试验一下"></a>试验一下</h3><p>按下图修改一下，使用sft得到的模型进行推理：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326110851718.png" class="" title="image-20250326110851718">

<p>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python cli_demo.py</span><br></pre></td></tr></table></figure>

<p>说的有点像人话了，但重复概率很高：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/c3a38a8e6ce3e077de06a9411d56881.png" class="" title="c3a38a8e6ce3e077de06a9411d56881">

<p>打开<code>model/chat_model.py</code>，将重复词惩罚打开，并设置为1.2：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250325193813523.png" class="" title="image-20250325193813523">

<p><strong>再次试验：</strong></p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/6323398b3316d8f21637256586767e8.png" class="" title="6323398b3316d8f21637256586767e8">

<p>好的，复读机现象没了，<code>1+1</code>也能算对了（神奇），但是关于李白是谁，回答的更离谱了。</p>
<hr>
<h2 id="RLHF-DPO"><a href="#RLHF-DPO" class="headerlink" title="RLHF -&gt; DPO"></a>RLHF -&gt; DPO</h2><p>OKOK，快完事了，前辈用RLHF（强化学习人类反馈优化方法，Reinforcement Learning from Human Feedback）的DPO（直接偏好优化，Direct Preference Optimization）微调，说是比较节省显存，无需训练奖励模型，取得正向回答（chosen）和负向回答（rejected）即可开始微调。微调的<code>chosen</code>文本来自原数据集<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/c-s-ale/alpaca-gpt4-data-zh">alpaca-gpt4-data-zh</a>，拒绝文本<code>rejected</code>来自SFT微调1个epoch后的模型输出，另外两个数据集：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Skepsun/huozi_rlhf_data_json">huozi_rlhf_data_json</a>和<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/beyond/rlhf-reward-single-round-trans_chinese">rlhf-reward-single-round-trans_chinese</a>，合并后共8万条dpo数据，dpo数据集处理过程在<code>utils/dpo_data_process.py</code>。</p>
<h3 id="数据集下载及制作"><a href="#数据集下载及制作" class="headerlink" title="数据集下载及制作"></a>数据集下载及制作</h3><p>在<code>data/raw_data</code>文件夹下打开终端，依次运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/Skepsun/huozi_rlhf_data_json</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://huggingface.co/datasets/beyond/rlhf-reward-single-round-trans_chinese</span><br></pre></td></tr></table></figure>

<p>然后，将</p>
<ul>
<li><p>alpaca_gpt4_data_zh&#x2F;alpaca_gpt4_data_zh.json</p>
</li>
<li><p>huozi_rlhf_data_json&#x2F;huozi_rlhf_data.json</p>
</li>
<li><p>rlhf-reward-single-round-trans_chinese&#x2F;data&#x2F;train-00000-of-00001-789dc5dece0f1fc1.parquet和</p>
<p>test-00000-of-00001-8ecd46436fadcf7f.parquet</p>
</li>
</ul>
<p>复制到raw_data文件夹下，打开<code>utils/dpo_data_process.py</code>，将主程序入口下方的函数注释去掉，然后运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python dpo_data_process.py</span><br></pre></td></tr></table></figure>

<p>由于STF微调的试验一下章节已经指定了推理路径使用sft的checkpoint-120000，所以rejected使用的也是这个检查点。</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/4c827d26cc2f531350a0c46f16fe5ea.png" class="" title="4c827d26cc2f531350a0c46f16fe5ea">

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/d42712e2f723e6b3d9fd87ede8e40ef.png" class="" title="d42712e2f723e6b3d9fd87ede8e40ef">

<hr>
<h3 id="DPO训练"><a href="#DPO训练" class="headerlink" title="DPO训练"></a>DPO训练</h3><p>数据集制作完毕后，先更改下DPO加载模型的路径，设置成sft的checkpoint-120000检查点加载：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326142917174.png" class="" title="image-20250326142917174">

<p>运行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install wandb</span><br><span class="line"></span><br><span class="line">cd ..</span><br><span class="line"></span><br><span class="line">python dpo_train.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/276b78ff698e642d99babdbf9ad95e1.png" class="" title="276b78ff698e642d99babdbf9ad95e1">

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/99b30048901aa654d190e49846588d6.png" class="" title="99b30048901aa654d190e49846588d6">

<h3 id="试验一下-2"><a href="#试验一下-2" class="headerlink" title="试验一下"></a>试验一下</h3><p>更改路径：</p>
<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/image-20250326152851163.png" class="" title="image-20250326152851163">

<p>运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python cli_demo.py</span><br></pre></td></tr></table></figure>

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/04733e147854cb62395463940ce8326.png" class="" title="04733e147854cb62395463940ce8326">

<img src="/2025/03/20/%E4%BB%8E%E9%9B%B6%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E4%B9%8BChatLM-mini-Chinese/11142dca3a425570c53ff7d93c52ba2.png" class="" title="11142dca3a425570c53ff7d93c52ba2">

<p>好的吧，依旧是玩具。</p>
<p>最后还有下游任务的微调-三元组信息抽取任务，有空时候再弄吧。</p>
<hr>
<h2 id="下游任务微调-三元组信息抽取"><a href="#下游任务微调-三元组信息抽取" class="headerlink" title="下游任务微调-三元组信息抽取"></a>下游任务微调-三元组信息抽取</h2><p>TODO…</p>
<hr>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="token-type"><a href="#token-type" class="headerlink" title="token_type"></a>token_type</h3><p><code>token_type=&#39;char&#39;</code>（字符级）和 <code>token_type=&#39;byte&#39;</code>（字节级）的主要区别在于 <strong>分词粒度、可扩展性和适用场景</strong>，下面详细分析它们的优缺点：</p>
<p><strong>1. <code>char</code>（字符级 BPE）</strong></p>
<p><strong>📌 原理</strong>：</p>
<ul>
<li>按照<strong>字符</strong>（Character）单位进行分词，不会拆分字符。</li>
<li>例如 <code>&quot;中国&quot;</code> 会作为两个独立字符 <code>&quot;中&quot;</code> 和 <code>&quot;国&quot;</code>。</li>
</ul>
<p><strong>✅ 优势</strong></p>
<ol>
<li><strong>更直观易理解</strong><ul>
<li>训练出来的 Tokenizer 结果更接近人类的直觉，比如 <code>&quot;你好&quot;</code> 会分成 <code>&quot;你&quot;</code> 和 <code>&quot;好&quot;</code>，而不会拆分成更小的部分。</li>
</ul>
</li>
<li><strong>适用于汉语、日语等非空格分隔语言</strong><ul>
<li>适用于中文、日文等 <strong>没有空格分词的语言</strong>，可以保留完整字符，提高可读性。</li>
</ul>
</li>
<li><strong>更少的 OOV（Out of Vocabulary，未登录词）</strong><ul>
<li>只要训练语料足够大，基本不会有 OOV，因为所有常见字符都会出现在词表里。</li>
</ul>
</li>
<li><strong>适用于字符粒度的 NLP 任务</strong><ul>
<li>适用于 <strong>拼写错误检测、文本纠错、光学字符识别（OCR）</strong>，因为能处理每个字符。</li>
</ul>
</li>
</ol>
<p><strong>❌ 缺点</strong></p>
<ol>
<li><strong>词表较大</strong><ul>
<li>每个字符都是一个 Token，导致 Token 总数较多（尤其是 Unicode 语言）。</li>
</ul>
</li>
<li><strong>序列长度较长</strong><ul>
<li>例如 <code>[&#39;你&#39;, &#39;好&#39;, &#39;世&#39;, &#39;界&#39;]</code> 这样拆分时，<strong>一个单词可能会被拆成多个字符</strong>，导致序列变长，增加 Transformer 计算量。</li>
</ul>
</li>
</ol>
<p><strong>2. <code>byte</code>（字节级 BPE）</strong></p>
<p><strong>📌 原理</strong>：</p>
<ul>
<li>按照<strong>字节</strong>（Byte）进行分词，而不是按照字符。</li>
<li>例如 <code>&quot;你好&quot;</code> 在 UTF-8 编码下会被拆分成多个字节 <code>\xe4\xbd\xa0</code> 和 <code>\xe5\xa5\xbd</code>。</li>
</ul>
<p><strong>✅ 优势</strong></p>
<ol>
<li><strong>通用性更强</strong><ul>
<li>能够处理 <strong>任何语言、特殊符号、表情符号</strong>，甚至可以用于 <strong>代码（编程语言）</strong>。</li>
</ul>
</li>
<li><strong>词表更小</strong><ul>
<li>只需学习 <strong>256 个字节（Byte-Level）</strong>，就能覆盖所有 Unicode 字符，不会有 OOV。</li>
</ul>
</li>
<li><strong>数据压缩效果好</strong><ul>
<li>由于 BPE 在字节级别运作，它可以动态学习子词，从而压缩文本（减少 Token 数量）。</li>
</ul>
</li>
<li><strong>适用于多语言场景</strong><ul>
<li>比如 GPT-2 选择 Byte-level BPE 就是为了支持<strong>多种语言</strong>。</li>
</ul>
</li>
</ol>
<p><strong>❌ 缺点</strong></p>
<ol>
<li><strong>可读性差</strong><ul>
<li><code>byte</code> 级别的分词不可读，例如 <code>&quot;你好&quot;</code> 可能变成 <code>[&#39;\xe4\xbd\xa0&#39;, &#39;\xe5\xa5\xbd&#39;]</code>，对于人类来说完全看不懂。</li>
</ul>
</li>
<li><strong>分词不自然</strong><ul>
<li>例如 <code>&quot;apple&quot;</code> 可能变成 <code>[&quot;ap&quot;, &quot;ple&quot;]</code>，会拆分奇怪的部分。</li>
</ul>
</li>
<li><strong>中文、日文、韩文（CJK 语言）分词效果不如 <code>char</code></strong><ul>
<li>由于 <code>byte</code> 级别是基于 <strong>Unicode 字节流</strong>，<strong>对于 CJK 语言会拆得很碎</strong>，导致上下文丢失。</li>
</ul>
</li>
</ol>
<p><strong>3. <code>char</code> vs <code>byte</code> 选择建议</strong></p>
<table>
<thead>
<tr>
<th>选择场景</th>
<th><code>char</code>（字符级 BPE）</th>
<th><code>byte</code>（字节级 BPE）</th>
</tr>
</thead>
<tbody><tr>
<td><strong>适用语言</strong></td>
<td>中文、日文、韩文（CJK）</td>
<td>多语言（特别是含代码、符号）</td>
</tr>
<tr>
<td><strong>可读性</strong></td>
<td>好，人能直接理解</td>
<td>差，分词结果不可读</td>
</tr>
<tr>
<td><strong>OOV（未登录词）</strong></td>
<td>低，基本不会有</td>
<td>永远没有 OOV</td>
</tr>
<tr>
<td><strong>模型适配</strong></td>
<td>更适合 NLP 任务</td>
<td>适合 GPT-2、编程语言</td>
</tr>
<tr>
<td><strong>词表大小</strong></td>
<td>较大</td>
<td>非常小（256 tokens）</td>
</tr>
<tr>
<td><strong>序列长度</strong></td>
<td>较长</td>
<td>较短（更适合 Transformer）</td>
</tr>
<tr>
<td><strong>特殊字符处理</strong></td>
<td>需要额外处理符号</td>
<td>直接支持任何符号</td>
</tr>
</tbody></table>
<p><strong>👉 推荐选择</strong></p>
<ul>
<li><strong>中文 NLP（ASR、翻译、摘要） → <code>char</code></strong></li>
<li><strong>多语言 NLP（GPT-2, LLaMA） → <code>byte</code></strong></li>
<li><strong>代码&#x2F;表情符号&#x2F;特殊符号 NLP → <code>byte</code></strong></li>
<li><strong>光学字符识别（OCR） → <code>char</code></strong></li>
<li><strong>数据量小，担心 OOV → <code>byte</code></strong></li>
</ul>
<p><strong>4. 总结</strong></p>
<table>
<thead>
<tr>
<th><code>char</code>（字符级 BPE）</th>
<th><code>byte</code>（字节级 BPE）</th>
</tr>
</thead>
<tbody><tr>
<td>✅ 适合 <strong>中文、日语等非空格分隔语言</strong></td>
<td>✅ 适合 <strong>多语言（GPT-2&#x2F;LLaMA）</strong></td>
</tr>
<tr>
<td>✅ <strong>可读性高</strong>，易理解</td>
<td>❌ <strong>不可读</strong>，难以分析</td>
</tr>
<tr>
<td>❌ <strong>词表较大</strong>，占内存</td>
<td>✅ <strong>词表小</strong>，节省显存</td>
</tr>
<tr>
<td>❌ <strong>序列长度长</strong>，增加计算量</td>
<td>✅ <strong>序列短</strong>，更适合 Transformer</td>
</tr>
<tr>
<td>✅ 适合 <strong>OCR、文本生成</strong></td>
<td>✅ 适合 <strong>代码&#x2F;NLP&#x2F;通用 AI</strong></td>
</tr>
</tbody></table>
<p><strong>最终建议</strong>：</p>
<ul>
<li><strong>处理单一语言（如中文 NLP）</strong> → <strong><code>char</code> 更合适</strong></li>
<li><strong>多语言处理（如 GPT-2）</strong> → <strong><code>byte</code> 更合适</strong></li>
<li><strong>如果不确定，默认 <code>char</code> 即可</strong> 🚀</li>
</ul>
<hr>
<h3 id="dpo日志"><a href="#dpo日志" class="headerlink" title="dpo日志"></a>dpo日志</h3><p>以下面这条为例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;loss&#x27;</span>: 0.1506, <span class="string">&#x27;learning_rate&#x27;</span>: 2.705977382875606e-07, <span class="string">&#x27;rewards/chosen&#x27;</span>: -0.2540930509567261, <span class="string">&#x27;rewards/rejected&#x27;</span>: -9.427597999572754, <span class="string">&#x27;rewards/accuracies&#x27;</span>: 0.949999988079071, <span class="string">&#x27;rewards/margins&#x27;</span>: 9.173505783081055, <span class="string">&#x27;logps/rejected&#x27;</span>: -284.3307189941406, <span class="string">&#x27;logps/chosen&#x27;</span>: -367.8659362792969, <span class="string">&#x27;logits/rejected&#x27;</span>: -5.64560079574585, <span class="string">&#x27;logits/chosen&#x27;</span>: -5.654943466186523, <span class="string">&#x27;epoch&#x27;</span>: 3.9&#125;</span><br></pre></td></tr></table></figure>

<p>每个字段分段是什么意思呢？</p>
<p><strong>📌 关键指标解读</strong></p>
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
<th>你的数值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong><code>loss</code></strong></td>
<td>训练损失</td>
<td><code>0.1506</code></td>
<td>下降了，说明模型在收敛</td>
</tr>
<tr>
<td><strong><code>learning_rate</code></strong></td>
<td>当前学习率</td>
<td><code>2.7e-07</code></td>
<td>可能处于衰减阶段</td>
</tr>
<tr>
<td><strong><code>rewards/chosen</code></strong></td>
<td>选中样本的奖励分数</td>
<td><code>-0.2541</code></td>
<td>负数，可能是因为 reward 标准化</td>
</tr>
<tr>
<td><strong><code>rewards/rejected</code></strong></td>
<td>被拒绝样本的奖励</td>
<td><code>-9.4276</code></td>
<td>远低于 <code>rewards/chosen</code>，符合预期</td>
</tr>
<tr>
<td><strong><code>rewards/accuracies</code></strong></td>
<td>训练准确率</td>
<td><code>0.9500</code></td>
<td>非常高，模型基本学会正确选择</td>
</tr>
<tr>
<td><strong><code>rewards/margins</code></strong></td>
<td>选中与拒绝样本的奖励差距</td>
<td><code>9.1735</code></td>
<td>说明模型能很好地区分好坏回答</td>
</tr>
<tr>
<td><strong><code>logps/rejected</code></strong></td>
<td>拒绝样本的 log 概率</td>
<td><code>-284.33</code></td>
<td>数值大，表示模型更倾向于拒绝</td>
</tr>
<tr>
<td><strong><code>logps/chosen</code></strong></td>
<td>选中样本的 log 概率</td>
<td><code>-367.87</code></td>
<td>数值更大，表示模型信心较低</td>
</tr>
<tr>
<td><strong><code>logits/rejected</code></strong></td>
<td>拒绝样本的原始 logit 分值</td>
<td><code>-5.65</code></td>
<td>和 chosen 相差不大</td>
</tr>
<tr>
<td><strong><code>logits/chosen</code></strong></td>
<td>选中样本的原始 logit 分值</td>
<td><code>-5.65</code></td>
<td>选中样本略好</td>
</tr>
<tr>
<td><strong>epoch</strong></td>
<td>当前轮数</td>
<td>3.9</td>
<td>即将完成（以4为目标）</td>
</tr>
</tbody></table>
<blockquote>
<p>信心（置信度）低表明虽然能选对，但犹豫不决，泛化（鲁棒）性不好。</p>
</blockquote>
<p><strong>📌 训练状态分析</strong></p>
<p>✅ <strong>损失（loss）已经降低</strong>，说明模型正在有效收敛。<br> ✅ <strong>准确率（rewards&#x2F;accuracies）很高</strong>：说明模型能正确区分大部分样本。<br> ✅ <strong>奖励分数对比</strong>：</p>
<ul>
<li><code>rewards/chosen</code> (<code>-0.2541</code>) 明显高于 <code>rewards/rejected</code> (<code>9.4276</code>)，说明模型在学习正确偏好。</li>
<li><code>rewards/margins = 9.1735</code>，说明模型能拉开好坏样本的差距。<br> ✅ 学习率下降：可能已经进入 <strong>学习率衰减（decay）</strong> 阶段，预计训练即将结束。</li>
</ul>
<p><strong>📌 需要关注的点</strong></p>
<p>🔹 <strong>Logit 差距较小</strong>（<code>logits/chosen ≈ logits/rejected</code>）</p>
<ul>
<li>说明模型对选中样本和拒绝样本的区分度还可以进一步优化。</li>
<li>可以检查 <code>DPOTrainer</code> 里的 <code>beta</code> 参数，看看是否需要调整。<br> 🔹 <strong>学习率已降到较低水平</strong></li>
<li>如果 loss 继续下降，可以考虑 <strong>提前停止训练（early stopping）</strong>，防止过拟合。</li>
<li>如果 loss 不再下降，可以 <strong>增加训练轮数</strong> 或 <strong>调整学习率调度策略</strong>（如 <code>cosine</code> 代替 <code>linear</code>）。</li>
</ul>
<p><strong>📌 总结</strong></p>
<p>📉 <strong>训练进度良好</strong>，模型在收敛，<code>loss</code> 降低，准确率高（<code>0.9500%</code>），reward 差距明显。<br> 🛠 <strong>可以考虑</strong> 观察 logit 变化，调整 <code>beta</code> 或学习率调度策略，确保最终模型的泛化能力。<br> 🚀 <strong>如果 loss 继续降低</strong>，可以 <strong>提前结束训练，保存最佳模型！</strong></p>
<hr>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/gmaaa123/article/details/140319691">https://blog.csdn.net/gmaaa123/article/details/140319691</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/charent/ChatLM-mini-Chinese">https://github.com/charent/ChatLM-mini-Chinese</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/SFT/" rel="tag"># SFT</a>
              <a href="/tags/ChatLM-mini-Chinese/" rel="tag"># ChatLM-mini-Chinese</a>
              <a href="/tags/Pre-train/" rel="tag"># Pre-train</a>
              <a href="/tags/DPO/" rel="tag"># DPO</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/03/18/Qwen2-5VL%E5%AE%89%E8%A3%85%E4%B8%8E%E5%AE%98%E6%96%B9%E7%8E%A9%E6%B3%95%E4%BD%93%E9%AA%8C/" rel="prev" title="Qwen2.5VL 7B安装与官方玩法体验">
                  <i class="fa fa-angle-left"></i> Qwen2.5VL 7B安装与官方玩法体验
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/21/AI%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%9A%E5%AF%BC%E8%AE%BAand%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%801/" rel="next" title="AI系列课程：导论and数学基础1">
                  AI系列课程：导论and数学基础1 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">279k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:56</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
