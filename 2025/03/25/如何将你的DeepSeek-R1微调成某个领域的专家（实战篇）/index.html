<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言偶然在bilibili大学刷到一个宝藏博主（我要我觉得）-code秘密花园，质量高（符合我的方向）但是低产（所以我能跟上😁），目前只有四个视频，但信息量也不小，遂打算先把前辈其中的与本文同名的视频整理一下（我与作者同名），见参考链接。顺便一提，本文长度超集长，简单总结一下：我通过colab折腾了好久，显示通过重新编译llama.cpp解决了一个严重问题，后制作GGUF文件成功后，通过olla">
<meta property="og:type" content="article">
<meta property="og:title" content="如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言偶然在bilibili大学刷到一个宝藏博主（我要我觉得）-code秘密花园，质量高（符合我的方向）但是低产（所以我能跟上😁），目前只有四个视频，但信息量也不小，遂打算先把前辈其中的与本文同名的视频整理一下（我与作者同名），见参考链接。顺便一提，本文长度超集长，简单总结一下：我通过colab折腾了好久，显示通过重新编译llama.cpp解决了一个严重问题，后制作GGUF文件成功后，通过olla">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326155208943.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250327200514880.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326160447684.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326162126205.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250327195641407.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326165209905.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326165405976.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326180930651.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210656222.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210753041.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210939841.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328211431024.png">
<meta property="article:published_time" content="2025-03-25T10:31:53.000Z">
<meta property="article:modified_time" content="2025-04-28T07:08:08.182Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="unsloth">
<meta property="article:tag" content="deepseek">
<meta property="article:tag" content="ollama">
<meta property="article:tag" content="huggingface">
<meta property="article:tag" content="llama-8b">
<meta property="article:tag" content="fine-tune">
<meta property="article:tag" content="colab">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326155208943.png">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/","path":"2025/03/25/如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）/","title":"如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>如何将你的DeepSeek-R1微调成某个领域的专家（实战篇） | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B3%E5%8F%B0%E8%AE%AD%E7%BB%83"><span class="nav-number">2.1.</span> <span class="nav-text">平台训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#colab%E8%AE%AD%E7%BB%83"><span class="nav-number">2.2.</span> <span class="nav-text">colab训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-number">2.2.1.</span> <span class="nav-text">代码解析</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0Jupyter-notebook%E8%BF%90%E8%A1%8C"><span class="nav-number">2.3.</span> <span class="nav-text">本地Jupyter notebook运行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vLLM%E8%BF%90%E8%A1%8C"><span class="nav-number">2.4.</span> <span class="nav-text">vLLM运行</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#unsloth%E5%AE%98%E6%96%B9%E7%A4%BA%E4%BE%8B"><span class="nav-number">2.5.</span> <span class="nav-text">unsloth官方示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">3.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">48</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">68</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="如何将你的DeepSeek-R1微调成某个领域的专家（实战篇） | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-25 18:31:53" itemprop="dateCreated datePublished" datetime="2025-03-25T18:31:53+08:00">2025-03-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-28 15:08:08" itemprop="dateModified" datetime="2025-04-28T15:08:08+08:00">2025-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/theory/" itemprop="url" rel="index"><span itemprop="name">theory</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>29k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:47</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>偶然在bilibili大学刷到一个宝藏博主（我要我觉得）-<a target="_blank" rel="noopener" href="https://space.bilibili.com/474921808">code秘密花园</a>，质量高（符合我的方向）但是低产（所以我能跟上😁），目前只有四个视频，但信息量也不小，遂打算先把前辈其中的与本文同名的视频整理一下（我与作者同名），见参考链接。顺便一提，本文长度超集长，简单总结一下：我通过colab折腾了好久，显示通过重新编译llama.cpp解决了一个严重问题，后制作GGUF文件成功后，通过ollama加载该文件失败，重装ollama也不行，但是用大佬的huggingface上的GGUF文件就可以运行，我又尝试了使用本机的jupyter notebook进行同等代码的运行，值得一提的时，模型直接从hugging face下载会出问题，遂先下载到本地，加载本地模型问题，也出现了llama.cpp的问题（同等方式解决）,也是制作成GGUF文件后，ollama无法正常加载（毁灭吧），最后还是没用上ollama，使用vLLM进行的模型加载，并简单直接的对比了模型的效果。</p>
<span id="more"></span>

<hr>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="平台训练"><a href="#平台训练" class="headerlink" title="平台训练"></a>平台训练</h3><hr>
<h3 id="colab训练"><a href="#colab训练" class="headerlink" title="colab训练"></a>colab训练</h3><p>Google <em>Colab</em>是一个免费的、云托管的 Jupyter Notebook 环境，您可以在浏览器中直接编写和运行 Python 代码。长话短说：可白嫖的云Jupter Notebook😀，好的，佬提供的<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1B4nS1L5_GuGHU4U8l-qI-Ej7EqeBNHg6#scrollTo=9m27TqNYzRog">代码</a>。</p>
<h4 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h4><ol>
<li><p>安装依赖</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%%capture  <span class="comment"># 这是一个 Jupyter Notebook 的魔法命令，用于隐藏命令的输出，让笔记本界面更整洁。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 unsloth 包。一个用于微调大型语言模型（LLM）的工具，可以让模型运行更快、占用更少内存。</span></span><br><span class="line">!pip install unsloth</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载当前已安装的 unsloth 包（如果已安装），然后从 GitHub 的源代码安装最新版本。</span></span><br><span class="line"><span class="comment"># 这样可以确保我们使用的是最新功能和修复。</span></span><br><span class="line">!pip uninstall unsloth -y &amp;&amp; pip install --upgrade --no-cache-<span class="built_in">dir</span> --no-deps git+https://github.com/unslothai/unsloth.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 bitsandbytes 和 unsloth_zoo 包。</span></span><br><span class="line"><span class="comment"># bitsandbytes 是一个用于量化和优化模型的库，可以帮助减少模型占用的内存。</span></span><br><span class="line"><span class="comment"># unsloth_zoo 可能包含了一些预训练模型或其他工具，方便我们使用。</span></span><br><span class="line">!pip install bitsandbytes unsloth_zoo</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载预训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel  <span class="comment"># 导入FastLanguageModel类，用来加载和使用模型</span></span><br><span class="line"><span class="keyword">import</span> torch  <span class="comment"># 导入torch工具，用于处理模型的数学运算</span></span><br><span class="line"></span><br><span class="line">max_seq_length = <span class="number">2048</span>  <span class="comment"># 设置模型处理文本的最大长度，相当于给模型设置一个“最大容量”</span></span><br><span class="line">dtype = <span class="literal">None</span>  <span class="comment"># 设置数据类型，让模型自动选择最适合的精度</span></span><br><span class="line">load_in_4bit = <span class="literal">True</span>  <span class="comment"># 使用4位量化来节省内存，就像把大箱子压缩成小箱子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型，并获取tokenizer工具</span></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name=<span class="string">&quot;unsloth/DeepSeek-R1-Distill-Llama-8B&quot;</span>,  <span class="comment"># 指定要加载的模型名称</span></span><br><span class="line">    max_seq_length=max_seq_length,  <span class="comment"># 使用前面设置的最大长度</span></span><br><span class="line">    dtype=dtype,  <span class="comment"># 使用前面设置的数据类型</span></span><br><span class="line">    load_in_4bit=load_in_4bit,  <span class="comment"># 使用4位量化</span></span><br><span class="line">    <span class="comment"># token=&quot;hf_...&quot;,  # 如果需要访问授权模型，可以在这里填入密钥</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里的load_in_4bit &#x3D; True控制的是模型权重加载（存储）到显存上的数据格式，指定 <strong>模型的权重</strong> 以 4-bit 量化的方式存储，以减少显存占用。这种 4-bit 量化通常使用 <code>NF4</code>（Normal Float 4）或 <code>FP4</code>（Float 4）格式，主要影响 <strong>存储和加载</strong>。、</p>
<p><code>dtype</code> 主要控制 <strong>推理或训练时</strong> 模型的计算数据类型，例如：</p>
<ul>
<li><code>float32</code></li>
<li><code>float16</code> (FP16)</li>
<li><code>bfloat16</code> (BF16)</li>
</ul>
<p>当 <code>dtype=None</code> 时，框架会根据硬件自动选择最优的计算精度：</p>
<ul>
<li><strong>如果 GPU 支持 BF16（如 A100、H100）</strong>，就会自动用 <code>bfloat16</code>，因为它在推理时更高效。</li>
<li><strong>如果 GPU 仅支持 FP16（如 RTX 3090、4090）</strong>，就会用 <code>float16</code>。</li>
</ul>
<p>两者不矛盾，而是互补的，一个影响存储，一个影响计算。</p>
</blockquote>
<p>输出：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326155208943.png" class="" title="image-20250326155208943">

<p>第二次运行：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250327200514880.png" class="" title="image-20250327200514880">
</li>
<li><p>微调前测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">prompt_style = <span class="string">&quot;&quot;&quot;以下是描述任务的指令，以及提供进一步上下文的输入。</span></span><br><span class="line"><span class="string">请写出一个适当完成请求的回答。</span></span><br><span class="line"><span class="string">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 指令：</span></span><br><span class="line"><span class="string">你是一位精通卜卦、星象和运势预测的算命大师。</span></span><br><span class="line"><span class="string">请回答以下算命问题。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 问题：</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 回答：</span></span><br><span class="line"><span class="string">&lt;think&gt;&#123;&#125;&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 定义提示风格的字符串模板，用于格式化问题</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义具体的算命问题</span></span><br><span class="line">question = <span class="string">&quot;1992年闰四月初九巳时生人，女，想了解健康运势&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备模型以进行推理（进入推理模式，关闭Dropout，减少不必要的梯度计算）</span></span><br><span class="line">FastLanguageModel.for_inference(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tokenizer 对格式化后的问题进行编码，并移动到 GPU</span></span><br><span class="line">inputs = tokenizer([prompt_style.<span class="built_in">format</span>(question, <span class="string">&quot;&quot;</span>)],return_tensors=<span class="string">&quot;pt&quot;</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型生成回答</span></span><br><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=<span class="number">1200</span>,</span><br><span class="line">    use_cache=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码模型生成的输出为可读文本</span></span><br><span class="line">response = tokenizer.batch_decode(outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印生成的回答部分</span></span><br><span class="line"><span class="built_in">print</span>(response[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
</blockquote>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;｜begin▁of▁sentence｜&gt;以下是描述任务的指令，以及提供进一步上下文的输入。</span><br><span class="line">请写出一个适当完成请求的回答。</span><br><span class="line">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 指令：</span></span><br><span class="line">你是一位精通卜卦、星象和运势预测的算命大师。</span><br><span class="line">请回答以下算命问题。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 问题：</span></span><br><span class="line"><span class="number">1992</span>年闰四月初九巳时生人，女，想了解健康运势</span><br><span class="line"></span><br><span class="line"><span class="comment">### 回答：</span></span><br><span class="line">&lt;think&gt;</span><br><span class="line">好的，来看这个问题。首先，用户的生日是<span class="number">1992</span>年闰四月初九巳时，女性，想了解自己的健康运势。作为算命大师，我需要先了解她的五行和星盘。</span><br><span class="line"></span><br><span class="line">首先，闰四月初九巳时，属于春季，生肖是兔年。兔年属火，火代表能量、活力和创造力。女性的命宫是水，代表灵性、情感和稳定。她的星盘需要看她的八字。</span><br><span class="line"></span><br><span class="line">接下来，分析她的五行和星盘。由于生肖是兔年，兔属火，女性的命宫是水，总的能量是火和水的组合，可能会比较平衡。她的身体健康方面，火能带来活力，但过多的火可能导致一些消耗，需要注意调节。</span><br><span class="line"></span><br><span class="line">然后，考虑她的健康运势。她的五行和星盘显示她可能会经历一些热带的变化，比如体温升高或者皮肤问题。建议她保持水分，注意身体的调节，避免过度劳累。</span><br><span class="line"></span><br><span class="line">最后，给出建议。保持良好的生活习惯，适度运动，注意饮食，多喝水，保持心情愉悦。这样可以帮助她保持身体健康，延长寿命。</span><br><span class="line"></span><br><span class="line">总结一下，她的健康运势中带有一些火元素，需要注意调节，保持平衡，才能维持长期的健康。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">根据你的生日<span class="number">1992</span>年闰四月初九巳时，我们可以分析你的五行和星盘。你的生肖是兔年，属于火元素。女性的命宫是水，代表灵性和情感。结合五行和星盘，可以看出你的健康运势中带有一些火元素，需要注意调节。</span><br><span class="line"></span><br><span class="line">健康方面，火能带来活力，但过多的火可能会导致身体的消耗。建议你保持水分，多喝水，保持身体的湿润。同时，注意调节情绪，保持心情平和，这对身体健康非常重要。</span><br><span class="line"></span><br><span class="line">总体来看，你的健康运势中有一定的火元素，需要注意调节，保持身体和心灵的平衡，这样才能维持长期的健康。&lt;｜end▁of▁sentence｜&gt;</span><br></pre></td></tr></table></figure>

<p>第二次运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&lt;｜begin▁of▁sentence｜&gt;以下是描述任务的指令，以及提供进一步上下文的输入。</span><br><span class="line">请写出一个适当完成请求的回答。</span><br><span class="line">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 指令：</span></span><br><span class="line">你是一位精通卜卦、星象和运势预测的算命大师。</span><br><span class="line">请回答以下算命问题。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 问题：</span></span><br><span class="line"><span class="number">1992</span>年闰四月初九巳时生人，女，想了解健康运势</span><br><span class="line"></span><br><span class="line"><span class="comment">### 回答：</span></span><br><span class="line">&lt;think&gt;</span><br><span class="line">嗯，今天有一个用户想了解自己的健康运势，她的生日是<span class="number">1992</span>年闰四月初九巳时，女性。首先，我需要分析她的生日和时间，确定她的生辰八字，然后根据五行理论来预测她的健康状况。</span><br><span class="line"></span><br><span class="line">首先，闰四月初九巳时，初九是阳煦，属于火旺日，这可能意味着她体质偏向火性，容易生气或者情绪波动较大。巳时对应的是土支，土能滋养和安定，但如果火旺，土可能会被弱化，容易出现干涸或消化不良的情况。</span><br><span class="line"></span><br><span class="line">接下来，五行分析方面，火代表她的主星，这意味着她的健康需要注意的是肺、胸腔、火相关的器官。可能会有哮喘、肺炎等问题，或者是高血压、心脏问题。土是她的辅助星，代表她的子宫、骨骼和肝脏健康，可能需要关注骨质疏松或肝脏问题。</span><br><span class="line"></span><br><span class="line">她的阴阳方面，阳气旺盛，容易活跃，可能会有失眠、易怒等问题，但也可以通过适当的运动和调节情绪来改善。阴气不足的话，可以通过食疗、补充水果和蔬菜来增强。</span><br><span class="line"></span><br><span class="line">生活建议方面，她需要保持良好的作息，适量运动，避免熬夜和过度劳累。情绪管理也很重要，学会放松心情，保持积极的心态。饮食上，多吃水果、蔬菜和全谷物，少吃辛辣油腻食物，适量喝红酒，保持体重。</span><br><span class="line"></span><br><span class="line">最后，提醒她定期体检，特别是肺和心脏方面的检查，及时发现问题，预防疾病。整体来看，她的健康状况需要关注火和土的平衡，保持良好的生活习惯和心态，才能维持健康运势。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">根据你的生日和时间，<span class="number">1992</span>年闰四月初九巳时，你的生辰八字为：</span><br><span class="line"></span><br><span class="line">- **出生时间**：<span class="number">1992</span>年<span class="number">4</span>月<span class="number">9</span>日，闰四月。</span><br><span class="line">- **出生地点**：未知，影响较小。</span><br><span class="line">- **时间**：巳时（<span class="number">3</span>-<span class="number">5</span>时）。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 五行分析：</span></span><br><span class="line">- **主星**：火（代表你的健康和运势）。</span><br><span class="line">- **辅助星**：土（代表子宫、骨骼、肝脏等）。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 占卜结果：</span></span><br><span class="line">根据你的生辰八字，你的身体健康状况需要注意以下几点：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **体质偏火**：你体质偏向火性，容易生气、容易疲劳、容易上火。建议多补水，少吃辛辣、油腻食物，保持心情舒畅。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **健康问题可能**：</span><br><span class="line">   - **肺部**：容易出现气喘、咽喉炎等问题。</span><br><span class="line">   - **心脏**：容易出现高血压、心脏病。</span><br><span class="line">   - **消化系统**：容易出现胃炎、胃溃疡、便秘等问题。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **建议**：</span><br><span class="line">   - 多运动，保持身体活泼。</span><br><span class="line">   - 注意情绪管理，避免长时间处于紧张状态。</span><br><span class="line">   - 多吃水果、蔬菜，少吃辛辣油腻食物。</span><br><span class="line">   - 定期体检，关注心肺功能。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> **运势**：</span><br><span class="line">   - 你的健康状况在后期可能会有所恶化，建议提前关注身体变化。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 总结：</span></span><br><span class="line">根据你的生辰八字，你的健康状况需要特别注意火和土的平衡问题，保持良好的生活习惯和心态，定期体检，及时发现和预防问题，可以延长健康寿命。&lt;｜end▁of▁sentence｜&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>加载数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个用于格式化提示的多行字符串模板</span></span><br><span class="line">train_prompt_style = <span class="string">&quot;&quot;&quot;以下是描述任务的指令，以及提供进一步上下文的输入。</span></span><br><span class="line"><span class="string">请写出一个适当完成请求的回答。</span></span><br><span class="line"><span class="string">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 指令：</span></span><br><span class="line"><span class="string">你是一位精通八字算命、 紫微斗数、 风水、易经卦象、塔罗牌占卜、星象、面相手相和运势预测等方面的算命大师。</span></span><br><span class="line"><span class="string">请回答以下算命问题。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 问题：</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 回答：</span></span><br><span class="line"><span class="string">&lt;思考&gt;</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string">&lt;/思考&gt;</span></span><br><span class="line"><span class="string">&#123;&#125;&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义结束标记（EOS_TOKEN），用于指示文本的结束</span></span><br><span class="line">EOS_TOKEN = tokenizer.eos_token  <span class="comment"># 必须添加结束标记</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入数据集加载函数</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="comment"># 加载指定的数据集，选择中文语言和训练集的前500条记录</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;Conard/fortune-telling&quot;</span>, <span class="string">&#x27;default&#x27;</span>, split = <span class="string">&quot;train[0:200]&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 打印数据集的列名，查看数据集中有哪些字段</span></span><br><span class="line"><span class="built_in">print</span>(dataset.column_names)</span><br></pre></td></tr></table></figure>

<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326160447684.png" class="" title="image-20250326160447684">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个函数，用于格式化数据集中的每条记录</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">formatting_prompts_func</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="comment"># 从数据集中提取问题、复杂思考过程和回答</span></span><br><span class="line">    inputs = examples[<span class="string">&quot;Question&quot;</span>]</span><br><span class="line">    cots = examples[<span class="string">&quot;Complex_CoT&quot;</span>]</span><br><span class="line">    outputs = examples[<span class="string">&quot;Response&quot;</span>]</span><br><span class="line">    texts = []  <span class="comment"># 用于存储格式化后的文本</span></span><br><span class="line">    <span class="comment"># 遍历每个问题、思考过程和回答，进行格式化</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span>, cot, output <span class="keyword">in</span> <span class="built_in">zip</span>(inputs, cots, outputs):</span><br><span class="line">        <span class="comment"># 使用字符串模板插入数据，并加上结束标记</span></span><br><span class="line">        text = train_prompt_style.<span class="built_in">format</span>(<span class="built_in">input</span>, cot, output) + EOS_TOKEN</span><br><span class="line">        texts.append(text)  <span class="comment"># 将格式化后的文本添加到列表中</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: texts,  <span class="comment"># 返回包含所有格式化文本的字典</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">dataset = dataset.<span class="built_in">map</span>(formatting_prompts_func, batched = <span class="literal">True</span>)</span><br><span class="line">dataset[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">以下是描述任务的指令，以及提供进一步上下文的输入。</span><br><span class="line">请写出一个适当完成请求的回答。</span><br><span class="line">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 指令：</span></span><br><span class="line">你是一位精通八字算命、 紫微斗数、 风水、易经卦象、塔罗牌占卜、星象、面相手相和运势预测等方面的算命大师。</span><br><span class="line">请回答以下算命问题。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 问题：</span></span><br><span class="line">新房装修,大门对着电梯好不好?要如何化解?</span><br><span class="line"></span><br><span class="line"><span class="comment">### 回答：</span></span><br><span class="line">&lt;思考&gt;</span><br><span class="line">好的，用户问的是新房装修时大门对着电梯好不好，以及如何化解。首先，我需要回忆一下风水学中关于大门和电梯的相关知识。电梯在风水中属于动气比较强的地方，因为电梯频繁开合，会带来不稳定的气流，也就是所谓的“煞气”。大门是住宅的纳气口，如果正对电梯，可能会让这些不稳定的气流直接冲进家里，影响居住者的健康和财运。</span><br><span class="line"></span><br><span class="line">接下来，我需要确认用户的具体情况。比如，大门和电梯的距离有多远？是否正对还是稍微偏一点？不过用户没有提供这些细节，所以只能给出一般性的建议。化解的方法通常有几种：屏风或玄关、门帘、五帝钱、植物、八卦镜等。需要逐一解释这些方法的原理和使用方式，同时提醒用户要根据实际情况选择，必要时咨询专业风水师。</span><br><span class="line"></span><br><span class="line">另外，还要注意语气要亲切，避免使用过于专业的术语，让用户容易理解。同时，要强调这些是传统方法，效果因人而异，保持客观中立。最后，可以建议用户如果情况复杂，最好请专业人士实地查看，这样更稳妥。</span><br><span class="line"></span><br><span class="line">&lt;/思考&gt;</span><br><span class="line">根据传统风水学的观点，大门正对电梯易形成<span class="string">&quot;开口煞&quot;</span>，电梯频繁升降会扰乱家宅气场。建议化解方案：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 玄关阻隔法</span><br><span class="line">在入门处设置L型屏风或文化砖玄关墙，高度以<span class="number">1.8</span>米为宜，既保持采光又形成缓冲带</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 五行通关法</span><br><span class="line">门槛石下埋设五帝钱+白玉葫芦，建议选丙申年铸造的真品古币，配合门楣悬挂九宫八卦镜</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 光影化解术</span><br><span class="line">安装磨砂玻璃内推门，门框镶嵌黄铜门槛，每日辰时用海盐净化门廊区域</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> 现代科技方案</span><br><span class="line">入户区安装智能感应灯带，设置循环播放的流水声效，运用声光电技术平衡磁场</span><br><span class="line"></span><br><span class="line">需注意电梯井方位与家主命卦的关系，建议提供具体户型平面图进行吉凶方位测算。当代建筑中可采用半透明艺术隔断结合空气净化系统，既符合科学原理又兼顾传统智慧。&lt;｜end▁of▁sentence｜&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行微调</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FastLanguageModel.for_training(model)</span><br><span class="line"></span><br><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,  <span class="comment"># 传入已经加载好的预训练模型</span></span><br><span class="line">    r = <span class="number">16</span>,  <span class="comment"># 设置 LoRA 的秩，决定添加的可训练参数数量</span></span><br><span class="line">    target_modules = [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>,<span class="comment">#指定模型中需要微调的关键模块</span></span><br><span class="line">                      <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>],</span><br><span class="line">    lora_alpha = <span class="number">16</span>,  <span class="comment"># 设置 LoRA 的超参数，影响可训练参数的训练方式</span></span><br><span class="line">    lora_dropout = <span class="number">0</span>,  <span class="comment"># 设置防止过拟合的参数，这里设置为 0 表示不丢弃任何参数</span></span><br><span class="line">    bias = <span class="string">&quot;none&quot;</span>,    <span class="comment"># 设置是否添加偏置项，这里设置为 &quot;none&quot; 表示不添加</span></span><br><span class="line">    use_gradient_checkpointing = <span class="string">&quot;unsloth&quot;</span>,  <span class="comment"># 使用优化技术节省显存并支持更大的批量大小</span></span><br><span class="line">    random_state = <span class="number">3407</span>,  <span class="comment"># 设置随机种子，确保每次运行代码时模型的初始化方式相同</span></span><br><span class="line">    use_rslora = <span class="literal">False</span>,  <span class="comment"># 设置是否使用Rank Stabilized LoRA技术，这里设置为 False 表示不使用</span></span><br><span class="line">    loftq_config = <span class="literal">None</span>,  <span class="comment"># 设置是否使用 LoftQ 技术，这里设置为 None 表示不使用</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unsloth <span class="number">2025.2</span><span class="number">.5</span> patched <span class="number">32</span> layers <span class="keyword">with</span> <span class="number">32</span> QKV layers, <span class="number">32</span> O layers <span class="keyword">and</span> <span class="number">32</span> MLP layers.</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> SFTTrainer  <span class="comment"># 导入 SFTTrainer，用于监督式微调</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments  <span class="comment"># 导入 TrainingArguments，用于设置训练参数</span></span><br><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> is_bfloat16_supported  <span class="comment"># 导入函数，检查是否支持 bfloat16 数据格式</span></span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(  <span class="comment"># 创建一个 SFTTrainer 实例</span></span><br><span class="line">    model=model,  <span class="comment"># 传入要微调的模型</span></span><br><span class="line">    tokenizer=tokenizer,  <span class="comment"># 传入 tokenizer，用于处理文本数据</span></span><br><span class="line">    train_dataset=dataset,  <span class="comment"># 传入训练数据集</span></span><br><span class="line">    dataset_text_field=<span class="string">&quot;text&quot;</span>,  <span class="comment"># 指定数据集中文本字段的名称</span></span><br><span class="line">    max_seq_length=max_seq_length,  <span class="comment"># 设置最大序列长度</span></span><br><span class="line">    dataset_num_proc=<span class="number">2</span>,  <span class="comment"># 设置数据处理的并行进程数</span></span><br><span class="line">    packing=<span class="literal">False</span>,  <span class="comment"># 是否启用打包功能（这里设置为 False，打包可以让训练更快，但可能影响效果）</span></span><br><span class="line">    args=TrainingArguments(  <span class="comment"># 定义训练参数</span></span><br><span class="line">        per_device_train_batch_size=<span class="number">2</span>,  <span class="comment"># 每个设备（如 GPU）上的批量大小</span></span><br><span class="line">        gradient_accumulation_steps=<span class="number">4</span>,  <span class="comment"># 梯度累积步数，用于模拟大批次训练</span></span><br><span class="line">        warmup_steps=<span class="number">5</span>,  <span class="comment"># 预热步数，训练开始时学习率逐渐增加的步数</span></span><br><span class="line">        max_steps=<span class="number">75</span>,  <span class="comment"># 最大训练步数</span></span><br><span class="line">        learning_rate=<span class="number">2e-4</span>,  <span class="comment"># 学习率，模型学习新知识的速度</span></span><br><span class="line">        fp16=<span class="keyword">not</span> is_bfloat16_supported(),  <span class="comment"># 是否使用 fp16 格式加速训练（如果环境不支持 bfloat16）</span></span><br><span class="line">        bf16=is_bfloat16_supported(),  <span class="comment"># 是否使用 bfloat16 格式加速训练（如果环境支持）</span></span><br><span class="line">        logging_steps=<span class="number">1</span>,  <span class="comment"># 每隔多少步记录一次训练日志</span></span><br><span class="line">        optim=<span class="string">&quot;adamw_8bit&quot;</span>,  <span class="comment"># 使用的优化器，用于调整模型参数</span></span><br><span class="line">        weight_decay=<span class="number">0.01</span>,  <span class="comment"># 权重衰减，防止模型过拟合</span></span><br><span class="line">        lr_scheduler_type=<span class="string">&quot;linear&quot;</span>,  <span class="comment"># 学习率调度器类型，控制学习率的变化方式</span></span><br><span class="line">        seed=<span class="number">3407</span>,  <span class="comment"># 随机种子，确保训练结果可复现</span></span><br><span class="line">        output_dir=<span class="string">&quot;outputs&quot;</span>,  <span class="comment"># 训练结果保存的目录</span></span><br><span class="line">        report_to=<span class="string">&quot;none&quot;</span>,  <span class="comment"># 是否将训练结果报告到外部工具（如 WandB），这里设置为不报告</span></span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326162126205.png" class="" title="image-20250326162126205">

<p>第二次运行：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250327195641407.png" class="" title="image-20250327195641407">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer_stats = trainer.train()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = <span class="number">1</span></span><br><span class="line">   \\   /|    Num examples = <span class="number">200</span> | Num Epochs = <span class="number">3</span></span><br><span class="line">O^O/ \_/ \    Batch size per device = <span class="number">2</span> | Gradient Accumulation steps = <span class="number">4</span></span><br><span class="line">\        /    Total batch size = <span class="number">8</span> | Total steps = <span class="number">75</span></span><br><span class="line"> <span class="string">&quot;-____-&quot;</span>     Number of trainable parameters = <span class="number">41</span>,<span class="number">943</span>,040</span><br><span class="line"> [<span class="number">75</span>/<span class="number">75</span> <span class="number">33</span>:<span class="number">17</span>, Epoch <span class="number">3</span>/<span class="number">3</span>]</span><br><span class="line">Step	Training Loss</span><br><span class="line"><span class="number">1</span>	<span class="number">1.100600</span></span><br><span class="line"><span class="number">2</span>	<span class="number">1.150300</span></span><br><span class="line"><span class="number">3</span>	<span class="number">1.178300</span></span><br><span class="line"><span class="number">4</span>	<span class="number">1.129200</span></span><br><span class="line"><span class="number">5</span>	<span class="number">1.081000</span></span><br><span class="line"><span class="number">6</span>	<span class="number">1.221900</span></span><br><span class="line"><span class="number">7</span>	<span class="number">1.159500</span></span><br><span class="line"><span class="number">8</span>	<span class="number">1.159400</span></span><br><span class="line"><span class="number">9</span>	<span class="number">1.139200</span></span><br><span class="line"><span class="number">10</span>	<span class="number">1.215800</span></span><br><span class="line"><span class="number">11</span>	<span class="number">1.215000</span></span><br><span class="line"><span class="number">12</span>	<span class="number">1.235800</span></span><br><span class="line"><span class="number">13</span>	<span class="number">1.159500</span></span><br><span class="line"><span class="number">14</span>	<span class="number">1.181800</span></span><br><span class="line"><span class="number">15</span>	<span class="number">1.084300</span></span><br><span class="line"><span class="number">16</span>	<span class="number">1.124400</span></span><br><span class="line"><span class="number">17</span>	<span class="number">1.139700</span></span><br><span class="line"><span class="number">18</span>	<span class="number">1.077200</span></span><br><span class="line"><span class="number">19</span>	<span class="number">1.084900</span></span><br><span class="line"><span class="number">20</span>	<span class="number">1.114400</span></span><br><span class="line"><span class="number">21</span>	<span class="number">1.081500</span></span><br><span class="line"><span class="number">22</span>	<span class="number">1.067300</span></span><br><span class="line"><span class="number">23</span>	<span class="number">1.243800</span></span><br><span class="line"><span class="number">24</span>	<span class="number">0.970000</span></span><br><span class="line"><span class="number">25</span>	<span class="number">1.002700</span></span><br><span class="line"><span class="number">26</span>	<span class="number">0.969900</span></span><br><span class="line"><span class="number">27</span>	<span class="number">0.809700</span></span><br><span class="line"><span class="number">28</span>	<span class="number">0.907700</span></span><br><span class="line"><span class="number">29</span>	<span class="number">0.892400</span></span><br><span class="line"><span class="number">30</span>	<span class="number">0.853600</span></span><br><span class="line"><span class="number">31</span>	<span class="number">0.887800</span></span><br><span class="line"><span class="number">32</span>	<span class="number">0.911100</span></span><br><span class="line"><span class="number">33</span>	<span class="number">0.800700</span></span><br><span class="line"><span class="number">34</span>	<span class="number">0.820800</span></span><br><span class="line"><span class="number">35</span>	<span class="number">0.801000</span></span><br><span class="line"><span class="number">36</span>	<span class="number">0.861300</span></span><br><span class="line"><span class="number">37</span>	<span class="number">0.808400</span></span><br><span class="line"><span class="number">38</span>	<span class="number">0.840700</span></span><br><span class="line"><span class="number">39</span>	<span class="number">0.728800</span></span><br><span class="line"><span class="number">40</span>	<span class="number">0.898300</span></span><br><span class="line"><span class="number">41</span>	<span class="number">0.844300</span></span><br><span class="line"><span class="number">42</span>	<span class="number">0.734000</span></span><br><span class="line"><span class="number">43</span>	<span class="number">0.890800</span></span><br><span class="line"><span class="number">44</span>	<span class="number">0.865800</span></span><br><span class="line"><span class="number">45</span>	<span class="number">0.747900</span></span><br><span class="line"><span class="number">46</span>	<span class="number">0.776400</span></span><br><span class="line"><span class="number">47</span>	<span class="number">0.740800</span></span><br><span class="line"><span class="number">48</span>	<span class="number">0.910900</span></span><br><span class="line"><span class="number">49</span>	<span class="number">1.033900</span></span><br><span class="line"><span class="number">50</span>	<span class="number">0.893700</span></span><br><span class="line"><span class="number">51</span>	<span class="number">0.651400</span></span><br><span class="line"><span class="number">52</span>	<span class="number">0.634500</span></span><br><span class="line"><span class="number">53</span>	<span class="number">0.580400</span></span><br><span class="line"><span class="number">54</span>	<span class="number">0.616100</span></span><br><span class="line"><span class="number">55</span>	<span class="number">0.662600</span></span><br><span class="line"><span class="number">56</span>	<span class="number">0.661900</span></span><br><span class="line"><span class="number">57</span>	<span class="number">0.627100</span></span><br><span class="line"><span class="number">58</span>	<span class="number">0.627000</span></span><br><span class="line"><span class="number">59</span>	<span class="number">0.615800</span></span><br><span class="line"><span class="number">60</span>	<span class="number">0.634600</span></span><br><span class="line"><span class="number">61</span>	<span class="number">0.580500</span></span><br><span class="line"><span class="number">62</span>	<span class="number">0.682000</span></span><br><span class="line"><span class="number">63</span>	<span class="number">0.703400</span></span><br><span class="line"><span class="number">64</span>	<span class="number">0.648300</span></span><br><span class="line"><span class="number">65</span>	<span class="number">0.629400</span></span><br><span class="line"><span class="number">66</span>	<span class="number">0.777200</span></span><br><span class="line"><span class="number">67</span>	<span class="number">0.578600</span></span><br><span class="line"><span class="number">68</span>	<span class="number">0.600700</span></span><br><span class="line"><span class="number">69</span>	<span class="number">0.661200</span></span><br><span class="line"><span class="number">70</span>	<span class="number">0.599500</span></span><br><span class="line"><span class="number">71</span>	<span class="number">0.597800</span></span><br><span class="line"><span class="number">72</span>	<span class="number">0.702700</span></span><br><span class="line"><span class="number">73</span>	<span class="number">0.584300</span></span><br><span class="line"><span class="number">74</span>	<span class="number">0.581900</span></span><br><span class="line"><span class="number">75</span>	<span class="number">0.603100</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>微调后测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(question) <span class="comment"># 打印前面的问题</span></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1992</span>年闰四月初九巳时生人，女，想了解健康运势</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 将模型切换到推理模式，准备回答问题</span><br><span class="line">FastLanguageModel.for_inference(model)</span><br><span class="line"></span><br><span class="line"># 将问题转换成模型能理解的格式，并发送到 GPU 上</span><br><span class="line">inputs = tokenizer([prompt_style.format(question, &quot;&quot;)],return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line"># 让模型根据问题生成回答，最多生成 4000 个新词</span><br><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,  # 输入的数字序列</span><br><span class="line">    attention_mask=inputs.attention_mask,  # 注意力遮罩，帮助模型理解哪些部分重要</span><br><span class="line">    max_new_tokens=4000,  # 最多生成 4000 个新词</span><br><span class="line">    use_cache=True,  # 使用缓存加速生成</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 将生成的回答从数字转换回文字</span><br><span class="line">response = tokenizer.batch_decode(outputs)</span><br><span class="line"></span><br><span class="line"># 打印回答</span><br><span class="line">print(response[0])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;｜begin▁of▁sentence｜&gt;以下是描述任务的指令，以及提供进一步上下文的输入。</span><br><span class="line">请写出一个适当完成请求的回答。</span><br><span class="line">在回答之前，请仔细思考问题，并创建一个逻辑连贯的思考过程，以确保回答准确无误。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 指令：</span></span><br><span class="line">你是一位精通卜卦、星象和运势预测的算命大师。</span><br><span class="line">请回答以下算命问题。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 问题：</span></span><br><span class="line"><span class="number">1992</span>年闰四月初九巳时生人，女，想了解健康运势</span><br><span class="line"></span><br><span class="line"><span class="comment">### 回答：</span></span><br><span class="line">&lt;think&gt;</span><br><span class="line"></span><br><span class="line">&lt;算命大师思考&gt;</span><br><span class="line">好的，用户提供的信息是：<span class="number">1992</span>年闰四月初九巳时生，女性，想了解健康运势。首先，我需要确认这个日期是否符合传统生辰八字的记录方法。闰四月在农历中属于春季，且闰月通常在每四年出现一次，所以闰四月初九对应的公历日期需要查证。不过根据传统八字排盘原则，闰月的干支会与非闰月形成不同的组合。接下来，巳时对应的时辰是上午<span class="number">11</span>点至下午<span class="number">1</span>点，属于日干戊时的午时，需结合年柱进行分析。</span><br><span class="line"></span><br><span class="line">接下来，用户的生肖是未，因为<span class="number">1992</span>年是龙年，未月对应的生肖是龙。日柱需要结合年柱和月柱来看，年柱为龙，月柱为未，时柱为巳。需要确定年柱龙与月柱未的组合，以及时柱巳与日柱的关系。然后，分析五行分布，龙属水，未属水，巳属火，日柱未土需要看是否有生扶或杀破。接着看健康运势，需要结合八字中的五行平衡、天干地支的互动，以及流年运势的影响。比如，流年是否有煞星或吉星，是否有健康星如天喜、天同等。同时，还要考虑用户的性别，女性健康运势可能有不同的侧重点，比如情绪、子宫健康等。</span><br><span class="line"></span><br><span class="line">最后，结合用户的具体问题，健康运势需要具体化，比如是否有需要注意的身体部位，是否需要增强某方面的健康习惯，或者是否有潜在的健康隐患。需要综合八字、流年、个人的喜用神等因素，给出具体建议。同时，要避免使用过于绝对的语言，保持建议的可操作性和积极性。可能还需要提醒用户注意环境因素，保持良好的生活习惯，平衡心身健康。</span><br><span class="line"></span><br><span class="line">&lt;/算命大师&gt;</span><br><span class="line">🌸 午时生辰的未月龙辰与巳时火日形成“龙火相生”的格局，需注意脾胃与肝胆的调和。建议：</span><br><span class="line">✧ 每日辰时（<span class="number">7</span>-<span class="number">9</span>点）饮用绿茶或菊花茶（<span class="number">3</span>杯/日），增强气血运转</span><br><span class="line">✧ 每月初九日（农历）佩戴粉晶手串调和三方，平衡五行</span><br><span class="line">✧ 春季注意食用养生品（如当归、枸杞），秋季适量补充黑芝麻</span><br><span class="line">✦ <span class="number">2024</span>甲辰年有“天喜”星照，适合进行太极拳、八段锦等调养</span><br><span class="line">✦ 戊子年需防“火星”动，避免高秋过度劳累，建议选择秋季的银耳茶时辰休息</span><br><span class="line">✨ 若有不适症状，建议在巳时（<span class="number">11</span>-<span class="number">13</span>点）静坐补命，配合艾灸（需专业师傅）&lt;｜end▁of▁sentence｜&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将微调后的模型保存为 GGUF 格式</p>
<p>需要到<a target="_blank" rel="noopener" href="https://huggingface.co/">huggingface</a>点击头像，点击<code>Access Tokens</code>，点击右上角的<code>+Create new token</code>，点击<code>Write</code>，<code>Token name</code>随意填（以<code>XX</code>为例），点击<code>Create token</code>，点击<code>Copy</code>。回到<code>colab</code>，点击下图箭头所示位置：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326165209905.png" class="" title="image-20250326165209905">

<p>然后按位置填入即可：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326165405976.png" class="" title="image-20250326165405976">



<p>注意，Colab 默认环境里没有 <code>llama.cpp</code>，但是下面的代码需要这个，所以需要先配置下环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!rm -rf llama.cpp &amp;&amp; git clone https://github.com/ggml-org/llama.cpp.git &amp;&amp; cd llama.cpp &amp;&amp; git checkout b3345 &amp;&amp; git submodule update --init --recursive &amp;&amp; make clean &amp;&amp; make <span class="built_in">all</span> -j &amp;&amp; git log -<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>否则会报错：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250326180930651.png" class="" title="image-20250326180930651">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError                              Traceback (most recent call last)</span><br><span class="line">&lt;ipython-<span class="built_in">input</span>-<span class="number">17</span>-e4f24433ed0e&gt; <span class="keyword">in</span> &lt;cell line: <span class="number">0</span>&gt;()</span><br><span class="line">      <span class="number">7</span> <span class="comment"># 将模型保存为 8 位量化格式（Q8_0）</span></span><br><span class="line">      <span class="number">8</span> <span class="comment"># 这种格式文件小且运行快，适合部署到资源受限的设备</span></span><br><span class="line">----&gt; <span class="number">9</span> <span class="keyword">if</span> <span class="literal">True</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer,)</span><br><span class="line">     <span class="number">10</span> </span><br><span class="line">     <span class="number">11</span> <span class="comment"># 将模型保存为 16 位量化格式（f16）</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span> frames</span><br><span class="line">/usr/local/lib/python3<span class="number">.11</span>/dist-packages/unsloth/save.py <span class="keyword">in</span> save_to_gguf(model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, _run_installer)</span><br><span class="line">   <span class="number">1083</span>             quantize_location = <span class="string">&quot;llama.cpp/llama-quantize&quot;</span></span><br><span class="line">   <span class="number">1084</span>         <span class="keyword">else</span>:</span><br><span class="line">-&gt; <span class="number">1085</span>             <span class="keyword">raise</span> RuntimeError(</span><br><span class="line">   <span class="number">1086</span>                 <span class="string">&quot;Unsloth: The file (&#x27;llama.cpp/llama-quantize&#x27; or &#x27;llama.cpp/llama-quantize.exe&#x27; if you are on Windows WSL) or &#x27;llama.cpp/quantize&#x27; does not exist.\n&quot;</span>\</span><br><span class="line">   <span class="number">1087</span>                 <span class="string">&quot;But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.&quot;</span></span><br><span class="line"></span><br><span class="line">RuntimeError: Unsloth: The file (<span class="string">&#x27;llama.cpp/llama-quantize&#x27;</span> <span class="keyword">or</span> <span class="string">&#x27;llama.cpp/llama-quantize.exe&#x27;</span> <span class="keyword">if</span> you are on Windows WSL) <span class="keyword">or</span> <span class="string">&#x27;llama.cpp/quantize&#x27;</span> does <span class="keyword">not</span> exist.</span><br><span class="line">But we expect this file to exist! Maybe the llama.cpp developers changed the name <span class="keyword">or</span> check extension of the llama-quantize file.</span><br></pre></td></tr></table></figure>

<p>运行下面代码保存模型文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Google Colab 的 userdata 模块，用于访问用户数据</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> userdata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 Google Colab 用户数据中获取 Hugging Face 的 API 令牌</span></span><br><span class="line">HUGGINGFACE_TOKEN = userdata.get(<span class="string">&#x27;HUGGINGFACE_TOKEN&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型保存为 8 位量化格式（Q8_0）</span></span><br><span class="line"><span class="comment"># 这种格式文件小且运行快，适合部署到资源受限的设备</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型保存为 16 位量化格式（f16）</span></span><br><span class="line"><span class="comment"># 16 位量化精度更高，但文件稍大</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_gguf(<span class="string">&quot;model_f16&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;f16&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型保存为 4 位量化格式（q4_k_m）</span></span><br><span class="line"><span class="comment"># 4 位量化文件最小，但精度可能稍低</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;q4_k_m&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br></pre></td><td class="code"><pre><span class="line">Unsloth<span class="punctuation">:</span> ##### The current model auto adds a BOS token.</span><br><span class="line">Unsloth<span class="punctuation">:</span> ##### Your chat template has a BOS token. We shall remove it temporarily.</span><br><span class="line">Unsloth<span class="punctuation">:</span> You have <span class="number">1</span> CPUs. Using `safe_serialization` is <span class="number">10</span>x slower.</span><br><span class="line">We shall switch to Pytorch saving<span class="punctuation">,</span> which might take <span class="number">3</span> minutes and not <span class="number">30</span> minutes.</span><br><span class="line">To force `safe_serialization`<span class="punctuation">,</span> set it to `None` instead.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Kaggle/Colab has limited disk space. We need to delete the downloaded</span><br><span class="line">model which will save <span class="number">4</span><span class="number">-16</span>GB of disk space<span class="punctuation">,</span> allowing you to save on Kaggle/Colab.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Will remove a cached repo with size <span class="number">6.0</span>G</span><br><span class="line">Unsloth<span class="punctuation">:</span> Merging <span class="number">4</span>bit and LoRA weights to <span class="number">16</span>bit...</span><br><span class="line">Unsloth<span class="punctuation">:</span> Will use up to <span class="number">4.42</span> out of <span class="number">12.67</span> RAM for saving.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving model... This might take <span class="number">5</span> minutes ...</span><br><span class="line"> <span class="number">47</span>%|████▋     | <span class="number">15</span>/<span class="number">32</span> <span class="punctuation">[</span><span class="number">00</span><span class="punctuation">:</span><span class="number">01</span>&lt;<span class="number">00</span><span class="punctuation">:</span><span class="number">01</span><span class="punctuation">,</span> <span class="number">15.12</span>it/s<span class="punctuation">]</span></span><br><span class="line">We will save to Disk and not RAM now.</span><br><span class="line"><span class="number">100</span>%|██████████| <span class="number">32</span>/<span class="number">32</span> <span class="punctuation">[</span><span class="number">02</span><span class="punctuation">:</span><span class="number">16</span>&lt;<span class="number">00</span><span class="punctuation">:</span><span class="number">00</span><span class="punctuation">,</span>  <span class="number">4.27</span>s/it<span class="punctuation">]</span></span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving tokenizer... Done.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving model/pytorch_model<span class="number">-00001</span>-of<span class="number">-00004.</span>bin...</span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving model/pytorch_model<span class="number">-00002</span>-of<span class="number">-00004.</span>bin...</span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving model/pytorch_model<span class="number">-00003</span>-of<span class="number">-00004.</span>bin...</span><br><span class="line">Unsloth<span class="punctuation">:</span> Saving model/pytorch_model<span class="number">-00004</span>-of<span class="number">-00004.</span>bin...</span><br><span class="line">Done.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Converting llama model. Can use fast conversion = False.</span><br><span class="line">==((====))==  Unsloth<span class="punctuation">:</span> Conversion from QLoRA to GGUF information</span><br><span class="line">   \\   /|    <span class="punctuation">[</span><span class="number">0</span><span class="punctuation">]</span> Installing llama.cpp might take <span class="number">3</span> minutes.</span><br><span class="line">O^O/ \_/ \    <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> Converting HF to GGUF <span class="number">16</span>bits might take <span class="number">3</span> minutes.</span><br><span class="line">\        /    <span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span> Converting GGUF <span class="number">16</span>bits to <span class="punctuation">[</span>&#x27;q8_0&#x27;<span class="punctuation">]</span> might take <span class="number">10</span> minutes each.</span><br><span class="line"> <span class="string">&quot;-____-&quot;</span>     In total<span class="punctuation">,</span> you will have to wait at least <span class="number">16</span> minutes.</span><br><span class="line"></span><br><span class="line">Unsloth<span class="punctuation">:</span> Installing llama.cpp. This might take <span class="number">3</span> minutes...</span><br><span class="line">Unsloth<span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span> Converting model at model into q8_0 GGUF format.</span><br><span class="line">The output location will be /content/model/unsloth.Q8_0.gguf</span><br><span class="line">This might take <span class="number">3</span> minutes...</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>Loading model<span class="punctuation">:</span> model</span><br><span class="line">INFO<span class="punctuation">:</span>gguf.gguf_writer<span class="punctuation">:</span>gguf<span class="punctuation">:</span> This GGUF file is for Little Endian only</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>Set model parameters</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> context length = <span class="number">131072</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> embedding length = <span class="number">4096</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> feed forward length = <span class="number">14336</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> head count = <span class="number">32</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> key-value head count = <span class="number">8</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> rope theta = <span class="number">500000.0</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> rms norm epsilon = <span class="number">1e-05</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> file type = <span class="number">7</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>Set model tokenizer</span><br><span class="line">INFO<span class="punctuation">:</span>numexpr.utils<span class="punctuation">:</span>NumExpr defaulting to <span class="number">2</span> threads.</span><br><span class="line">WARNING<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Adding merges requested but no merges found<span class="punctuation">,</span> output may be non-functional.</span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting special token type bos to <span class="number">128000</span></span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting special token type eos to <span class="number">128001</span></span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting special token type pad to <span class="number">128004</span></span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting add_bos_token to True</span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting add_eos_token to False</span><br><span class="line">INFO<span class="punctuation">:</span>gguf.vocab<span class="punctuation">:</span>Setting chat_template to <span class="punctuation">&#123;</span>% if not add_generation_prompt is defined %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% set add_generation_prompt = <span class="literal"><span class="keyword">false</span></span> %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% set ns = namespace(is_first=<span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span> is_tool=<span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span> is_output_first=<span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span> system_prompt=&#x27;&#x27;) %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- for message in messages %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if message<span class="punctuation">[</span>&#x27;role&#x27;<span class="punctuation">]</span> == &#x27;system&#x27; %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% set ns.system_prompt = message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endfor %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>ns.system_prompt<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- for message in messages %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if message<span class="punctuation">[</span>&#x27;role&#x27;<span class="punctuation">]</span> == &#x27;user&#x27; %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_tool = <span class="literal"><span class="keyword">false</span></span> -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜User｜&gt;&#x27; + message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if message<span class="punctuation">[</span>&#x27;role&#x27;<span class="punctuation">]</span> == &#x27;assistant&#x27; and message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> is none %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_tool = <span class="literal"><span class="keyword">false</span></span> -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- for tool in message<span class="punctuation">[</span>&#x27;tool_calls&#x27;<span class="punctuation">]</span>%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if not ns.is_first %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜Assistant｜&gt;&lt;｜tool▁calls▁begin｜&gt;&lt;｜tool▁call▁begin｜&gt;&#x27; + tool<span class="punctuation">[</span>&#x27;type&#x27;<span class="punctuation">]</span> + &#x27;&lt;｜tool▁sep｜&gt;&#x27; + tool<span class="punctuation">[</span>&#x27;function&#x27;<span class="punctuation">]</span><span class="punctuation">[</span>&#x27;name&#x27;<span class="punctuation">]</span> + &#x27;\n&#x27; + &#x27;```json&#x27; + &#x27;\n&#x27; + tool<span class="punctuation">[</span>&#x27;function&#x27;<span class="punctuation">]</span><span class="punctuation">[</span>&#x27;arguments&#x27;<span class="punctuation">]</span> + &#x27;\n&#x27; + &#x27;```&#x27; + &#x27;&lt;｜tool▁call▁end｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_first = <span class="literal"><span class="keyword">true</span></span> -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- else %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;\n&#x27; + &#x27;&lt;｜tool▁call▁begin｜&gt;&#x27; + tool<span class="punctuation">[</span>&#x27;type&#x27;<span class="punctuation">]</span> + &#x27;&lt;｜tool▁sep｜&gt;&#x27; + tool<span class="punctuation">[</span>&#x27;function&#x27;<span class="punctuation">]</span><span class="punctuation">[</span>&#x27;name&#x27;<span class="punctuation">]</span> + &#x27;\n&#x27; + &#x27;```json&#x27; + &#x27;\n&#x27; + tool<span class="punctuation">[</span>&#x27;function&#x27;<span class="punctuation">]</span><span class="punctuation">[</span>&#x27;arguments&#x27;<span class="punctuation">]</span> + &#x27;\n&#x27; + &#x27;```&#x27; + &#x27;&lt;｜tool▁call▁end｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜tool▁calls▁end｜&gt;&lt;｜end▁of▁sentence｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endfor %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if message<span class="punctuation">[</span>&#x27;role&#x27;<span class="punctuation">]</span> == &#x27;assistant&#x27; and message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> is not none %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if ns.is_tool %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27; + message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> + &#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_tool = <span class="literal"><span class="keyword">false</span></span> -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- else %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% set content = message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% if &#x27;&lt;/think&gt;&#x27; in content %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% set content = content.split(&#x27;&lt;/think&gt;&#x27;)<span class="punctuation">[</span><span class="number">-1</span><span class="punctuation">]</span> %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜Assistant｜&gt;&#x27; + content + &#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if message<span class="punctuation">[</span>&#x27;role&#x27;<span class="punctuation">]</span> == &#x27;tool&#x27; %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_tool = <span class="literal"><span class="keyword">true</span></span> -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- if ns.is_output_first %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜tool▁outputs▁begin｜&gt;&lt;｜tool▁output▁begin｜&gt;&#x27; + message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> + &#x27;&lt;｜tool▁output▁end｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- set ns.is_output_first = <span class="literal"><span class="keyword">false</span></span> %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- else %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;\n&lt;｜tool▁output▁begin｜&gt;&#x27; + message<span class="punctuation">[</span>&#x27;content&#x27;<span class="punctuation">]</span> + &#x27;&lt;｜tool▁output▁end｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>%- endfor -%<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% if ns.is_tool %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% endif %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% if add_generation_prompt and not ns.is_tool %<span class="punctuation">&#125;</span><span class="punctuation">&#123;</span><span class="punctuation">&#123;</span>&#x27;&lt;｜Assistant｜&gt;&lt;think&gt;\n&#x27;<span class="punctuation">&#125;</span><span class="punctuation">&#125;</span><span class="punctuation">&#123;</span>% endif %<span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>Exporting model...</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> loading model weight map from &#x27;pytorch_model.bin.index.json&#x27;</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> loading model part &#x27;pytorch_model<span class="number">-00001</span>-of<span class="number">-00004.</span>bin&#x27;</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>token_embd.weight<span class="punctuation">,</span>           torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">128256</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.0</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.1</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.2</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.3</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.4</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.5</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.6</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.7</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.8</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> loading model part &#x27;pytorch_model<span class="number">-00002</span>-of<span class="number">-00004.</span>bin&#x27;</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.attn_q.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.attn_k.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.attn_v.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.attn_output.weight<span class="punctuation">,</span>    torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.ffn_gate.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.ffn_up.weight<span class="punctuation">,</span>         torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.ffn_down.weight<span class="punctuation">,</span>       torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.attn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.9</span>.ffn_norm.weight<span class="punctuation">,</span>       torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.10</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.11</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.12</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.13</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.14</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.15</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.16</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.17</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.18</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.19</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> loading model part &#x27;pytorch_model<span class="number">-00003</span>-of<span class="number">-00004.</span>bin&#x27;</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.20</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.21</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.22</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.23</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.24</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.25</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.26</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.27</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.28</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.29</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.30</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.attn_q.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.attn_k.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.attn_v.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">1024</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.attn_output.weight<span class="punctuation">,</span>   torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.ffn_gate.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.ffn_up.weight<span class="punctuation">,</span>        torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">14336</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>gguf<span class="punctuation">:</span> loading model part &#x27;pytorch_model<span class="number">-00004</span>-of<span class="number">-00004.</span>bin&#x27;</span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.ffn_down.weight<span class="punctuation">,</span>      torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">14336</span><span class="punctuation">,</span> <span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.attn_norm.weight<span class="punctuation">,</span>     torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>blk<span class="number">.31</span>.ffn_norm.weight<span class="punctuation">,</span>      torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>output_norm.weight<span class="punctuation">,</span>          torch.float16 --&gt; F32<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>output.weight<span class="punctuation">,</span>               torch.float16 --&gt; Q8_0<span class="punctuation">,</span> shape = <span class="punctuation">&#123;</span><span class="number">4096</span><span class="punctuation">,</span> <span class="number">128256</span><span class="punctuation">&#125;</span></span><br><span class="line">INFO<span class="punctuation">:</span>gguf.gguf_writer<span class="punctuation">:</span>Writing the following files<span class="punctuation">:</span></span><br><span class="line">INFO<span class="punctuation">:</span>gguf.gguf_writer<span class="punctuation">:</span>/content/model/unsloth.Q8_0.gguf<span class="punctuation">:</span> n_tensors = <span class="number">291</span><span class="punctuation">,</span> total_size = <span class="number">8.5</span>G</span><br><span class="line">Writing<span class="punctuation">:</span> <span class="number">100</span>%|██████████| <span class="number">8.53</span>G/<span class="number">8.53</span>G <span class="punctuation">[</span><span class="number">03</span><span class="punctuation">:</span><span class="number">58</span>&lt;<span class="number">00</span><span class="punctuation">:</span><span class="number">00</span><span class="punctuation">,</span> <span class="number">35.8</span>Mbyte/s<span class="punctuation">]</span></span><br><span class="line">INFO<span class="punctuation">:</span>hf-to-gguf<span class="punctuation">:</span>Model successfully exported to /content/model/unsloth.Q8_0.gguf</span><br><span class="line">Unsloth<span class="punctuation">:</span> ##### The current model auto adds a BOS token.</span><br><span class="line">Unsloth<span class="punctuation">:</span> ##### We removed it in GGUF&#x27;s chat template for you.</span><br><span class="line">Unsloth<span class="punctuation">:</span> Conversion completed! Output location<span class="punctuation">:</span> /content/model/unsloth.Q8_0.gguf</span><br></pre></td></tr></table></figure>
</li>
<li><p>将GGUF模型传输到Huggingface仓库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 Hugging Face Hub 的 create_repo 函数，用于创建一个新的模型仓库</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> create_repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 Hugging Face Hub 上创建一个新的模型仓库</span></span><br><span class="line">create_repo(<span class="string">&quot;caihaoran/fortunetelling1&quot;</span>, token=HUGGINGFACE_TOKEN, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将模型和分词器上传到 Hugging Face Hub 上的仓库</span></span><br><span class="line">model.push_to_hub_gguf(<span class="string">&quot;caihaoran/fortunetelling1&quot;</span>, tokenizer, token=HUGGINGFACE_TOKEN)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br></pre></td><td class="code"><pre><span class="line">Unsloth: <span class="comment">##### The current model auto adds a BOS token.</span></span><br><span class="line">Unsloth: <span class="comment">##### Your chat template has a BOS token. We shall remove it temporarily.</span></span><br><span class="line">Unsloth: Merging 4bit <span class="keyword">and</span> LoRA weights to 16bit...</span><br><span class="line">Unsloth: Will use up to <span class="number">4.29</span> out of <span class="number">12.67</span> RAM <span class="keyword">for</span> saving.</span><br><span class="line">Unsloth: Saving model... This might take <span class="number">5</span> minutes ...</span><br><span class="line"><span class="number">100</span>%|██████████| <span class="number">32</span>/<span class="number">32</span> [02:<span class="number">32</span>&lt;<span class="number">00</span>:<span class="number">00</span>,  <span class="number">4.76</span>s/it]</span><br><span class="line">Unsloth: Saving tokenizer... Done.</span><br><span class="line">Unsloth: Saving caihaoran/fortunetelling1/pytorch_model-00001-of-<span class="number">00004.</span><span class="built_in">bin</span>...</span><br><span class="line">Unsloth: Saving caihaoran/fortunetelling1/pytorch_model-00002-of-<span class="number">00004.</span><span class="built_in">bin</span>...</span><br><span class="line">Unsloth: Saving caihaoran/fortunetelling1/pytorch_model-00003-of-<span class="number">00004.</span><span class="built_in">bin</span>...</span><br><span class="line">Unsloth: Saving caihaoran/fortunetelling1/pytorch_model-00004-of-<span class="number">00004.</span><span class="built_in">bin</span>...</span><br><span class="line">Done.</span><br><span class="line">==((====))==  Unsloth: Conversion <span class="keyword">from</span> QLoRA to GGUF information</span><br><span class="line">   \\   /|    [<span class="number">0</span>] Installing llama.cpp might take <span class="number">3</span> minutes.</span><br><span class="line">O^O/ \_/ \    [<span class="number">1</span>] Converting HF to GGUF 16bits might take <span class="number">3</span> minutes.</span><br><span class="line">\        /    [<span class="number">2</span>] Converting GGUF 16bits to [<span class="string">&#x27;q8_0&#x27;</span>] might take <span class="number">10</span> minutes each.</span><br><span class="line"> <span class="string">&quot;-____-&quot;</span>     In total, you will have to wait at least <span class="number">16</span> minutes.</span><br><span class="line"></span><br><span class="line">Unsloth: Installing llama.cpp. This might take <span class="number">3</span> minutes...</span><br><span class="line">Unsloth: [<span class="number">1</span>] Converting model at caihaoran/fortunetelling1 into q8_0 GGUF <span class="built_in">format</span>.</span><br><span class="line">The output location will be /content/caihaoran/fortunetelling1/unsloth.Q8_0.gguf</span><br><span class="line">This might take <span class="number">3</span> minutes...</span><br><span class="line">INFO:hf-to-gguf:Loading model: fortunetelling1</span><br><span class="line">INFO:gguf.gguf_writer:gguf: This GGUF file <span class="keyword">is</span> <span class="keyword">for</span> Little Endian only</span><br><span class="line">INFO:hf-to-gguf:<span class="type">Set</span> model parameters</span><br><span class="line">INFO:hf-to-gguf:gguf: context length = <span class="number">131072</span></span><br><span class="line">INFO:hf-to-gguf:gguf: embedding length = <span class="number">4096</span></span><br><span class="line">INFO:hf-to-gguf:gguf: feed forward length = <span class="number">14336</span></span><br><span class="line">INFO:hf-to-gguf:gguf: head count = <span class="number">32</span></span><br><span class="line">INFO:hf-to-gguf:gguf: key-value head count = <span class="number">8</span></span><br><span class="line">INFO:hf-to-gguf:gguf: rope theta = <span class="number">500000.0</span></span><br><span class="line">INFO:hf-to-gguf:gguf: rms norm epsilon = <span class="number">1e-05</span></span><br><span class="line">INFO:hf-to-gguf:gguf: file <span class="built_in">type</span> = <span class="number">7</span></span><br><span class="line">INFO:hf-to-gguf:<span class="type">Set</span> model tokenizer</span><br><span class="line">INFO:numexpr.utils:NumExpr defaulting to <span class="number">2</span> threads.</span><br><span class="line">WARNING:gguf.vocab:Adding merges requested but no merges found, output may be non-functional.</span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> bos to <span class="number">128000</span></span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> eos to <span class="number">128001</span></span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> pad to <span class="number">128004</span></span><br><span class="line">INFO:gguf.vocab:Setting add_bos_token to <span class="literal">True</span></span><br><span class="line">INFO:gguf.vocab:Setting add_eos_token to <span class="literal">False</span></span><br><span class="line">INFO:gguf.vocab:Setting chat_template to &#123;% <span class="keyword">if</span> <span class="keyword">not</span> add_generation_prompt <span class="keyword">is</span> defined %&#125;&#123;% <span class="built_in">set</span> add_generation_prompt = false %&#125;&#123;% endif %&#125;&#123;% <span class="built_in">set</span> ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt=<span class="string">&#x27;&#x27;</span>) %&#125;&#123;%- <span class="keyword">for</span> message <span class="keyword">in</span> messages %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;system&#x27;</span> %&#125;&#123;% <span class="built_in">set</span> ns.system_prompt = message[<span class="string">&#x27;content&#x27;</span>] %&#125;&#123;%- endif %&#125;&#123;%- endfor %&#125;&#123;&#123;ns.system_prompt&#125;&#125;&#123;%- <span class="keyword">for</span> message <span class="keyword">in</span> messages %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;user&#x27;</span> %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = false -%&#125;&#123;&#123;<span class="string">&#x27;&lt;｜User｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>]&#125;&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;assistant&#x27;</span> <span class="keyword">and</span> message[<span class="string">&#x27;content&#x27;</span>] <span class="keyword">is</span> none %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = false -%&#125;&#123;%- <span class="keyword">for</span> tool <span class="keyword">in</span> message[<span class="string">&#x27;tool_calls&#x27;</span>]%&#125;&#123;%- <span class="keyword">if</span> <span class="keyword">not</span> ns.is_first %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&lt;｜tool▁calls▁begin｜&gt;&lt;｜tool▁call▁begin｜&gt;&#x27;</span> + tool[<span class="string">&#x27;type&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁sep｜&gt;&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```json&#x27;</span> + <span class="string">&#x27;\n&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;arguments&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_first = true -%&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;&#123;<span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁begin｜&gt;&#x27;</span> + tool[<span class="string">&#x27;type&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁sep｜&gt;&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```json&#x27;</span> + <span class="string">&#x27;\n&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;arguments&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁end｜&gt;&#x27;</span>&#125;&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁calls▁end｜&gt;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endfor %&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;assistant&#x27;</span> <span class="keyword">and</span> message[<span class="string">&#x27;content&#x27;</span>] <span class="keyword">is</span> <span class="keyword">not</span> none %&#125;&#123;%- <span class="keyword">if</span> ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = false -%&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;% <span class="built_in">set</span> content = message[<span class="string">&#x27;content&#x27;</span>] %&#125;&#123;% <span class="keyword">if</span> <span class="string">&#x27;&lt;/think&gt;&#x27;</span> <span class="keyword">in</span> content %&#125;&#123;% <span class="built_in">set</span> content = content.split(<span class="string">&#x27;&lt;/think&gt;&#x27;</span>)[-<span class="number">1</span>] %&#125;&#123;% endif %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&#x27;</span> + content + <span class="string">&#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;tool&#x27;</span> %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = true -%&#125;&#123;%- <span class="keyword">if</span> ns.is_output_first %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁begin｜&gt;&lt;｜tool▁output▁begin｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁output▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_output_first = false %&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;&#123;<span class="string">&#x27;\n&lt;｜tool▁output▁begin｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁output▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endif %&#125;&#123;%- endfor -%&#125;&#123;% <span class="keyword">if</span> ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27;</span>&#125;&#125;&#123;% endif %&#125;&#123;% <span class="keyword">if</span> add_generation_prompt <span class="keyword">and</span> <span class="keyword">not</span> ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&lt;think&gt;\n&#x27;</span>&#125;&#125;&#123;% endif %&#125;</span><br><span class="line">INFO:hf-to-gguf:Exporting model...</span><br><span class="line">INFO:hf-to-gguf:gguf: loading model weight <span class="built_in">map</span> <span class="keyword">from</span> <span class="string">&#x27;pytorch_model.bin.index.json&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:gguf: loading model part <span class="string">&#x27;pytorch_model-00001-of-00004.bin&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:token_embd.weight,           torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">128256</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.0</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.1</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.2</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.3</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.4</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.5</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.6</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.7</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.8</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:gguf: loading model part <span class="string">&#x27;pytorch_model-00002-of-00004.bin&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.attn_q.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.attn_k.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.attn_v.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.attn_output.weight,    torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.ffn_gate.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.ffn_up.weight,         torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.ffn_down.weight,       torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.attn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.9</span>.ffn_norm.weight,       torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.10</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.11</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.12</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.13</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.14</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.15</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.16</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.17</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.18</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.19</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:gguf: loading model part <span class="string">&#x27;pytorch_model-00003-of-00004.bin&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.20</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.21</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.22</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.23</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.24</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.25</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.26</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.27</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.28</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.29</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.30</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.attn_q.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.attn_k.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.attn_v.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">1024</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.attn_output.weight,   torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.ffn_gate.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.ffn_up.weight,        torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">14336</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:gguf: loading model part <span class="string">&#x27;pytorch_model-00004-of-00004.bin&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.ffn_down.weight,      torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">14336</span>, <span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.attn_norm.weight,     torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:blk<span class="number">.31</span>.ffn_norm.weight,      torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:output_norm.weight,          torch.float16 --&gt; F32, shape = &#123;<span class="number">4096</span>&#125;</span><br><span class="line">INFO:hf-to-gguf:output.weight,               torch.float16 --&gt; Q8_0, shape = &#123;<span class="number">4096</span>, <span class="number">128256</span>&#125;</span><br><span class="line">INFO:gguf.gguf_writer:Writing the following files:</span><br><span class="line">INFO:gguf.gguf_writer:/content/caihaoran/fortunetelling1/unsloth.Q8_0.gguf: n_tensors = <span class="number">291</span>, total_size = <span class="number">8.5</span>G</span><br><span class="line">Writing: <span class="number">100</span>%|██████████| <span class="number">8.53</span>G/<span class="number">8.53</span>G [03:<span class="number">16</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">43.4</span>Mbyte/s]</span><br><span class="line">INFO:hf-to-gguf:Model successfully exported to /content/caihaoran/fortunetelling1/unsloth.Q8_0.gguf</span><br><span class="line">Unsloth: Conversion completed! Output location: /content/caihaoran/fortunetelling1/unsloth.Q8_0.gguf</span><br><span class="line">Unsloth: Uploading GGUF to Huggingface Hub...</span><br><span class="line"><span class="number">100</span>%</span><br><span class="line"> <span class="number">1</span>/<span class="number">1</span> [01:<span class="number">18</span>&lt;<span class="number">00</span>:<span class="number">00</span>, <span class="number">78.21</span>s/it]</span><br><span class="line">unsloth.Q8_0.gguf: </span><br><span class="line"> <span class="number">8.54</span>G/? [01:<span class="number">17</span>&lt;<span class="number">00</span>:<span class="number">00</span>, 383MB/s]</span><br><span class="line">Unsloth: <span class="comment">##### The current model auto adds a BOS token.</span></span><br><span class="line">Unsloth: <span class="comment">##### We removed it in GGUF&#x27;s chat template for you.</span></span><br><span class="line">Saved GGUF to https://huggingface.co/caihaoran/fortunetelling1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>我转化成GGUF的模型在ollama中运行不起来（Error:unable to load model: usr&#x2F;share&#x2F;ollama&#x2F;.ollama&#x2F;models&#x2F;blobs&#x2F;sha256-…）</p>
</blockquote>
</li>
</ol>
<hr>
<h3 id="本地Jupyter-notebook运行"><a href="#本地Jupyter-notebook运行" class="headerlink" title="本地Jupyter notebook运行"></a>本地Jupyter notebook运行</h3><p>上面的llama.cpp整了好几次没正好，T4 GPU运行时给弄限额了，还是回到ubuntu本机的Jupyter notebook运行吧，运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mkdir fortunetelling</span><br><span class="line">cd fortunetelling</span><br><span class="line">conda create -n unsloth python=<span class="number">3.11</span></span><br><span class="line">conda activate unsloth</span><br><span class="line">pip install unsloth datasets trl bitsandbytes unsloth_zoo</span><br><span class="line">pip install Xformer  <span class="comment"># 很重要，小友可以试下先不安装这个库进行实验🤨（顺便猜猜为什么我知道它很重要）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 给conda环境创建特殊内核</span></span><br><span class="line">conda install ipykernel</span><br><span class="line">python -m ipykernel install --user --name unsloth --display-name <span class="string">&quot;Python (unsloth)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回到base环境</span></span><br><span class="line">jupyter kernelspec <span class="built_in">list</span> <span class="comment"># 即可看到unsloth</span></span><br><span class="line">jupyter notebook  <span class="comment"># 启动jupyter notebook </span></span><br></pre></td></tr></table></figure>

<p>好的，将代码复制过来<br>但是运行第一段下载8B模型的时候报错：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line">OSError                                   Traceback (most recent call last)</span><br><span class="line">Cell In[1], line 9</span><br><span class="line">      6 load_in_4bit = True  <span class="comment"># 使用4位量化来节省内存，就像把大箱子压缩成小箱子</span></span><br><span class="line">      8 <span class="comment"># 加载预训练模型，并获取tokenizer工具</span></span><br><span class="line">----&gt; 9 model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">     10     model_name=<span class="string">&quot;unsloth/DeepSeek-R1-Distill-Llama-8B&quot;</span>,  <span class="comment"># 指定要加载的模型名称</span></span><br><span class="line">     11     max_seq_length=max_seq_length,  <span class="comment"># 使用前面设置的最大长度</span></span><br><span class="line">     12     dtype=dtype,  <span class="comment"># 使用前面设置的数据类型</span></span><br><span class="line">     13     load_in_4bit=load_in_4bit,  <span class="comment"># 使用4位量化</span></span><br><span class="line">     14     <span class="comment"># token=&quot;hf_...&quot;,  # 如果需要访问授权模型，可以在这里填入密钥</span></span><br><span class="line">     15 )</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/models/loader.py:363, <span class="keyword">in</span> FastLanguageModel.from_pretrained(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)</span><br><span class="line">    360     pass</span><br><span class="line">    361 pass</span><br><span class="line">--&gt; 363 model, tokenizer = dispatch_model.from_pretrained(</span><br><span class="line">    364     model_name        = model_name,</span><br><span class="line">    365     max_seq_length    = max_seq_length,</span><br><span class="line">    366     dtype             = _get_dtype(dtype),</span><br><span class="line">    367     load_in_4bit      = load_in_4bit,</span><br><span class="line">    368     token             = token,</span><br><span class="line">    369     device_map        = device_map,</span><br><span class="line">    370     rope_scaling      = rope_scaling,</span><br><span class="line">    371     fix_tokenizer     = fix_tokenizer,</span><br><span class="line">    372     model_patcher     = dispatch_model,</span><br><span class="line">    373     tokenizer_name    = tokenizer_name,</span><br><span class="line">    374     trust_remote_code = trust_remote_code,</span><br><span class="line">    375     revision          = revision <span class="keyword">if</span> not is_peft <span class="keyword">else</span> None,</span><br><span class="line">    376 </span><br><span class="line">    377     fast_inference    = fast_inference,</span><br><span class="line">    378     gpu_memory_utilization = gpu_memory_utilization,</span><br><span class="line">    379     float8_kv_cache   = float8_kv_cache,</span><br><span class="line">    380     random_state      = random_state,</span><br><span class="line">    381     max_lora_rank     = max_lora_rank,</span><br><span class="line">    382     disable_log_stats = disable_log_stats,</span><br><span class="line">    383     *args, **kwargs,</span><br><span class="line">    384 )</span><br><span class="line">    386 <span class="keyword">if</span> resize_model_vocab is not None:</span><br><span class="line">    387     model.resize_token_embeddings(resize_model_vocab)</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/unsloth/models/llama.py:1780, <span class="keyword">in</span> FastLlamaModel.from_pretrained(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)</span><br><span class="line">   1777 <span class="keyword">if</span> load_in_4bit: kwargs[<span class="string">&quot;quantization_config&quot;</span>] = bnb_config</span><br><span class="line">   1779 <span class="keyword">if</span> not fast_inference:</span><br><span class="line">-&gt; 1780     model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">   1781         model_name,</span><br><span class="line">   1782         device_map              = device_map,</span><br><span class="line">   1783         torch_dtype             = dtype,</span><br><span class="line">   1784         <span class="comment"># quantization_config     = bnb_config,</span></span><br><span class="line">   1785         token                   = token,</span><br><span class="line">   1786         max_position_embeddings = max_position_embeddings,</span><br><span class="line">   1787         trust_remote_code       = trust_remote_code,</span><br><span class="line">   1788         attn_implementation     = <span class="string">&quot;eager&quot;</span>,</span><br><span class="line">   1789         **kwargs,</span><br><span class="line">   1790     )</span><br><span class="line">   1791     model.fast_generate = model.generate</span><br><span class="line">   1792     model.fast_generate_batches = None</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:573, <span class="keyword">in</span> _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)</span><br><span class="line">    571 <span class="keyword">elif</span> <span class="built_in">type</span>(config) <span class="keyword">in</span> cls._model_mapping.keys():</span><br><span class="line">    572     model_class = _get_model_class(config, cls._model_mapping)</span><br><span class="line">--&gt; 573     <span class="built_in">return</span> model_class.from_pretrained(</span><br><span class="line">    574         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs</span><br><span class="line">    575     )</span><br><span class="line">    576 raise ValueError(</span><br><span class="line">    577     f<span class="string">&quot;Unrecognized configuration class &#123;config.__class__&#125; for this kind of AutoModel: &#123;cls.__name__&#125;.\n&quot;</span></span><br><span class="line">    578     f<span class="string">&quot;Model type should be one of &#123;&#x27;, &#x27;.join(c.__name__ for c in cls._model_mapping.keys())&#125;.&quot;</span></span><br><span class="line">    579 )</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/transformers/modeling_utils.py:272, <span class="keyword">in</span> restore_default_torch_dtype.&lt;locals&gt;._wrapper(*args, **kwargs)</span><br><span class="line">    270 old_dtype = torch.get_default_dtype()</span><br><span class="line">    271 try:</span><br><span class="line">--&gt; 272     <span class="built_in">return</span> func(*args, **kwargs)</span><br><span class="line">    273 finally:</span><br><span class="line">    274     torch.set_default_dtype(old_dtype)</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/transformers/modeling_utils.py:4317, <span class="keyword">in</span> PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)</span><br><span class="line">   4312 <span class="keyword">if</span> gguf_file is not None and hf_quantizer is not None:</span><br><span class="line">   4313     raise ValueError(</span><br><span class="line">   4314         <span class="string">&quot;You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.&quot;</span></span><br><span class="line">   4315     )</span><br><span class="line">-&gt; 4317 checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(</span><br><span class="line">   4318     pretrained_model_name_or_path=pretrained_model_name_or_path,</span><br><span class="line">   4319     subfolder=subfolder,</span><br><span class="line">   4320     variant=variant,</span><br><span class="line">   4321     gguf_file=gguf_file,</span><br><span class="line">   4322     from_tf=from_tf,</span><br><span class="line">   4323     from_flax=from_flax,</span><br><span class="line">   4324     use_safetensors=use_safetensors,</span><br><span class="line">   4325     cache_dir=cache_dir,</span><br><span class="line">   4326     force_download=force_download,</span><br><span class="line">   4327     proxies=proxies,</span><br><span class="line">   4328     local_files_only=local_files_only,</span><br><span class="line">   4329     token=token,</span><br><span class="line">   4330     user_agent=user_agent,</span><br><span class="line">   4331     revision=revision,</span><br><span class="line">   4332     commit_hash=commit_hash,</span><br><span class="line">   4333 )</span><br><span class="line">   4335 is_sharded = sharded_metadata is not None</span><br><span class="line">   4336 is_quantized = hf_quantizer is not None</span><br><span class="line"></span><br><span class="line">File ~/anaconda3/envs/unsloth/lib/python3.11/site-packages/transformers/modeling_utils.py:1110, <span class="keyword">in</span> _get_resolved_checkpoint_files(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)</span><br><span class="line">   1102 has_file_kwargs = &#123;</span><br><span class="line">   1103     <span class="string">&quot;revision&quot;</span>: revision,</span><br><span class="line">   1104     <span class="string">&quot;proxies&quot;</span>: proxies,</span><br><span class="line">   (...)</span><br><span class="line">   1107     <span class="string">&quot;local_files_only&quot;</span>: local_files_only,</span><br><span class="line">   1108 &#125;</span><br><span class="line">   1109 <span class="keyword">if</span> has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):</span><br><span class="line">-&gt; 1110     raise EnvironmentError(</span><br><span class="line">   1111         f<span class="string">&quot;&#123;pretrained_model_name_or_path&#125; does not appear to have a file named&quot;</span></span><br><span class="line">   1112         f<span class="string">&quot; &#123;_add_variant(WEIGHTS_NAME, variant)&#125; but there is a file for TensorFlow weights.&quot;</span></span><br><span class="line">   1113         <span class="string">&quot; Use `from_tf=True` to load this model from those weights.&quot;</span></span><br><span class="line">   1114     )</span><br><span class="line">   1115 <span class="keyword">elif</span> has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):</span><br><span class="line">   1116     raise EnvironmentError(</span><br><span class="line">   1117         f<span class="string">&quot;&#123;pretrained_model_name_or_path&#125; does not appear to have a file named&quot;</span></span><br><span class="line">   1118         f<span class="string">&quot; &#123;_add_variant(WEIGHTS_NAME, variant)&#125; but there is a file for Flax weights. Use&quot;</span></span><br><span class="line">   1119         <span class="string">&quot; `from_flax=True` to load this model from those weights.&quot;</span></span><br><span class="line">   1120     )</span><br><span class="line"></span><br><span class="line">OSError: unsloth/deepseek-r1-distill-llama-8b-unsloth-bnb-4bit does not appear to have a file named pytorch_model.bin but there is a file <span class="keyword">for</span> TensorFlow weights. Use `from_tf=True` to load this model from those weights.</span><br></pre></td></tr></table></figure>

<p>叽里呱啦的说一大堆，看着是下载的模型与想要的模型格式不匹配，但是我花时间下载的模型文件呢！不能用就给我删了！？算了，我直接git下载模型，然后指定本地模型路径得了，在<code>fortunetelling</code>文件夹下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B</span><br></pre></td></tr></table></figure>
<p>下载完成后，将<code>model_name</code>改成<code>./DeepSeek-R1-Distill-Llama-8B</code>，由于Jupyter Notebook不像Colab可以方便的设置秘钥，所以最后的代码需要显式指定（我是这么做的）HUGGINGFACE_TOKEN，好的，还有最后一个大坑，保存微调后的模型文件需要使用<code>llama.cpp</code>的<code>llama.cpp/quantize</code>，但新版本的<code>Llama.cpp</code>库没有quantize（不在一级目录下）,解决方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rm</span> -rf llama.cpp  <span class="comment"># 删除掉现在的llama.cpp</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/ggml-org/llama.cpp.git</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br><span class="line">git checkout b3345</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line">make clean</span><br><span class="line">make all -j</span><br><span class="line">git <span class="built_in">log</span> -1</span><br><span class="line"></span><br><span class="line">就畅通无阻了</span><br></pre></td></tr></table></figure>

<blockquote>
<p>我这里转化成GGUF的模型在ollama中也运行不起来（Error:unable to load model: usr&#x2F;share&#x2F;ollama&#x2F;.ollama&#x2F;models&#x2F;blobs&#x2F;sha256-…）(彻底疯狂.gif)</p>
</blockquote>
<hr>
<h3 id="vLLM运行"><a href="#vLLM运行" class="headerlink" title="vLLM运行"></a>vLLM运行</h3><p>好好好，怎么整在<code>ollama</code>中都运行不起来，但是下载大佬的<code>GGUF</code>模型却能运行起来，没招了，我还是用<code>vLLM</code>运行它吧。</p>
<p>鉴于我在4090上如果使用8B的Llama模型，只能设置<code>--max-model-len=1024</code>，所以我又微调了个<code>Qwen2.5 7b</code>，知识模型变了下，其他都没变，在代码最后加上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained_merged(<span class="string">&quot;model_vllm_qwen&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_16bit&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>用来保存微调后的完整模型，用于vLLM方式启动服务端：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server --model model_vllm_qwen  --served-model-name Qwen --<span class="built_in">max</span>-model-<span class="built_in">len</span>=<span class="number">4096</span></span><br></pre></td></tr></table></figure>

<p>下载<a target="_blank" rel="noopener" href="https://cherry-ai.com/download">CHERRY STUDIO</a>，然后：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210656222.png" class="" title="image-20250328210656222">

<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210753041.png" class="" title="image-20250328210753041">

<p>点击确定后，API秘钥随意（我填的是EMPTY)，API地址是<a target="_blank" rel="noopener" href="http://192.168.0.138:8000/">http://192.168.0.138:8000</a>(根据自己的来)，点击添加，写入模型ID，即为你服务端指定的<code>--served-model-name</code></p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328210939841.png" class="" title="image-20250328210939841">

<p>回到对话栏，就可以选择刚才添加的模型了：</p>
<img src="/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/image-20250328211431024.png" class="" title="image-20250328211431024">

<p>现在就可以愉快的交流了，那么看看咱们微调的对比效果吧：</p>
<p><strong>原始</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">Q<span class="punctuation">:</span><span class="number">1992</span>年闰四月初九巳时生人，女，想了解健康运势</span><br><span class="line"></span><br><span class="line">A<span class="punctuation">:</span>好，我现在要帮用户分析一下她<span class="number">1992</span>年闰四月初九巳时生的健康运势。首先，用户是女性，所以可能需要关注一些女性特有的健康问题。闰四月这个时间点在农历中比较特殊，可能会对她的身体健康产生影响。</span><br><span class="line"></span><br><span class="line">首先，我应该考虑她的出生时间，闰四月初九巳时，这可能意味着她的生日在公历的<span class="number">4</span>月<span class="number">9</span>日，或者在农历闰四月的某一天。我需要明确她的生日到底是哪一天，但可能用户只是大致的时间，所以可能需要根据农历转换成公历日期。</span><br><span class="line"></span><br><span class="line">接下来，我应该分析她的五行。她的出生时间是巳时，也就是下午<span class="number">4</span>点左右，属于巳时。在五行中，巳对应的地支是已，已为土。所以她的五行可能以土为主，但还需要结合具体的日干来判断。<span class="number">1992</span>年是龙年，地支是龙，对应的日干是木。所以她的五行应该是木 dominant，可能带有火的性质，因为龙年通常有木和火的元素。</span><br><span class="line"></span><br><span class="line">五行方面，木生火，火生土，土生金，金生水，水生木。所以木的五行比较旺，可能她的身体在木相关的方面较强，比如肝木、肺火等。但五行过旺也可能带来一些问题，比如容易上火、口干舌燥等。</span><br><span class="line"></span><br><span class="line">接下来，我需要分析她的命宫和宫位。四月初九，农历四月是春季，四月初九可能在清明节前后，这是一个比较重要的节气，可能影响她的健康。四月初九属于四月的中下旬，可能与肝胆有关，因为四月在农历中对应的是胆和肝。</span><br><span class="line"></span><br><span class="line">她属于闰年中的初九，可能她的身体状况在春天容易出现一些问题，比如消化不良、容易上火等。作为女性，她的月经周期也可能受到影响，需要特别注意。</span><br><span class="line"></span><br><span class="line">接下来，我应该考虑她的五行相克。木生火，所以她的火性可能比较强，容易导致口干舌燥、容易生气、情绪波动大等问题。土生金，所以她的金属性也可能较强，可能在学习、工作效率上有优势，但可能在某些方面容易固执。</span><br><span class="line"></span><br><span class="line">水生木，水比较弱的话，可能在她的身体上会有一定的滋养作用，帮助调节木旺带来的问题。金生水，可能她的金元素比较弱，容易导致情绪低落、容易焦虑等。</span><br><span class="line"></span><br><span class="line">在健康运势方面，可能需要注意以下几点：</span><br><span class="line"><span class="number">1.</span> 生活作息：要注意规律，避免熬夜，尤其是晚上容易上火的时候，可以适当减少电子设备的使用时间。</span><br><span class="line"><span class="number">2.</span> 饮食调理：多吃清淡的食物，避免油腻和辛辣，多喝水，保持一定的水分，避免干燥。</span><br><span class="line"><span class="number">3.</span> 气候变化：春天容易变化无常，注意保暖，避免着凉，特别是在晚上。</span><br><span class="line"><span class="number">4.</span> 作息时间：保证充足的睡眠，特别是在晚上容易上火的时候，可以适当减少咖啡因和糖分的摄入。</span><br><span class="line"><span class="number">5.</span> 环境调整：尽量保持居住环境的干燥，避免潮湿和异味，可能需要使用一些除湿设备。</span><br><span class="line"></span><br><span class="line">此外，作为女性，她的月经周期可能会受到影响，需要特别关注自己的身体状态，避免过度劳累或情绪波动过大。如果有不适，可能需要及时就医咨询。</span><br><span class="line"></span><br><span class="line">总的来说，她的健康运势可能以木旺为主，需要注意调节火性，保持身体的平衡。同时，注意情绪的调节，避免长期情绪低落，可能会对身体健康产生负面影响。</span><br><span class="line"></span><br><span class="line">最后，我应该提醒用户，健康运势仅供参考，遇到身体不适时，应尽早就医，不要延误治疗。同时，保持良好的生活习惯，注意饮食和作息，对整体健康有积极的影响。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">根据你<span class="number">1992</span>年闰四月初九巳时的出生时间，以下是针对你的健康运势的分析和建议：</span><br><span class="line"></span><br><span class="line">### 健康运势分析：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **五行分析**：</span><br><span class="line">   - 你的五行以木为主，木生火，火生土，土生金，金生水，水生木。木旺可能带来消化不良、口干舌燥等问题，但水的滋养作用可以调节木旺带来的不适。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **命宫分析**：</span><br><span class="line">   - 闰四月初九属于四月，四月在农历中对应胆和肝。作为女性，你可能在春季容易出现消化不良或情绪波动。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **五行相克**：</span><br><span class="line">   - 火性较强，可能引起口干舌燥、生气等不适。</span><br><span class="line">   - 土性较弱，可能影响情绪稳定性，容易出现焦虑。</span><br><span class="line"></span><br><span class="line">### 健康建议：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **作息调整**：</span><br><span class="line">   - 注意规律作息，避免熬夜，尤其是晚上容易上火时，减少电子设备使用时间。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **饮食调理**：</span><br><span class="line">   - 多吃清淡食物，避免油腻和辛辣，保持适量饮水，保持身体湿润。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **气候适应**：</span><br><span class="line">   - 春天气候多变，注意保暖，避免着凉。晚上尽量减少空调使用，保持舒适环境。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> **作息安排**：</span><br><span class="line">   - 保证充足睡眠，晚上减少咖啡因和糖分摄入，帮助身体调节。</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> **环境调整**：</span><br><span class="line">   - 保持居住环境干燥，使用除湿设备，避免潮湿和异味。</span><br><span class="line"></span><br><span class="line">### 情绪建议：</span><br><span class="line"></span><br><span class="line">- 注意情绪调节，避免长期情绪低落，可能影响身体健康。保持乐观心态，必要时寻求心理咨询。</span><br><span class="line"></span><br><span class="line">### 注意事项：</span><br><span class="line"></span><br><span class="line">- 女性月经周期可能受到影响，注意观察身体状况，避免过度劳累。</span><br><span class="line"></span><br><span class="line">### 总结：</span><br><span class="line"></span><br><span class="line">你的健康运势以木旺为主，需要注意调节火性，保持身体平衡。同时，注意作息和饮食，避免上火，保持良好的心态。如出现不适，及时就医咨询。</span><br><span class="line"></span><br><span class="line">健康运势仅供参考，生活中的健康问题请以专业医生的诊断为准。</span><br></pre></td></tr></table></figure>

<p><strong>后</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">好的，用户提供了出生日期和时辰，想知道健康运势。首先，我需要确认<span class="number">1992</span>年的闰四月初九巳时是否符合传统命理分析。闰年、月份、日期和时辰都是关键因素。</span><br><span class="line"></span><br><span class="line">接下来，我需要查一下<span class="number">1992</span>年的农历四月初九的具体日期对应的公历日期，以及当时的时辰转换成现代时间，以便更好地分析。闰四月意味着这一年有<span class="number">13</span>个月，可能会影响月份的节气和重要日期。</span><br><span class="line"></span><br><span class="line">然后，考虑八字排盘。需要将阳历日期转换成农历，再排出年柱、月柱、日柱和时柱。年柱是甲申年，月柱是丙辰月，日柱需要根据具体的公历日期转换成农历，再查对应的天干地支。时辰是巳时，对应的地支是巳，需要转换成<span class="number">24</span>小时制的时辰，方便排四柱。</span><br><span class="line"></span><br><span class="line">接下来，分析五行。甲木日主，日主偏强，可能需要看是否有用神。月柱丙火，丙火为阴火，需看是否得时令火生。时柱巳时为午时，午时对应的五行属土，可能与日主木相合，但需要看具体的生克关系。</span><br><span class="line"></span><br><span class="line">健康方面，甲木为根，木为阴，可能有湿热或缺水的问题。要注意饮食、作息和情绪管理。日主偏强，可能有压力或情绪波动，需注意情绪调节。月柱丙火，火为阴，可能有阴气过重，需注意保暖。时柱土，土为阳，可能有疲劳或消化问题，需注意劳逸结合。</span><br><span class="line"></span><br><span class="line">需要提醒用户，命理分析仅供参考，需结合实际生活调整。同时，建议咨询专业 astrologer 进行更详细的解读。</span><br><span class="line"></span><br><span class="line">最后，整理这些信息，用中文口语化的方式表达，保持自然流畅，结构清晰，分点说明。注意避免使用专业术语过多，让用户容易理解。确保内容准确，同时保持积极鼓励的态度，让用户感到被关心和支持。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">**<span class="number">1992</span>年闰四月初九巳时女命健康运势解析**</span><br><span class="line"></span><br><span class="line">### **八字排盘**</span><br><span class="line">**年柱：甲申年**（阳干甲木，阴支申金）</span><br><span class="line">**月柱：丙辰月**（阳支辰土，阴支辰土）</span><br><span class="line">**日柱：**（需根据公历日期转换成农历，再排）</span><br><span class="line">**时柱：巳时**（阳时火，阴时火）</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **五行平衡分析**</span><br><span class="line"><span class="number">1.</span> **日主（甲木）**：</span><br><span class="line">   甲木为根，偏强，需火生木。月柱（丙辰土）为阴土，土生木，但土过强，木被土耗尽，形成“火土相克”的关系。建议注意情绪管理，避免过度焦虑或压力。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **月支（辰土）**：</span><br><span class="line">   土为阴性，土为阴气过重，需用火来生土。时柱（巳火）为阳火，火旺生土，但火过旺，土被火耗尽，形成“木土相克”的关系。注意防寒保暖，避免久坐或久站。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **时柱（巳火）**：</span><br><span class="line">   火为阳，火为阳气过旺，需用土来克火。日柱（甲木）为木，木生火，但木过弱，火被木耗尽，形成“火木相克”的关系。建议适量运动，保持身体活性。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **健康提示**</span><br><span class="line"><span class="number">1.</span> **饮食方面**：</span><br><span class="line">   - **注意事项**：避免过热或油腻食物，尤其是午间（火旺时）不宜过多进补。</span><br><span class="line">   - **建议**：多摄入新鲜蔬菜水果（木生火，火生土），如胡萝卜、苹果、绿叶菜等。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **作息方面**：</span><br><span class="line">   - **注意事项**：午间（<span class="number">13</span><span class="number">-15</span>点）避免剧烈运动或久坐，以免火气耗尽。</span><br><span class="line">   - **建议**：午间可以进行轻微拉伸或深呼吸，增强气机平衡。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **情绪管理**：</span><br><span class="line">   - **注意事项**：注意情绪波动，避免因压力过大的情绪耗尽木气。</span><br><span class="line">   - **建议**：每天安排<span class="number">15</span><span class="number">-30</span>分钟的冥想或深呼吸练习，保持心身平衡。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> **禁忌**：</span><br><span class="line">   - 避免在火旺的时段（如正午）进行剧烈运动、烹饪或使用电子设备。</span><br><span class="line">   - 避免穿过于紧身或厚重的衣服，以免耗尽土气。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **综合建议**</span><br><span class="line"><span class="number">1.</span> **日常防护**：</span><br><span class="line">   - 饮食宜清淡，多食清热解毒的食物（如菊花茶、苦瓜、银耳）。</span><br><span class="line">   - 保持充足的睡眠（每晚<span class="number">11</span>点前入睡），避免熬夜。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **重要节点**：</span><br><span class="line">   - **农历初九**：此日为“冲日”，注意防风防寒。</span><br><span class="line">   - **农历四月初九**：此日为“四正”，宜选择阴凉处避暑，避免阳气过旺。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **重要时间**：</span><br><span class="line">   - **卯时（<span class="number">6</span><span class="number">-8</span>点）**：卯时属土，土生木，此时宜做有助于木气提升的活动（如太极拳、打坐）。</span><br><span class="line">   - **巳时（<span class="number">13</span><span class="number">-15</span>点）**：巳时属火，火耗木，此时宜避免剧烈运动，可做轻微拉伸。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">**注**：以上分析基于传统命理理论，具体健康问题还需结合个人实际情况和专业医疗建议。</span><br></pre></td></tr></table></figure>

<p><strong>前</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">大门对着路有忌讳吗</span><br><span class="line">嗯，大门对着路，这在风水学上是不是有什么忌讳呢？首先，我需要了解一下大门对着路在中国传统文化中的意义。在中国，大门象征着家庭的 entrance 和迎接的 start of the day. 对着路，可能意味着家庭与外界的连接。但是，从风水的角度来看，这可能涉及到气场的流动和能量的平衡。</span><br><span class="line"></span><br><span class="line">我记得风水学中提到，大门是家庭能量的入口，如果大门对着路，可能会导致气场外泄，影响家庭的内部环境和成员的健康。此外，路象征着外面的世界和外界的干扰，如果大门直接对着路，可能会让家庭成员感到孤独或缺乏安全感。</span><br><span class="line"></span><br><span class="line">另外，我还听说过“阳气过旺”和“气场不稳定”的说法，这可能与大门对着路有关。阳气过旺可能会影响家庭成员的健康，甚至导致一些不吉利的事情发生。</span><br><span class="line"></span><br><span class="line">不过，我也听说过“聚气”和“纳气”的概念，如果大门对着路，可能会影响家庭成员的运势和事业。聚气指的是家庭成员聚集的气场，而纳气指的是家庭成员吸收外界能量的能力。如果大门对着路，可能会影响家庭成员的运势，甚至导致一些问题。</span><br><span class="line"></span><br><span class="line">另外，我还想了解一下，是否有其他风水师建议，说大门对着路是不吉利的，还是有人认为这是正常的。可能还需要查阅一些风水书籍或咨询专业的风水师，以获得更准确的信息。</span><br><span class="line"></span><br><span class="line">总的来说，我觉得大门对着路在中国传统文化中可能有一定的忌讳，但具体情况还需要根据家庭的实际情况来判断。如果家庭成员对此有疑虑，可以考虑调整大门的位置，以达到更好的风水效果。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">在中国传统文化中，大门是对家庭能量的象征，象征着家庭的迎接和外部的连接。一般来说，大门对着路可能会被认为有一定的忌讳，因为这可能导致气场外泄，影响家庭的内部环境和成员的健康。以下是一些具体的解释和建议：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> **气场流动**：</span><br><span class="line">   - 大门对着路，可能让气场更容易外泄，影响到家庭内部的和谐与平衡。气场的外泄可能导致成员感到孤独、不安，甚至影响到他们的健康状况。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **外部干扰**：</span><br><span class="line">   - 大门对着路，意味着家庭外部的干扰可能更多，这可能让家庭成员感到外界的不稳定和不安全。这对于需要外界支持的家庭成员来说，可能是一种压力。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **聚气与纳气**：</span><br><span class="line">   - 聚气指的是家庭成员聚集的气场，而纳气指的是家庭成员吸收外界能量的能力。大门对着路可能会影响家庭成员的运势和事业，因为这可能导致气场的不稳定，影响他们的运势。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> **风水建议**：</span><br><span class="line">   - 如果家庭的大门确实对着路，可以考虑在大门前放置一些有助于稳定气场的物品，如植物、镜子等，以减少气场外泄的影响。</span><br><span class="line">   - 另外，还可以通过调整室内的气场布局，如使用风水物品、悬挂 appropriate 的书法作品等，来改善家庭的气场环境。</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> **家庭成员的健康**：</span><br><span class="line">   - 对于家庭成员的健康来说，大门对着路可能会影响他们的身体健康。如果家庭成员对此有疑虑，可以考虑调整大门的位置，以达到更好的风水效果。</span><br><span class="line"></span><br><span class="line">总之，虽然大门对着路在中国传统文化中可能有一些忌讳，但具体情况还需要根据家庭的实际情况来判断。如果家庭成员对此有疑虑，可以咨询专业的风水师，以获得更准确的建议。</span><br></pre></td></tr></table></figure>

<p><strong>后</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">好的，用户询问的是“大门对着路有忌讳吗”。首先，我需要理解用户的问题背景。用户可能在选择房屋布局或者 house design 时遇到了问题，想知道大门对着路是否符合传统风水原则。这可能涉及到对风水知识的不了解，或者他们正在做室内设计，想要优化空间布局。</span><br><span class="line"></span><br><span class="line">接下来，我需要回忆一下风水中关于大门的方向和朝向的指导。一般来说，大门应该朝南，因为南方属阳气，与男性的阳气相合，有助于驱邪纳气。如果大门对路，可能会影响气场的流动。需要解释这一点，同时也要提到不同文化或地方的差异，说明风水学是地方性的，可能因地区而异。</span><br><span class="line"></span><br><span class="line">然后，要分析大门对路的具体影响。比如，如果大门正对街道，可能会导致财气外泄，不利财运。或者，如果对的是正门，可能需要采取补救措施，比如设置屏风、安装植物等，来化解煞气。同时，还要提到现代风水中的一些新观点，比如传统风水与现代建筑的关系，提醒用户现代设计中可能需要结合实际需求。</span><br><span class="line"></span><br><span class="line">还要考虑用户可能的深层需求。他们可能担心自己的风水布局是否正确，或者对风水有误解，需要进一步解释。另外，用户可能对风水有一定的兴趣，但不确定具体的禁忌，所以需要提供实用的建议，而不仅仅是理论。</span><br><span class="line"></span><br><span class="line">需要注意的是，避免绝对化的说法，比如“绝对忌讳”或“绝对没问题”，因为风水学是传统文化的一部分，因人而异，需要结合个人情况。同时，要保持中立，不鼓励用户迷信，但提供合理的解释和建议。</span><br><span class="line"></span><br><span class="line">最后，要确保回答结构清晰，分点说明问题、原因、建议，以及注意事项，让用户能够一目了然。还要注意用词要通俗易懂，避免使用过于专业的术语，但又不失准确性。</span><br><span class="line"></span><br><span class="line">总结一下，回答应包括以下几点：</span><br><span class="line"><span class="number">1.</span> 传统风水中大门对路的忌讳。</span><br><span class="line"><span class="number">2.</span> 原因分析（如财气外泄）。</span><br><span class="line"><span class="number">3.</span> 解决方法（如设置屏风、植物等）。</span><br><span class="line"><span class="number">4.</span> 现代视角下的应对策略。</span><br><span class="line"><span class="number">5.</span> 需要结合实际，避免绝对化。</span><br><span class="line"><span class="number">6.</span> 鼓励科学理性，避免迷信。</span><br><span class="line"></span><br><span class="line">这样既满足了用户的疑问，又提供了实用的建议，同时保持了中立和科学的态度。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">在中国传统文化中，风水学确实对大门的方向和朝向有特定的指导原则，但需要注意的是，风水学更多是地方性和文化背景的习俗，具体是否“忌讳”需要结合个人命理、空间布局以及地方习俗来综合判断。以下是一些常见的风水观念，供参考：</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **传统风水观点**</span><br><span class="line"><span class="number">1.</span> **大门对路是否忌讳？**</span><br><span class="line">   - 在传统风水中，大门的方向和朝向与“气场”流动有关。如果大门正对街道（即“对路”），可能会形成“通途”（通路）的概念，认为这种方向可能带来“财气外泄”或“煞气冲出”，对居住者的运势产生负面影响。</span><br><span class="line">   - 但需要注意的是，这种说法更多是针对“起居”“办公”等特定用途的房间，而非“大门”本身。大门的主要作用是引导气流进入室内，对“通路”（即街道）的指向性可能有不同的解释。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **具体原因：**</span><br><span class="line">   - **财气外泄**：大门正对街道可能被认为“财气外泄”，尤其是对正对的正门（即街道方向），容易导致财富、资源外流，影响运势。</span><br><span class="line">   - **煞气冲出**：如果大门正对正门，可能形成“煞气冲出”，尤其是如果大门正对正门，且大门本身没有遮挡物（如屏风、植物等），可能对健康、财运产生负面影响。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **化解方法**</span><br><span class="line">如果大门正对街道（正门或正对正门），可以通过以下方式化解：</span><br><span class="line"><span class="number">1.</span> **设置遮挡物**：</span><br><span class="line">   - 在大门正前方设置屏风、植物墙或隔断，形成“隔断”“屏气”之感，避免“通路”或“煞气”直冲而来。</span><br><span class="line">   - 如果是正对正门，可以将门框朝内倾斜，形成“冲破正门”的效果，但需注意倾斜过大可能会导致室内气流紊乱。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **使用风水物品**：</span><br><span class="line">   - 在大门正前方放置绿植（如发财树、绿萝），有助于“生财”“纳气”。</span><br><span class="line">   - 如果是正对正门，可以放置红色（红色代表火，生土，土生木，木生火）的植物或红色系的装饰物，形成“五行相生”的效果。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **调整朝向**：</span><br><span class="line">   - 如果无法改变大门方向，可以将家具或装饰物偏向正门（例如将沙发、书架等偏向正门方向），形成“冲正门”的效果，避免正对正门的煞气直冲。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **现代视角**</span><br><span class="line">风水学在现代社会中逐渐被科学化、理性化，更多关注能量流动和空间设计的科学性。例如：</span><br><span class="line"><span class="number">1.</span> **现代风水师**可能建议使用“方位传感器”或“气流追踪设备”来分析室内气流流动方向。</span><br><span class="line"><span class="number">2.</span> **大门正对街道**是否需要避免，更多取决于个人对“通路”“煞气”的主观感受，而不是绝对的“忌讳”。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **综合建议**</span><br><span class="line"><span class="number">1.</span> **大门正对街道是否需要特别处理？**</span><br><span class="line">   - 如果是正对正门，且大门本身没有遮挡物，建议设置屏风或植物化解煞气。</span><br><span class="line">   - 如果是正对街道（但正对正门），可以尝试将门框朝内倾斜，形成“冲正门”的效果。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **避免绝对化**</span><br><span class="line">   - 风水学是传统文化的一部分，更多是地方性习俗，具体是否“忌讳”需要结合个人命理、空间布局和实际需求来判断。</span><br><span class="line">   - 也可以通过现代建筑技术（如flowsuite、V型结构）来优化室内能量流动，减少对大门方向的影响。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **理性看待**</span><br><span class="line">   - 风水更多是“趋吉避凶”的建议，而不是绝对的“迷信”。如果对“忌讳”有疑虑，可以通过科学方法（如测量气流、使用气流净化设备）来验证风水布局对健康、运势的影响。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **总结**</span><br><span class="line">**大门正对街道（尤其是正对正门）**在传统风水中可能被认为“通路”或“煞气冲出”，但现代风水更注重能量流动的科学性，建议结合实际需求和空间布局，通过合理的布局和设计来优化气场流动。</span><br></pre></td></tr></table></figure>

<p><strong>前</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">最近感觉有点霉运，怎么改运</span><br><span class="line"></span><br><span class="line">嗯，最近感觉有点霉运，想改运。首先，我应该了解一下霉运是什么意思。霉运在风水学中通常指的是不利的气场或不顺利的情况，可能影响健康、事业、感情等方面。那么，为什么会有霉运呢？可能是因为个人的五行不平衡，或者环境因素，比如居住环境的气场不稳定。</span><br><span class="line"></span><br><span class="line">首先，我需要确认自己的五行情况。五行包括金、木、水、火、土，不同的五行组合会影响运势。如果五行不均衡，可能会导致霉运。例如，五行缺失或者过旺都可能导致霉运。</span><br><span class="line"></span><br><span class="line">接下来，我应该检查一下自己的居住环境。大门对着路，这可能是一个霉运的开始，因为气场容易外泄，影响内部环境。我需要调整大门的方向或位置，确保气场能够稳定下来。可能需要使用一些风水物品，比如植物、镜子等，来平衡气场。</span><br><span class="line"></span><br><span class="line">另外，生活作息也很重要。如果睡眠不足或者饮食不规律，可能会导致身体和心理上的霉运。我应该调整作息时间，保证充足的睡眠，同时注意饮食健康，避免过多的辛辣、油腻食物。</span><br><span class="line"></span><br><span class="line">情绪管理也是一个关键点。如果最近情绪波动较大，可能会影响到霉运。我需要学会调节情绪，保持乐观的心态，必要时可以寻求心理咨询或朋友的建议。</span><br><span class="line"></span><br><span class="line">还有，事业和财运方面，如果最近感觉工作或财运不顺，可能需要调整工作方式或财运布局。比如，寻找新的工作机会，调整预算，避免不必要的开支。</span><br><span class="line"></span><br><span class="line">环境布置也需要注意。家中摆放的物品应该有助于稳定气场，比如绿色植物、镜子等。避免使用一些过于 Sharp 或者装饰过于繁复的物品，以免分散气场。</span><br><span class="line"></span><br><span class="line">此外，还需要关注身体健康。如果最近身体有不适，比如疲劳、头痛等，可能与霉运有关。我应该注意锻炼身体，保持良好的身体状态，避免过度劳累。</span><br><span class="line"></span><br><span class="line">最后，如果霉运持续时间较长，影响了生活和工作，可能需要寻求专业的风水师或心理咨询师的帮助，进行更深入的调整和指导。</span><br><span class="line"></span><br><span class="line">总的来说，改运需要从多个方面入手，包括五行平衡、居住环境调整、生活习惯改善、情绪管理、财运布局和身体健康等。需要耐心和坚持，逐步改善霉运带来的影响。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">改运是一个系统性的过程，需要从多个方面进行调整和改善。以下是一些具体的步骤和建议，帮助你改运：</span><br><span class="line"></span><br><span class="line">### <span class="number">1.</span> **确认五行平衡**</span><br><span class="line">   - **五行分析**：首先，了解你的五行组合，确认五行是否均衡。如果五行缺失或过旺，可以通过调整饮食、作息和环境来平衡五行。</span><br><span class="line">   - **五行建议**：</span><br><span class="line">     - **金不足**：可以适当增加金元素，如佩戴金饰、接触金质物品、进行太极拳等。</span><br><span class="line">     - **木不足**：增加木元素，如种植树木、养宠物、进行户外活动等。</span><br><span class="line">     - **水不足**：增加水元素，如喝温水、泡脚、阅读等。</span><br><span class="line">     - **火不足**：减少火元素，如避免使用大火烹饪、少看电视等。</span><br><span class="line">     - **土不足**：增加土元素，如种植花草、进行室内运动等。</span><br><span class="line"></span><br><span class="line">### <span class="number">2.</span> **调整居住环境**</span><br><span class="line">   - **大门方向**：确保大门正对正门，避免对着路或墙壁。如果确实无法调整，可以使用风水物品如红色帘子、铜镜等来遮挡。</span><br><span class="line">   - **气场平衡**：在客厅、卧室等主要房间放置绿色植物，如发财树、绿萝等，有助于平衡气场。</span><br><span class="line">   - **消除煞气**：如果家中有煞气，可以通过悬挂 appropriate 的书法作品、放置 appropriate 的装饰来化解。</span><br><span class="line"></span><br><span class="line">### <span class="number">3.</span> **改善作息和饮食**</span><br><span class="line">   - **作息规律**：保证充足的睡眠，避免熬夜。早晨起床后进行简单的锻炼，如太极拳或八段锦。</span><br><span class="line">   - **饮食调理**：选择清淡、营养均衡的饮食，避免过多的辛辣、油腻食物。多吃水果、蔬菜，保持身体湿润。</span><br><span class="line">   - **五谷四时**：遵循五谷四时的原则，多吃时令水果，保持身体的平衡。</span><br><span class="line"></span><br><span class="line">### <span class="number">4.</span> **情绪管理与心理健康**</span><br><span class="line">   - **情绪调节**：学会调节情绪，避免长期处于不良情绪中。可以通过冥想、深呼吸等方式来缓解压力。</span><br><span class="line">   - **寻求帮助**：如果情绪波动较大，可以寻求朋友、家人或专业的心理咨询师的帮助。</span><br><span class="line">   - **培养兴趣**：通过培养兴趣爱好，转移注意力，缓解霉运带来的影响。</span><br><span class="line"></span><br><span class="line">### <span class="number">5.</span> **调整财运和事业运势**</span><br><span class="line">   - **财运布局**：如果最近财运不顺，可以通过调整预算、寻找新的工作机会或投资来改善。避免过度依赖某一种方式，保持多元化。</span><br><span class="line">   - **职业调整**：如果事业上遇到困难，可以通过调整工作方式、学习新技能或寻找合作伙伴来改善。</span><br><span class="line"></span><br><span class="line">### <span class="number">6.</span> **增强身体免疫力**</span><br><span class="line">   - **锻炼身体**：每天进行适量的锻炼，增强体质。可以进行瑜伽、太极拳等慢节奏的运动。</span><br><span class="line">   - **保持水分**：多喝水，保持身体湿润，避免干燥引起的不适。</span><br><span class="line">   - **避免过度劳累**：合理安排工作和生活，避免长时间的高强度工作，防止疲劳。</span><br><span class="line"></span><br><span class="line">### <span class="number">7.</span> **风水布置**</span><br><span class="line">   - **室内布置**：在家中摆放适当的装饰品，如镜子、书法作品等，以平衡气场。</span><br><span class="line">   - **使用风水物品**：如绿色植物、红色 pendulum 等，有助于稳定气场，改善霉运。</span><br><span class="line">   - **避免负面物品**：避免放置负面物品，如镜子中的倒影或装饰过于 Sharp 的物品，以免影响气场。</span><br><span class="line"></span><br><span class="line">### <span class="number">8.</span> **关注心理健康**</span><br><span class="line">   - **定期体检**：关注身体健康，定期进行体检，排除潜在的健康问题。</span><br><span class="line">   - **保持乐观**：保持积极的心态，避免过度悲观，霉运往往与消极情绪有关。</span><br><span class="line"></span><br><span class="line">### <span class="number">9.</span> **寻求专业帮助**</span><br><span class="line">   - **风水师咨询**：如果霉运持续时间较长，影响了生活和工作，可以寻求专业的风水师或命师的帮助，进行详细分析和调整。</span><br><span class="line">   - **心理咨询**：如果情绪问题严重，可以寻求心理咨询师的帮助，进行心理辅导和治疗。</span><br><span class="line"></span><br><span class="line">### <span class="number">10.</span> **总结与反思**</span><br><span class="line">   - **定期总结**：每隔一段时间，回顾一下自己的生活和运势，找出问题所在，并进行相应的调整。</span><br><span class="line">   - **持续改进**：霉运的改善是一个长期的过程，需要持续的努力和调整，逐步达到理想的状态。</span><br><span class="line"></span><br><span class="line">通过以上步骤和建议，你可以逐步改善霉运带来的影响，提升整体运势。记住，改运需要耐心和坚持，不要急于求成，逐步调整，你会发现霉运带来的影响正在逐步减少。</span><br></pre></td></tr></table></figure>

<p><strong>后</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">好的，用户最近感觉有点霉运，想改运。首先，我需要理解用户的具体情况。用户没有提供太多细节，比如性别、出生时间、五行强弱等，这可能会影响命理分析。不过，我可以先从常见的霉运原因入手，比如事业、健康、人际关系等方面。</span><br><span class="line"></span><br><span class="line">霉运在风水学中通常与“煞气”“煞星”相关，但用户可能更关心的是如何改善当前的状态。需要避免使用过于绝对化的说法，而是提供实际可行的建议。比如，调整风水布局、调整作息、保持心态等。</span><br><span class="line"></span><br><span class="line">接下来，要结合用户可能的性别（女）和时间（<span class="number">1992</span>年），但没有具体日期，可能需要泛指。需要提醒用户风水是传统文化，需结合实际情况，避免绝对化。同时，提供科学合理的建议，比如调整床铺、保持充足睡眠、运动等。</span><br><span class="line"></span><br><span class="line">还要注意用户的心理预期，避免让用户感到被批评，而是给予鼓励和支持。最后，建议用户多关注自己的状态，调整心态，保持积极。</span><br><span class="line"></span><br><span class="line">在回复时，要结构清晰，分点说明，避免使用过于专业的术语，让用户容易理解。同时，保持语气友好，让用户感到被理解和支持。</span><br><span class="line"></span><br><span class="line">最后，检查是否有遗漏的信息，比如用户是否有特别的生肖或命理特点，但用户没有提供，所以需要保持普遍适用性。确保建议具体、实用，帮助用户实际改善运势。</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **霉运分析与改运建议（基于<span class="number">1992</span>年闰四月初九巳时女命）**</span><br><span class="line"></span><br><span class="line">#### **一、霉运成因（结合八字分析）**</span><br><span class="line"><span class="number">1.</span> **命理特点**</span><br><span class="line">   - **日主（甲木）**：木为阴性，需火生土，但火土过旺，土被火耗尽，形成“火土相克”。</span><br><span class="line">   - **时柱（巳火）**：火为阳性，火过旺，木被火耗尽，形成“火木相克”。</span><br><span class="line">   - **地支（申金）**：金为阳性，土生金，但土过旺，金被土耗尽，形成“土金相克”。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **常见霉运方向**</span><br><span class="line">   - **事业**：火土过旺，易引发压力或人际关系矛盾，建议调整工作方式。</span><br><span class="line">   - **健康**：土被火耗尽，易引发湿热或肝火过旺的健康问题，建议注意饮食和作息。</span><br><span class="line">   - **人际关系**：火木相克，易引发口舌之争或事业上的合作冲突，建议多沟通化解。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **二、改运核心要点**</span><br><span class="line"><span class="number">1.</span> **五行平衡**</span><br><span class="line">   - **补火**：建议选择火元素（如-safe fire）的物品，如紫砂壶、黄水晶等，增强木气。</span><br><span class="line">   - **补土**：在卧室摆放植物（如发财树、绿萝）或使用五行土的装饰品，增强土气。</span><br><span class="line">   - **生木**：多接触绿色植物（如绿植、绿萝），保持木气活跃。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **风水布局**</span><br><span class="line">   - **卧室**：</span><br><span class="line">     - 卧室正对正门时，建议将床铺朝内倾斜（<span class="number">45</span>度）以“冲破正门”；</span><br><span class="line">     - 正门正对正门时，可放置土元素（如红色门帘或红色 plush）化解“土土相克”。</span><br><span class="line">   - **厨房**：</span><br><span class="line">     - 灶头火位（需具体调整）避免正对正门或正对正门正对正门的“煞气”。</span><br><span class="line">     - 灶头火位可放置红色或绿色的石子（五行木）化解火土过旺的问题。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **作息调整**</span><br><span class="line">   - **作息时间**：</span><br><span class="line">     - 早起后进行<span class="number">10</span>分钟的“阳起”（如太极拳或气功练习），增强阳气。</span><br><span class="line">     - 晚上<span class="number">11</span>点前<span class="number">1</span>小时进行<span class="number">10</span>分钟的“阴闭”（如冥想或静坐），平衡五行。</span><br><span class="line">   - **饮食**：</span><br><span class="line">     - 早餐避免过热或油腻，选择清淡的绿色（木）食物（如菠菜、银耳）。</span><br><span class="line">     - 晚餐避免过量饮酒（酒属火，火过旺）。</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> **情绪管理**</span><br><span class="line">   - **正念冥想**：每天早起或睡前<span class="number">15</span>分钟进行<span class="number">5</span>分钟的正念冥想（如六字真言或八段锦），保持心气平衡。</span><br><span class="line">   - **化解冲突**：遇到口舌之争时，保持冷静，通过沟通或化解（如换位思考）化解矛盾。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **三、重要节点建议**</span><br><span class="line"><span class="number">1.</span> **<span class="number">2024</span>年**</span><br><span class="line">   - **<span class="number">2</span>月**（农历二月）：木气增强，适合进行补土或补火的活动。</span><br><span class="line">   - **<span class="number">8</span>月**（农历七月）：木气耗尽，注意防寒保暖，避免剧烈运动。</span><br><span class="line"><span class="number">2.</span> **<span class="number">2025</span>年**</span><br><span class="line">   - **<span class="number">4</span>月**（农历四月）：木气恢复，适合进行补土或补火的活动。</span><br><span class="line">   - **<span class="number">12</span>月**（农历December）：木气过旺，注意调节情绪，避免过度劳累。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **四、注意事项**</span><br><span class="line"><span class="number">1.</span> **风水与科学结合**</span><br><span class="line">   风水是传统文化的一部分，需结合现代科学（如心理调适、健康检查）进行综合调理。</span><br><span class="line"><span class="number">2.</span> **避免绝对化**</span><br><span class="line">   风水学更多是“趋吉避凶”的建议，具体是否“霉运”还需结合个人状态进行判断。</span><br><span class="line"><span class="number">3.</span> **长期调整**</span><br><span class="line">   霉运并非一朝一夕，需通过长期的风水布局与作息调整来改善，避免过度依赖单一方法。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **总结**</span><br><span class="line">霉运是命理规律的提醒，而非绝对的“厄运”。通过五行平衡、风水调整和科学生活方式的结合，可以显著改善当前状态。建议从**补火**（如紫砂壶）、**补土**（如发财树）和**生木**（如绿色植物）三个方面入手，同时结合个人命理和实际情况进行调整。霉运并非终点，改运的关键在于**坚持与耐心**。</span><br></pre></td></tr></table></figure>

<p><strong>前</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">卫生间和厨房挨着，有忌讳吗，需要化解吗</span><br><span class="line"></span><br><span class="line">嗯，用户之前问过关于大门对着路的霉运问题，现在又提到了卫生间和厨房挨着是否会有忌讳，需要化解。看来他对风水问题很感兴趣，可能是在装修或布置房间的时候遇到了困扰。</span><br><span class="line"></span><br><span class="line">首先，我需要明确卫生间和厨房挨着在风水上是否真的有忌讳。根据我的了解，传统风水中，卧室和起居区需要保持一定的距离，而厨房和卫生间虽然功能不同，但并没有明确的忌讳。不过，用户可能担心这样的布局会影响家庭的气场或运势，所以想了解是否需要采取措施化解。</span><br><span class="line"></span><br><span class="line">接下来，我会思考用户可能的深层需求。他可能在装修过程中发现厨房和卫生间挨着，担心这会带来不好的影响，比如健康问题或生活上的不便。因此，他希望得到一些建议，如何通过风水调整来改善这种情况。</span><br><span class="line"></span><br><span class="line">在解释厨房和卫生间挨着的问题时，我需要指出这并不符合传统风水的禁忌，但一些布局上的不合理可能会带来一些负面影响。比如，厨房是烹饪的地方，与卫生间相邻可能会导致湿气过重，影响健康。同时，卫生间的隐私问题也可能导致心理压力。</span><br><span class="line"></span><br><span class="line">然后，我会提供一些化解的方法。首先，保持厨房和卫生间的基本整洁，避免卫生问题。其次，考虑在厨房和卫生间之间放置一些隔断或装饰物，减少功能上的干扰。另外，调整床铺的位置，确保卧室与厨房和卫生间保持一定的距离，有助于改善气场。</span><br><span class="line"></span><br><span class="line">我还需要提醒用户，如果霉运持续，可能需要更深入的风水调整，比如调整家具的位置或使用风水物品来平衡气场。同时，保持良好的生活习惯和积极的心态也很重要。</span><br><span class="line"></span><br><span class="line">最后，我应该总结一下，厨房和卫生间挨着并不算禁忌，但合理的布局和适当的调整可以减少负面影响，帮助用户更好地生活和改善运势。</span><br><span class="line">&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">卫生间和厨房挨着并不是传统风水中明确的忌讳，但具体情况需要结合实际情况来判断。以下是关于这一布局的分析以及可能的化解建议：</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### <span class="number">1.</span> **传统风水中的布局要求**</span><br><span class="line">- **卧室与起居区需要保持距离**：卧室和起居区（如客厅、厨房、卫生间等）需要有一定的距离，以保持气场的平衡和生活的安宁。</span><br><span class="line">- **厨房与卧室的关系**：厨房是烹饪和储存食物的地方，与卧室（尤其是主卧）的距离一般要求在<span class="number">2</span>米以上，以避免气场的干扰。</span><br><span class="line">- **卫生间与卧室的关系**：卫生间主要负责清洁和存储生活用品，与卧室的距离一般也需要保持在<span class="number">1</span>米以上。</span><br><span class="line"></span><br><span class="line">因此，厨房和卫生间挨着并不违反传统风水的禁忌，但可能会因为功能上的干扰而带来一些负面影响。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### <span class="number">2.</span> **厨房和卫生间挨着的潜在问题**</span><br><span class="line">- **湿气过重**：厨房和卫生间通常都比较潮湿，如果这两个房间直接挨在一起，容易导致整体环境过于潮湿，尤其是卫生间的湿气可能通过墙壁或地面传到厨房，影响整体的干燥环境。</span><br><span class="line">- **隐私问题**：如果厨房和卫生间靠近，可能会因为声音或光线的干扰影响到生活，尤其是当有人在使用卫生间的设备时，可能会感到心理压力。</span><br><span class="line">- **气场干扰**：厨房和卫生间是家庭中功能较为特殊的空间，如果这两个空间过于靠近，可能会对整体的气场产生干扰，导致生活中的不稳定感。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### <span class="number">3.</span> **如何化解厨房和卫生间挨着的问题**</span><br><span class="line">如果厨房和卫生间确实挨着，可以通过以下方式来化解：</span><br><span class="line"></span><br><span class="line">#### （<span class="number">1</span>）**保持基本整洁**</span><br><span class="line">- **定期清理卫生间的污渍**：保持卫生间的清洁，避免因为卫生间的污渍影响厨房的环境。</span><br><span class="line">- **合理分区**：厨房和卫生间的功能分区明确，避免因日常使用中出现混淆或干扰。</span><br><span class="line"></span><br><span class="line">#### （<span class="number">2</span>）**使用隔断或装饰物**</span><br><span class="line">- **安装防雾柜或隔板**：在厨房的橱柜中安装防雾柜或隔板，可以有效减少卫生间的湿气对厨房的影响。</span><br><span class="line">- **装饰物隔开空间**：在厨房和卫生间的门框上放置一些装饰物，如植物或镜子，来隔开空间，减少功能上的干扰。</span><br><span class="line"></span><br><span class="line">#### （<span class="number">3</span>）**调整家具和床铺位置**</span><br><span class="line">- **保持卧室与厨房、卫生间的距离**：如果卧室与厨房、卫生间过于靠近，可以通过调整家具的位置（如床铺、书桌等）来增加卧室与其他房间的间距。</span><br><span class="line">- **使用家具隔断**：在卧室和厨房之间放置一张桌子或书架，起到分割空间的作用。</span><br><span class="line"></span><br><span class="line">#### （<span class="number">4</span>）**使用风水物品**</span><br><span class="line">- **放置绿色植物**：在厨房和卫生间的门frame 或窗台上放置一些绿色植物（如发财树、绿萝等），可以吸收湿气，平衡气场。</span><br><span class="line">- **悬挂书法作品或镜子**：在厨房和卫生间的门frame 悬挂一些书法作品或镜子，可以平衡气场，减少压力。</span><br><span class="line"></span><br><span class="line">#### （<span class="number">5</span>）**保持良好的生活习惯**</span><br><span class="line">- **定期清洁卫生间的设备**：避免卫生间的设备长时间处于湿润状态，影响厨房的环境。</span><br><span class="line">- **合理安排使用时间**：尽量避免在厨房和卫生间的使用时间上有重叠，减少功能上的干扰。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### <span class="number">4.</span> **总结**</span><br><span class="line">厨房和卫生间挨着并不是风水上的大忌，但可以通过合理的布局和适当的调整来改善整体环境。如果发现家中有明显的负面影响，可以通过以上方法进行优化。同时，保持良好的生活习惯和积极的心态，也能帮助改善整体的运势和居住环境。</span><br></pre></td></tr></table></figure>

<p><strong>后</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">好的，用户问的是“卫生间和厨房挨着，有忌讳吗，需要化解吗”。首先，我需要分析用户的需求。用户可能对风水有一定的了解，或者最近在布置房间时发现厨房和卫生间相邻，担心风水有问题，想要确认是否需要采取措施化解。</span><br><span class="line"></span><br><span class="line">接下来，我需要回顾一下风水中关于相邻房间的禁忌。根据传统风水学，厨房和卫生间相邻属于“三合”或“三害”，因为厨房属火，卫生间属水，火和水相克，可能带来水土过旺或火土过旺的气场问题。同时，厨房和卫生间是生财和储存财位的房间，相邻可能会影响财气的流动。</span><br><span class="line"></span><br><span class="line">然后，我要考虑用户的具体情况。用户提到“女命<span class="number">1992</span>年”，需要结合八字分析。但用户没有提供具体出生日期，所以只能基于一般情况分析。同时，用户没有提到具体的命理问题，比如五行强弱，因此需要给出一般性的建议。</span><br><span class="line"></span><br><span class="line">接下来，我需要分点解释，先说明风水上的问题，再给出化解方法。首先，解释厨房和卫生间相邻的风水问题，包括“三合”或“三害”的概念，以及对财气、健康和人际关系的影响。然后，分点列出化解方法，如使用风水物品、调整床铺、保持清洁等。</span><br><span class="line"></span><br><span class="line">需要注意的是，风水学是传统文化的一部分，现代科学对此有不同看法，用户可能希望结合实际生活进行调整。因此，建议在保持风水化解的同时，结合现代生活习惯，比如保持厨房和卫生间清洁，合理布局，避免过度使用风水物品。</span><br><span class="line"></span><br><span class="line">还要确保语言通俗易懂，避免使用过于专业的术语，让用户容易理解和操作。同时，避免绝对化，强调风水是趋吉避凶的建议，需结合个人实际情况。最后，提醒用户风水是传统文化，需结合个人命理和实际需求，避免过度迷信。</span><br><span class="line"></span><br><span class="line">总结来说，用户需要的是关于厨房和卫生间相邻的风水问题分析和化解建议。要全面解释问题，提供可行的方法，同时结合现代生活习惯，让用户既能遵循风水，又能适应现代需求。最后，鼓励用户保持积极心态，风水只是参考，实际生活更重要。</span><br><span class="line"></span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **厨房与卫生间相邻的风水分析与化解建议**</span><br><span class="line"></span><br><span class="line">#### **一、风水问题（基于女命<span class="number">1992</span>年）**</span><br><span class="line"><span class="number">1.</span> **“三合”或“三害”**</span><br><span class="line">   - 厨房属火，卫生间属水，火水相克，形成“三合”或“三害”（三合为“水火相克”；三害为“水火相克”）。</span><br><span class="line">   - 会导致“水土过旺”或“火土过旺”的气场问题，影响财气流动、健康状况和人际关系。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **具体影响**</span><br><span class="line">   - **财气**：厨房和卫生间是生财之位，相邻可能形成“财气外泄”，导致财运不稳定或财运受限。</span><br><span class="line">   - **健康**：水火相克易引发湿热或肝火过旺的问题，如口舌不清、胃火过旺等。</span><br><span class="line">   - **人际关系**：水火相克易引发口舌之争或合作冲突，建议化解冲突时多沟通或换位思考。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **二、化解方法**</span><br><span class="line"><span class="number">1.</span> **风水化解**</span><br><span class="line">   - **床铺调整**</span><br><span class="line">     - 将厨房正门正对的床铺朝内倾斜（<span class="number">45</span>度），形成“冲破正门”效果。</span><br><span class="line">     - 在床尾放置红色（火元素）或黑色（水元素）的装饰物，平衡五行。</span><br><span class="line">   - **使用风水物品**</span><br><span class="line">     - 在卫生间摆放红色门帘或红色 plush（如门帘、窗帘），增强五行土的平衡。</span><br><span class="line">     - 墙壁挂“生门”（绿色）或“死门”（红色）装饰，避免“死门冲门”。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> **现代科学结合**</span><br><span class="line">   - **保持整洁**</span><br><span class="line">     - 厨房和卫生间保持整洁，避免污物形成“水土过旺”之气。</span><br><span class="line">     - 使用防滑地垫或防滑地毯，减少湿滑对健康的影响。</span><br><span class="line">   - **合理布局**</span><br><span class="line">     - 厨卫分开使用，避免“三合”或“三害”气场的相互影响。</span><br><span class="line">     - 使用“四合”或“五合”布局（如将厨房和卫生间分别放置在房间的不同方位，避免正对正门）。</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> **调整作息**</span><br><span class="line">   - **早起时间**</span><br><span class="line">     - 早起后进行<span class="number">10</span>分钟“阳起”（如太极拳或气功），增强阳气。</span><br><span class="line">     - 晚上<span class="number">11</span>点前<span class="number">1</span>小时进行<span class="number">10</span>分钟“阴闭”（如冥想或静坐），平衡五行。</span><br><span class="line">   - **饮食**</span><br><span class="line">     - 早餐选择绿色（木元素）食物（如菠菜、银耳），避免过热或油腻。</span><br><span class="line">     - 晚餐避免过量饮酒（酒属火，火过旺）。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **三、重要节点建议**</span><br><span class="line"><span class="number">1.</span> **<span class="number">2024</span>年**</span><br><span class="line">   - **<span class="number">2</span>月**（农历二月）：木气增强，适合进行补土或补火的调整。</span><br><span class="line">   - **<span class="number">8</span>月**（农历七月）：木气耗尽，注意防寒保暖，避免剧烈运动。</span><br><span class="line"><span class="number">2.</span> **<span class="number">2025</span>年**</span><br><span class="line">   - **<span class="number">4</span>月**（农历四月）：木气恢复，适合进行补土或补火的调整。</span><br><span class="line">   - **<span class="number">12</span>月**（农历December）：木气过旺，注意调节情绪，避免过度劳累。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### **四、注意事项**</span><br><span class="line"><span class="number">1.</span> **风水与科学结合**</span><br><span class="line">   风水是传统文化的一部分，需结合现代科学（如心理调适、健康检查）进行综合调理。</span><br><span class="line"><span class="number">2.</span> **避免绝对化**</span><br><span class="line">   风水是趋吉避凶的建议，需结合个人状态和实际需求进行调整。</span><br><span class="line"><span class="number">3.</span> **长期调整**</span><br><span class="line">   霉运并非一朝一夕，需通过长期的风水布局与作息调整来改善，避免过度依赖单一方法。</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### **总结**</span><br><span class="line">厨房与卫生间相邻属于“三合”或“三害”，可能影响财气、健康和人际关系。通过调整床铺、使用风水物品、保持整洁和合理布局等方法，可以有效化解不利影响。建议结合个人命理和实际需求进行调整，风水是趋吉避凶的参考，实际生活更重要。</span><br></pre></td></tr></table></figure>

<p><strong>我看是微调后的看着更厉害些</strong>（不明觉厉）😄。好的就这样吧。</p>
<hr>
<h3 id="unsloth官方示例"><a href="#unsloth官方示例" class="headerlink" title="unsloth官方示例"></a>unsloth官方示例</h3><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb#scrollTo=IqircWtpYuqj">链接</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%%capture</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">if</span> <span class="string">&quot;COLAB_&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> <span class="string">&quot;&quot;</span>.join(os.environ.keys()):</span><br><span class="line">    !pip install unsloth</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Do this only in Colab notebooks! Otherwise use pip install unsloth</span></span><br><span class="line">    !pip install --no-deps bitsandbytes accelerate xformers==<span class="number">0.0</span><span class="number">.29</span>.post3 peft trl triton cut_cross_entropy unsloth_zoo</span><br><span class="line">    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer</span><br><span class="line">    !pip install --no-deps unsloth</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>检测是否运行在 Google Colab</strong>，如果不是，则直接 <code>pip install unsloth</code>。</li>
<li><strong>如果在 Colab</strong>，则手动安装 <code>unsloth</code> 及其相关依赖，以避免 Colab 可能的依赖冲突。</li>
<li><strong><code>--no-deps</code></strong> 选项用于防止 <code>pip</code> 自动安装依赖，可能是因为 Colab 自带了一些库，而 <code>pip</code> 的自动安装可能会导致库版本冲突。</li>
</ul>
<p>这段代码主要用于<strong>确保 <code>unsloth</code> 在不同环境下的安装稳定性</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">max_seq_length = <span class="number">2048</span> <span class="comment"># Choose any! We auto support RoPE Scaling internally!</span></span><br><span class="line">dtype = <span class="literal">None</span> <span class="comment"># None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+</span></span><br><span class="line">load_in_4bit = <span class="literal">True</span> <span class="comment"># Use 4bit quantization to reduce memory usage. Can be False.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4bit pre quantized models we support for 4x faster downloading + no OOMs.</span></span><br><span class="line">fourbit_models = [</span><br><span class="line">    <span class="string">&quot;unsloth/mistral-7b-v0.3-bnb-4bit&quot;</span>,      <span class="comment"># New Mistral v3 2x faster!</span></span><br><span class="line">    <span class="string">&quot;unsloth/mistral-7b-instruct-v0.3-bnb-4bit&quot;</span>,</span><br><span class="line">    <span class="string">&quot;unsloth/llama-3-8b-bnb-4bit&quot;</span>,           <span class="comment"># Llama-3 15 trillion tokens model 2x faster!</span></span><br><span class="line">    <span class="string">&quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;</span>,</span><br><span class="line">    <span class="string">&quot;unsloth/llama-3-70b-bnb-4bit&quot;</span>,</span><br><span class="line">    <span class="string">&quot;unsloth/Phi-3-mini-4k-instruct&quot;</span>,        <span class="comment"># Phi-3 2x faster!</span></span><br><span class="line">    <span class="string">&quot;unsloth/Phi-3-medium-4k-instruct&quot;</span>,</span><br><span class="line">    <span class="string">&quot;unsloth/mistral-7b-bnb-4bit&quot;</span>,</span><br><span class="line">    <span class="string">&quot;unsloth/gemma-7b-bnb-4bit&quot;</span>,             <span class="comment"># Gemma 2.2x faster!</span></span><br><span class="line">] <span class="comment"># More models at https://huggingface.co/unsloth</span></span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;unsloth/llama-3-8b-bnb-4bit&quot;</span>,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dtype = dtype,</span><br><span class="line">    load_in_4bit = load_in_4bit,</span><br><span class="line">    <span class="comment"># token = &quot;hf_...&quot;, # use one if using gated models like meta-llama/Llama-2-7b-hf</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>和上文代码基本一样，这里<code>unsloth.FastLanguageModel</code>：Unsloth 提供的<strong>高效语言模型加载器</strong>，优化了 Hugging Face <code>transformers</code> 的 <code>AutoModelForCausalLM</code>。比 <code>transformers.AutoModelForCausalLM.from_pretrained</code> <strong>加载速度快 2-4 倍</strong>，并且优化了推理性能。</p>
<p>但是这里加载的模型已经是<code>4bit</code>量化的了，又让<code>load_in_4bit = True</code>不知意义何在。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = <span class="number">16</span>, <span class="comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span></span><br><span class="line">    target_modules = [<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">                      <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>,],</span><br><span class="line">    lora_alpha = <span class="number">16</span>,</span><br><span class="line">    lora_dropout = <span class="number">0</span>, <span class="comment"># Supports any, but = 0 is optimized</span></span><br><span class="line">    bias = <span class="string">&quot;none&quot;</span>,    <span class="comment"># Supports any, but = &quot;none&quot; is optimized</span></span><br><span class="line">    <span class="comment"># [NEW] &quot;unsloth&quot; uses 30% less VRAM, fits 2x larger batch sizes!</span></span><br><span class="line">    use_gradient_checkpointing = <span class="string">&quot;unsloth&quot;</span>, <span class="comment"># True or &quot;unsloth&quot; for very long context</span></span><br><span class="line">    random_state = <span class="number">3407</span>,</span><br><span class="line">    use_rslora = <span class="literal">False</span>,  <span class="comment"># We support rank stabilized LoRA</span></span><br><span class="line">    loftq_config = <span class="literal">None</span>, <span class="comment"># And LoftQ</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>与上文代码一样，</p>
<p>使用LoRA轻量微调方法，它不会更新整个 LLM 的权重，而是对特定层（如 <code>q_proj</code>、<code>k_proj</code>）添加低秩适配矩阵。这样：<strong>大幅减少参数更新量</strong>，<strong>降低显存占用</strong>，<strong>加快训练速度</strong>.</p>
<p>关键参数：<code>r=16</code>表示LoRA <strong>秩（rank）</strong>，控制适配矩阵的大小，通常选择 <strong>8、16、32、64、128</strong>。<code>r</code> 越大，<strong>微调效果更强，但显存占用更高</strong>。<code>r=16</code> 是一个<strong>平衡点</strong>，适合大多数任务。<code>target_modules</code>选择 <strong>哪些 Transformer 层</strong> 进行 LoRA 适配。这里选择了 <strong>自注意力（q_proj, k_proj, v_proj, o_proj）</strong> 和 <strong>前馈网络（gate_proj, up_proj, down_proj）</strong> 层，这些层对模型性能影响最大。</p>
<p>lora_alpha:<strong>LoRA 缩放系数</strong>，相当于学习率的放大因子。<strong><code>lora_alpha / r</code> 是实际的学习率</strong>，这里是 <code>16 / 16 = 1</code>，是一个标准选择。</p>
<p>lora_dropout:LoRA 训练时的 Dropout 概率。<strong><code>0</code> 是最优配置</strong>，确保最大化利用适配矩阵，不丢失信息。</p>
<p>LoRA 是否对模型的 <code>bias</code> 进行微调。<strong><code>none</code> 是最优选择</strong>，意味着 <strong>仅调整 LoRA 矩阵，而不调整 bias</strong>，可以减少不必要的参数更新。</p>
<p>use_gradient_checkpointing:<strong>启用梯度检查点</strong>（Gradient Checkpointing），减少训练显存占用。<code>use_gradient_checkpointing = &quot;unsloth&quot;</code> <strong>比标准 Gradient Checkpointing 省 30% 显存，并支持更长的上下文（context length）</strong>。<strong>适合长文本训练，例如 4K、8K 上下文长度的 LoRA 训练</strong>。</p>
<p>random_state:<strong>随机种子</strong>，用于<strong>保证训练可复现</strong>。</p>
<p><strong>是否使用 Rank-Stabilized LoRA（RsLoRA）</strong>。RsLoRA 是一种 <strong>动态调整 <code>r</code> 的方法</strong>，在训练过程中逐渐降低 <code>r</code> 以减少参数，但它尚未完全成熟，所以默认 <strong><code>False</code></strong>。</p>
<p><strong>是否使用 LoftQ（LoRA-Friendly Quantization）</strong>。LoftQ 是一种 <strong>优化 LoRA 训练的量化方法</strong>，可以在 LoRA 训练过程中使用量化模型。<strong>默认 <code>None</code> 关闭 LoftQ</strong>，如果要用 LoftQ，需要传入量化配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;vicgalle/alpaca-gpt4&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(dataset.column_names)</span><br></pre></td></tr></table></figure>

<p>与上文代码一样，这个数据集是<strong>基于原始 Stanford Alpaca（基于 GPT-3.5）数据集</strong>，但是由 <strong>GPT-4 重新生成</strong>，质量更高，常用于<strong>指令微调（Instruction Tuning）</strong>，<code>dataset.column_names</code> 获取 <strong>数据集的所有列名</strong> 并打印出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> to_sharegpt</span><br><span class="line"></span><br><span class="line">dataset = to_sharegpt(</span><br><span class="line">    dataset,</span><br><span class="line">    merged_prompt=<span class="string">&quot;&#123;instruction&#125;[[\nYour input is:\n&#123;input&#125;]]&quot;</span>,</span><br><span class="line">    output_column_name=<span class="string">&quot;output&quot;</span>,</span><br><span class="line">    conversation_extension=<span class="number">3</span>,  <span class="comment"># Select more to handle longer conversations</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>将 <code>vicgalle/alpaca-gpt4</code> 数据集转换为 ShareGPT 格式，以用于对话模型的微调</strong>。<code>to_sharegpt</code> 是 <code>Unsloth</code> 提供的 <strong>数据预处理工具</strong>，用于将 <strong>普通文本指令数据集（如 Alpaca）转换为 ShareGPT 格式</strong>。ShareGPT 格式<strong>更适合训练聊天对话模型</strong>，因为它可以表示多轮对话结构。</p>
<ul>
<li><p><strong><code>dataset</code></strong>: 传入 <strong>Alpaca-GPT4 数据集</strong>。</p>
</li>
<li><p><strong><code>merged_prompt</code></strong>: 定义 <strong>输入格式</strong>，将 <code>instruction</code> 和 <code>input</code> 组合成 ShareGPT 适用的格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&#123;instruction&#125;[[\nYour input is:\n&#123;input&#125;]]&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>[[\nYour input is:\n&#123;input&#125;]]</code>：<ul>
<li><code>[[...]]</code> 是 <code>Unsloth</code> 处理对话边界的特殊标记。</li>
<li><code>Your input is:\n&#123;input&#125;</code> 让 LLM 知道 <code>input</code> 是用户的额外上下文（如果存在）。</li>
</ul>
</li>
</ul>
<p><strong><code>output_column_name=&quot;output&quot;</code></strong>: 指定数据集 <code>output</code> 列为 <strong>模型的目标输出</strong>（训练时的正确答案）。</p>
<p><strong><code>conversation_extension=3</code></strong>:</p>
<ul>
<li><strong>扩展对话能力</strong>，用于处理 <strong>更长的对话历史</strong>。</li>
<li><code>conversation_extension=3</code> 意味着 <strong>3 轮对话上下文</strong>，适合<strong>微调多轮对话模型</strong>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> standardize_sharegpt</span><br><span class="line"></span><br><span class="line">dataset = standardize_sharegpt(dataset)</span><br></pre></td></tr></table></figure>

<p><strong>将数据集标准化，使其完全符合 ShareGPT 格式</strong>，以便用于<strong>训练对话模型</strong>（如 Llama、Mistral 等）。</p>
<p>例（仅做说明，帮助理解，不保证真实性）：</p>
<p>标准化前：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;merged_prompt&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Summarize the following text.[[\nYour input is:\nMeditation is an ancient practice...]]&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Meditation is a time-tested method for relaxation.&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>标准化后：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;conversations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Summarize the following text.\n\nYour input is:\nMeditation is an ancient practice...&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Meditation is a time-tested method for relaxation.&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">chat_template = <span class="string">&quot;&quot;&quot;Below are some instructions that describe some tasks. Write responses that appropriately complete each request.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Instruction:</span></span><br><span class="line"><span class="string">&#123;INPUT&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Response:</span></span><br><span class="line"><span class="string">&#123;OUTPUT&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> apply_chat_template</span><br><span class="line"></span><br><span class="line">dataset = apply_chat_template(</span><br><span class="line">    dataset,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    chat_template=chat_template,</span><br><span class="line">    <span class="comment"># default_system_message = &quot;You are a helpful assistant&quot;, &lt;&lt; [OPTIONAL]</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>将数据集转换为符合 Chat 格式的 Prompt 结构</strong>，以适配 Tokenizer 和模型训练。 <strong><code>chat_template</code></strong> 定义了一个对话格式，包含：</p>
<ul>
<li><code>Instruction</code>（用户指令）</li>
<li><code>Response</code>（模型的回答）</li>
</ul>
<p><code>&#123;INPUT&#125;</code> 和 <code>&#123;OUTPUT&#125;</code> 是 <strong>占位符</strong>，会被 <code>dataset</code> 里的具体 <code>instruction</code> 和 <code>output</code> 填充。</p>
<p><code>apply_chat_template()</code> 是 <code>Unsloth</code> 提供的工具，用于<strong>格式化数据并使其适配 tokenizer</strong>。</p>
<p><strong><code>dataset</code></strong>: 传入预处理后的数据集。</p>
<p><strong><code>tokenizer</code></strong>: 使用 <code>Unsloth</code> 加载的 <code>tokenizer</code>，确保数据符合 Tokenizer 规则。</p>
<p><strong><code>chat_template=chat_template</code></strong>: 指定 <strong>对话格式</strong>，确保数据按照 <code>Instruction → Response</code> 结构组织。</p>
<p><strong><code>default_system_message</code></strong>（可选）:</p>
<ul>
<li>如果设定，<strong>会添加系统消息</strong>，如 <code>&quot;You are a helpful assistant&quot;</code>。</li>
</ul>
</li>
</ul>
<p><strong>训练模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> SFTTrainer</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> is_bfloat16_supported</span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model = model,</span><br><span class="line">    tokenizer = tokenizer,</span><br><span class="line">    train_dataset = dataset,</span><br><span class="line">    dataset_text_field = <span class="string">&quot;text&quot;</span>,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dataset_num_proc = <span class="number">2</span>,</span><br><span class="line">    packing = <span class="literal">False</span>, <span class="comment"># Can make training 5x faster for short sequences.</span></span><br><span class="line">    args = TrainingArguments(</span><br><span class="line">        per_device_train_batch_size = <span class="number">2</span>,</span><br><span class="line">        gradient_accumulation_steps = <span class="number">4</span>,</span><br><span class="line">        warmup_steps = <span class="number">5</span>,</span><br><span class="line">        max_steps = <span class="number">60</span>,</span><br><span class="line">        <span class="comment"># num_train_epochs = 1, # For longer training runs!</span></span><br><span class="line">        learning_rate = <span class="number">2e-4</span>,</span><br><span class="line">        fp16 = <span class="keyword">not</span> is_bfloat16_supported(),</span><br><span class="line">        bf16 = is_bfloat16_supported(),</span><br><span class="line">        logging_steps = <span class="number">1</span>,</span><br><span class="line">        optim = <span class="string">&quot;adamw_8bit&quot;</span>,</span><br><span class="line">        weight_decay = <span class="number">0.01</span>,</span><br><span class="line">        lr_scheduler_type = <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">        seed = <span class="number">3407</span>,</span><br><span class="line">        output_dir = <span class="string">&quot;outputs&quot;</span>,</span><br><span class="line">        report_to = <span class="string">&quot;none&quot;</span>, <span class="comment"># Use this for WandB etc</span></span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>与上文代码基本一致。</p>
<p><strong>查看当前gpu状态</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># @title Show current memory stats</span></span><br><span class="line">gpu_stats = torch.cuda.get_device_properties(<span class="number">0</span>)</span><br><span class="line">start_gpu_memory = <span class="built_in">round</span>(torch.cuda.max_memory_reserved() / <span class="number">1024</span> / <span class="number">1024</span> / <span class="number">1024</span>, <span class="number">3</span>)</span><br><span class="line">max_memory = <span class="built_in">round</span>(gpu_stats.total_memory / <span class="number">1024</span> / <span class="number">1024</span> / <span class="number">1024</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;GPU = <span class="subst">&#123;gpu_stats.name&#125;</span>. Max memory = <span class="subst">&#123;max_memory&#125;</span> GB.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;start_gpu_memory&#125;</span> GB of memory reserved.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>执行训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer_stats = trainer.train()</span><br></pre></td></tr></table></figure>

<p><strong>显示最终内存和时间统计</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># @title Show final memory and time stats</span><br><span class="line">used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)</span><br><span class="line">used_memory_for_lora = round(used_memory - start_gpu_memory, 3)</span><br><span class="line">used_percentage = round(used_memory / max_memory * 100, 3)</span><br><span class="line">lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)</span><br><span class="line">print(f&quot;&#123;trainer_stats.metrics[&#x27;train_runtime&#x27;]&#125; seconds used for training.&quot;)</span><br><span class="line">print(</span><br><span class="line">    f&quot;&#123;round(trainer_stats.metrics[&#x27;train_runtime&#x27;]/60, 2)&#125; minutes used for training.&quot;</span><br><span class="line">)</span><br><span class="line">print(f&quot;Peak reserved memory = &#123;used_memory&#125; GB.&quot;)</span><br><span class="line">print(f&quot;Peak reserved memory for training = &#123;used_memory_for_lora&#125; GB.&quot;)</span><br><span class="line">print(f&quot;Peak reserved memory % of max memory = &#123;used_percentage&#125; %.&quot;)</span><br><span class="line">print(f&quot;Peak reserved memory for training % of max memory = &#123;lora_percentage&#125; %.&quot;)</span><br></pre></td></tr></table></figure>

<p><strong>推理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">FastLanguageModel.for_inference(model) <span class="comment"># Enable native 2x faster inference</span></span><br><span class="line">messages = [                    <span class="comment"># Change below!</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line">input_ids = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>,</span><br><span class="line">    return_tensors = <span class="string">&quot;pt&quot;</span>,</span><br><span class="line">).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TextStreamer</span><br><span class="line">text_streamer = TextStreamer(tokenizer, skip_prompt = <span class="literal">True</span>)</span><br><span class="line">_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = <span class="number">128</span>, pad_token_id = tokenizer.eos_token_id)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FastLanguageModel.for_inference(model) <span class="comment"># Enable native 2x faster inference</span></span><br><span class="line">messages = [                         <span class="comment"># Change below!</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,      <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;The fibonacci sequence continues as 13, 21, 34, 55 and 89.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,      <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is France&#x27;s tallest tower called?&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line">input_ids = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>,</span><br><span class="line">    return_tensors = <span class="string">&quot;pt&quot;</span>,</span><br><span class="line">).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TextStreamer</span><br><span class="line">text_streamer = TextStreamer(tokenizer, skip_prompt = <span class="literal">True</span>)</span><br><span class="line">_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = <span class="number">128</span>, pad_token_id = tokenizer.eos_token_id)</span><br></pre></td></tr></table></figure>

<p><strong>保存和加载模型</strong></p>
<ul>
<li>保存Lora模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained(<span class="string">&quot;lora_model&quot;</span>)  <span class="comment"># Local saving</span></span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;lora_model&quot;</span>)</span><br><span class="line"><span class="comment"># model.push_to_hub(&quot;your_name/lora_model&quot;, token = &quot;...&quot;) # Online saving</span></span><br><span class="line"><span class="comment"># tokenizer.push_to_hub(&quot;your_name/lora_model&quot;, token = &quot;...&quot;) # Online saving</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:	<span class="comment"># 需要加载微调后的模型时设置为True</span></span><br><span class="line">    <span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line">    model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">        model_name = <span class="string">&quot;lora_model&quot;</span>, <span class="comment"># YOUR MODEL YOU USED FOR TRAINING</span></span><br><span class="line">        max_seq_length = max_seq_length,</span><br><span class="line">        dtype = dtype,</span><br><span class="line">        load_in_4bit = load_in_4bit,</span><br><span class="line">    )</span><br><span class="line">    FastLanguageModel.for_inference(model) <span class="comment"># Enable native 2x faster inference</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">messages = [                    <span class="comment"># Change below!</span></span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line">input_ids = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>,</span><br><span class="line">    return_tensors = <span class="string">&quot;pt&quot;</span>,</span><br><span class="line">).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TextStreamer</span><br><span class="line">text_streamer = TextStreamer(tokenizer, skip_prompt = <span class="literal">True</span>)</span><br><span class="line">_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = <span class="number">128</span>, pad_token_id = tokenizer.eos_token_id)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>保存微调后的整个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained_merged(<span class="string">&quot;model_vllm&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_16bit&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>看这个名字你就知道我想做什么了吧🤪。</p>
</li>
</ul>
<p><strong>制作ollama支持的GGUF文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save to 8bit Q8_0</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer,)</span><br><span class="line"><span class="comment"># Remember to go to https://huggingface.co/settings/tokens for a token!</span></span><br><span class="line"><span class="comment"># And change hf to your username!</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_gguf(<span class="string">&quot;hf/model&quot;</span>, tokenizer, token = <span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>依旧是重新编译<code>llama.cpp</code>，然后制作好<code>GGUF</code>文件后，<code>ollama</code>加载不成功，累了，毁灭吧🙂。</p>
<hr>
<p>附录</p>
<p><strong>自定义对话模板</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">alpaca_prompt = <span class="string">&quot;&quot;&quot;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Instruction:</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Input:</span></span><br><span class="line"><span class="string">&#123;&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### Response:</span></span><br><span class="line"><span class="string">&#123;&#125;&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>





<hr>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li>w</li>
<li><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb#scrollTo=IqircWtpYuqj">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb#scrollTo=IqircWtpYuqj</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/unsloth/" rel="tag"># unsloth</a>
              <a href="/tags/deepseek/" rel="tag"># deepseek</a>
              <a href="/tags/ollama/" rel="tag"># ollama</a>
              <a href="/tags/huggingface/" rel="tag"># huggingface</a>
              <a href="/tags/llama-8b/" rel="tag"># llama-8b</a>
              <a href="/tags/fine-tune/" rel="tag"># fine-tune</a>
              <a href="/tags/colab/" rel="tag"># colab</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/03/24/LLamaFactory%E5%BE%AE%E8%B0%83Qwen2-5-7B/" rel="prev" title="LLamaFactory微调Qwen2.5 7B">
                  <i class="fa fa-angle-left"></i> LLamaFactory微调Qwen2.5 7B
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/27/ollama%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%EF%BC%88ubuntu%EF%BC%89/" rel="next" title="ollama安装与使用（ubuntu）">
                  ollama安装与使用（ubuntu） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">205k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">12:24</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
