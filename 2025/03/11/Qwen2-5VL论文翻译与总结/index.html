<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言说来惭愧，玩了这么久LLM，还没正经看过LLM的论文，最近要好好玩玩Qwen2.5 VL，目的是探索能基于它做什么应用（主要看能不能做到实时检测），这里先看一下论文，有个原理上的认识，再去微调Qwen2.5 VL。 论文名：Qwen2.5-VL Technical Report 作者：Qwen Team, Alibaba Group 论文地址： https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;25">
<meta property="og:type" content="article">
<meta property="og:title" content="论文：Qwen2.5VL">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言说来惭愧，玩了这么久LLM，还没正经看过LLM的论文，最近要好好玩玩Qwen2.5 VL，目的是探索能基于它做什么应用（主要看能不能做到实时检测），这里先看一下论文，有个原理上的认识，再去微调Qwen2.5 VL。 论文名：Qwen2.5-VL Technical Report 作者：Qwen Team, Alibaba Group 论文地址： https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;25">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311144619136.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311152513173.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311161912268.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250312104636009.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314140802625.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314142212215.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314143316009.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314144415902.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314144806604.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314155533200.png">
<meta property="article:published_time" content="2025-03-11T03:21:48.000Z">
<meta property="article:modified_time" content="2025-04-28T07:08:22.501Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="qwen2.5">
<meta property="article:tag" content="qwen2.5 vl">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311144619136.png">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/","path":"2025/03/11/Qwen2-5VL论文翻译与总结/","title":"论文：Qwen2.5VL"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>论文：Qwen2.5VL | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81%EF%BC%88Abstract%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">摘要（Abstract）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%BB%8B%E7%BB%8D%EF%BC%88Introduction%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">1. 介绍（Introduction）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%96%B9%E6%B3%95%EF%BC%88Approach%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">2. 方法（Approach）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">4.1.</span> <span class="nav-text">2.1 模型架构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-%E5%BF%AB%E9%80%9F%E5%92%8C%E9%AB%98%E6%95%88%E7%9A%84%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">4.1.1.</span> <span class="nav-text">2.1.1 快速和高效的视觉编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-%E5%8E%9F%E7%94%9F%E5%8A%A8%E6%80%81%E5%88%86%E8%BE%A8%E7%8E%87%E5%92%8C%E5%B8%A7%E7%8E%87"><span class="nav-number">4.1.2.</span> <span class="nav-text">2.1.2 原生动态分辨率和帧率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-3-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E4%B8%8E%E7%BB%9D%E5%AF%B9%E6%97%B6%E9%97%B4%E5%AF%B9%E9%BD%90"><span class="nav-number">4.1.3.</span> <span class="nav-text">2.1.3 多模态旋转位置嵌入与绝对时间对齐</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">4.2.</span> <span class="nav-text">2.2 预训练</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.2.1预训练数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88%EF%BC%88Training-Recipe%EF%BC%89"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2.2 训练方案（Training Recipe）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%90%8E%E8%AE%AD%E7%BB%83%EF%BC%88Post-training%EF%BC%89"><span class="nav-number">4.3.</span> <span class="nav-text">2.3 后训练（Post-training）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE"><span class="nav-number">4.3.1.</span> <span class="nav-text">2.3.1 指令数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-%E6%95%B0%E6%8D%AE%E8%BF%87%E6%BB%A4%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="nav-number">4.3.2.</span> <span class="nav-text">2.3.2 数据过滤流水线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-%E5%9F%BA%E4%BA%8E%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E7%9A%84%E5%A2%9E%E5%BC%BA%E6%8E%A8%E7%90%86"><span class="nav-number">4.3.3.</span> <span class="nav-text">2.3.3 基于拒绝采样的增强推理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-4-%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88"><span class="nav-number">4.3.4.</span> <span class="nav-text">2.3.4 训练方案</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%AE%9E%E9%AA%8C"><span class="nav-number">5.</span> <span class="nav-text">3 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E4%B8%8E%E5%85%88%E8%BF%9B%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%BE%83"><span class="nav-number">5.1.</span> <span class="nav-text">3.1 与先进模型比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E7%BA%AF%E6%96%87%E6%9C%AC%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%A1%A8%E7%8E%B0"><span class="nav-number">5.2.</span> <span class="nav-text">3.2 纯文本任务的表现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%AE%9A%E9%87%8F%E7%BB%93%E6%9E%9C"><span class="nav-number">5.3.</span> <span class="nav-text">3.3 定量结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-%E9%80%9A%E7%94%A8%E8%A7%86%E8%A7%89%E9%97%AE%E7%AD%94%EF%BC%88VQA%EF%BC%89"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.3.1 通用视觉问答（VQA）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-%E6%96%87%E6%A1%A3%E7%90%86%E8%A7%A3%E4%B8%8E-OCR"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.3.2 文档理解与 OCR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-%E7%A9%BA%E9%97%B4%E7%90%86%E8%A7%A3"><span class="nav-number">5.3.3.</span> <span class="nav-text">3.3.3 空间理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-4-%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E4%B8%8E%E5%AE%9A%E4%BD%8D"><span class="nav-number">5.3.4.</span> <span class="nav-text">3.3.4 视频理解与定位</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-5-%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%88Agent%EF%BC%89"><span class="nav-number">5.3.5.</span> <span class="nav-text">3.3.5 智能体（Agent）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%BB%93%E8%AE%BA"><span class="nav-number">6.</span> <span class="nav-text">4 结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">7.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vit"><span class="nav-number">7.1.</span> <span class="nav-text">Vit</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">7.1.1.</span> <span class="nav-text">ViT是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT-%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86"><span class="nav-number">7.1.2.</span> <span class="nav-text">ViT 的核心原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT-%E5%92%8C-CNN%EF%BC%88%E4%BC%A0%E7%BB%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">7.1.3.</span> <span class="nav-text">ViT 和 CNN（传统卷积神经网络）的对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT-%E5%8F%AA%E6%98%AF%E4%B8%80%E7%A7%8D%E7%AE%97%E6%B3%95%EF%BC%8C%E8%BF%98%E6%98%AF%E4%B8%80%E7%B1%BB%E7%AE%97%E6%B3%95%EF%BC%9F"><span class="nav-number">7.1.4.</span> <span class="nav-text">ViT 只是一种算法，还是一类算法？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%82%E5%90%88%E5%B0%8F%E7%99%BD%E7%9A%84%E6%80%BB%E7%BB%93"><span class="nav-number">7.1.5.</span> <span class="nav-text">适合小白的总结</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">99</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="论文：Qwen2.5VL | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文：Qwen2.5VL
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-03-11 11:21:48" itemprop="dateCreated datePublished" datetime="2025-03-11T11:21:48+08:00">2025-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-28 15:08:22" itemprop="dateModified" datetime="2025-04-28T15:08:22+08:00">2025-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/theory/" itemprop="url" rel="index"><span itemprop="name">theory</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>53 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>说来惭愧，玩了这么久LLM，还没正经看过LLM的论文，最近要好好玩玩Qwen2.5 VL，目的是探索能基于它做什么应用（主要看能不能做到实时检测），这里先看一下论文，有个原理上的认识，再去微调Qwen2.5 VL。</p>
<p><strong>论文名：Qwen2.5-VL Technical Report</strong></p>
<p><strong>作者：Qwen Team, Alibaba Group</strong></p>
<p><strong>论文地址：</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.13923">https://arxiv.org/abs/2502.13923</a></p>
<p><strong>Qwen Chat:</strong> <a target="_blank" rel="noopener" href="https://chat.qwenlm.ai/">https://chat.Qwenlm.ai</a></p>
<p><strong>Hugging face:</strong> <a target="_blank" rel="noopener" href="https://huggingface.co/Qwen">https://huggingface.co/Qwen</a></p>
<p><strong>ModelScope:</strong> <a target="_blank" rel="noopener" href="https://modelscope.cn/organization/Qwen">https://modelscope.cn/organization/Qwen</a></p>
<p><strong>Github:</strong> <a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen2.5-VL">https://github.com/QwenLM/Qwen2.5-VL</a></p>
<p><em><strong>年份：2025</strong></em></p>
<span id="more"></span>

<hr>
<h2 id="摘要（Abstract）"><a href="#摘要（Abstract）" class="headerlink" title="摘要（Abstract）"></a>摘要（Abstract）</h2><p>我们介绍了Qwen2.5-VL，Qwen视觉语言系列的最新旗舰模型，它展示了在基础功能和创新功能方面的重大进步。Qwen2.5-VL通过增强的视觉识别、精确的对象定位、稳健的文档解析和长视频理解，实现了在理解和与世界交互方面的重大飞跃。Qwen2.5-VL通过增强的视觉识别、精确的对象定位、鲁棒的文档解析和长视频理解，实现了在理解世界和互动方面的重大飞跃。Qwen2.5-VL的一个突出的特点是它能够使用边界框或点精确地定位对象。它提供了从发票、表单和表格中提取的健壮的结构化数据，以及对图表和布局的详细分析。为了处理复杂的输入，Qwen2.5- VL引入了动态分辨率处理和绝对时间编码，使其能够通过二级事件定位来处理不同大小的图像和延长持续时间（长达数小时）的视频。这使得模型能够原生地感知空间尺度和时间动态，而无需依赖传统的归一化技术。通过从零开始训练一个原生的动态分辨率视觉 Transformer (ViT)，并结合窗口注意力 (Window Attention)，我们显著降低了计算开销，同时保持了原生分辨率。因此，Qwen2.5-VL 不仅在静态图像和文档理解方面表现出色，还能作为一个交互式视觉智能体，在现实世界场景中执行推理、工具使用和任务操作，例如操作计算机和移动设备。</p>
<p>该模型在各个领域都具备强大的泛化能力，无需针对特定任务进行微调。Qwen2.5-VL 提供三种尺寸，满足从边缘 AI 到高性能计算的多样化需求。其中，旗舰版 Qwen2.5-VL-72B 在文档和图表理解方面表现突出，能够与 GPT-4o 和 Claude 3.5 Sonnet 等最新顶尖模型相媲美。较小的 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B 也超越了同类竞争对手，即使在资源受限的环境下仍然具备强大的能力。此外，Qwen2.5-VL 还保持了稳健的语言能力，继承了 Qwen2.5 大语言模型 (LLM) 的核心语言能力。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311144619136.png" class="" title="image-20250311144619136">

<h2 id="1-介绍（Introduction）"><a href="#1-介绍（Introduction）" class="headerlink" title="1. 介绍（Introduction）"></a>1. 介绍（Introduction）</h2><p>大型视觉语言模型（LVLMs, Large vision-language models）是人工智能的关键突破，标志着多模式理解和交互的变革性方法。通过将视觉感知与自然语言处理无缝集成，这些高级模型从根本上重塑了机器解释和分析跨不同领域的复杂信息的方式。尽管多模态大型语言模型取得了重大进步，但这些模型目前的功能可以被比作三明治饼干的中间层——能够胜任各种任务，但却缺乏出色的性能。精细粒度视觉任务构成了这个类比的基础层。在Qwen2.5-VL的这次迭代中，我们致力于探索细粒度的感知能力，旨在为LVLMS建立一个坚实的基础，并为现实世界的应用创建一个代理放大器。该框架的顶层是多模态推理，它通过利用最新的Qwen2.5 LLM和使用多模态QA数据构建而得到增强。</p>
<p>一系列的作品促进了多模态大型模型的发展，其特点是架构设计、视觉输入处理和数据管理。LVLMs进步的主要驱动力之一是体系结构的持续创新。（Alayrac等，2022年；李等，2022a；2023b；Liu等，2023b；a；王等，2024i；张等，2024b；王等，2023）逐步塑造了当前范式，通常由视觉编码器、跨模态投影仪和LLM组成。细粒度感知模型已经成为另一个关键领域。模型如（Xiao等人，2023年；刘等人，2023年；任等人，2024年；张等人，2024a；d；彭等人，2023；Deitke等人，2024）突破了详细视觉理解方面可能的边界。Omni（Li等人，2024g；2025b；Ye等人，2024）和MoE（Riquelme等人，2021；Lee等人，2024；Li等人，2024h；c；Wu等人，2024b）的架构也激发了LVLMs的未来发展。视觉编码器的增强（Chen等，2023；Liu等，2024b；Liang等，2025）和分辨率缩放（李等，2023c；叶等人，2023；李等人，2023a)在提高实际视觉理解质量方面发挥了关键作用。管理更多样化的场景和更高质量的数据是训练高级LVLMS的必要步骤。（郭等，2024年；2024年陈等，2024d；刘等，2024a；陈等，2024a；唐等，2024；李等，2024a)对这项努力具有非常有价值的贡献。</p>
<p>然而，尽管视觉语言模型取得了显著的进展，但它们目前仍面临着发展瓶颈，包括计算复杂性、有限的上下文理解、糟糕的细粒度视觉感知，以及在不同序列长度上不一致的表现。</p>
<p>在本报告中，我们介绍了最新的工作Qwen2.5-VL，它延续了Qwen系列的开源哲学，在各种基准测试上实现甚至超越了顶级的闭源代码模型。从技术上讲，我们的贡献有四方面： </p>
<ol>
<li>在视觉编码器中实现窗口注意力，以优化推理效率；</li>
<li>我们引入了动态FPS采样，将动态分辨率扩展到时间维度，并能够在不同的采样率下实现全面的视频理解；</li>
<li>我们通过对齐绝对时间来升级时间域中的MRoPE，从而促进了更复杂的时间序列学习；</li>
<li>我们在管理训练前和监督微调的高质量数据方面做了重大努力，进一步将训练前语料库从1.2万亿标记扩展到4.1万亿个tokens。</li>
</ol>
<p>Qwen2.5-VL的突出特点是：</p>
<ul>
<li>强大的文档解析功能： Qwen2.5-VL将文本识别升级到完整解析，在处理多场景、多语言和各种内置的（手写、表格、图表、化学公式和音乐表）文档方面表现出色。</li>
<li>跨格式的精确对象接地： Qwen2.5-VL解锁提高了检测、指向和计数对象的精度，适应了绝对坐标和JSON格式的高级空间推理。</li>
<li>超长视频理解和细粒度视频接地：我们的模型将原生动态分辨率扩展到时间维度，增强了理解持续数小时的视频的能力，同时在秒内提取事件片段。</li>
<li>增强的计算机和移动设备的代理(agent)功能：针对计算机和移动设备的增强代理功能：利用先进的基础、推理和决策能力，在智能手机和计算机上使用优越的代理功能来增强模型。</li>
</ul>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311152513173.png" class="" title="image-20250311152513173">

<p>图片1：Qwen2.5-VL框架演示了一个视觉编码器和一个语言模型解码器的集成来处理多模态输入，包括图像和视频。视觉编码器被设计用来处理其本地分辨率的输入，并支持动态FPS采样。不同大小的图像和具有不同FPS速率的视频帧被动态地映射到不同长度的token序列上。值得注意的是，MRoPE将时间id与时间维度上的绝对时间对齐，使模型能够更好地理解时间动态，比如事件的节奏和精确的时刻定位。处理后的视觉数据随后被输入Qwen2.5 LM解码器。我们重新设计了视觉transformer（ViT）架构，结合了先进的组件，如带有SwiGLU激活函数的FFN，RMSNorm用于标准化，以及基于窗口的注意机制来提高性能和效率。</p>
<h2 id="2-方法（Approach）"><a href="#2-方法（Approach）" class="headerlink" title="2. 方法（Approach）"></a>2. 方法（Approach）</h2><p>在本节中，我们首先概述了Qwen2.5-VL系列模型的体系结构更新，并提供了数据和训练细节的概述。</p>
<h3 id="2-1-模型架构"><a href="#2-1-模型架构" class="headerlink" title="2.1 模型架构"></a>2.1 模型架构</h3><p>Qwen2.5-VL的整体模型体系结构由三个部分组成：</p>
<p><strong>大语言模型（Large Language Model）：</strong> Qwen2.5-VL系列采用大型语言模型作为其基本部分。该模型用Qwen2.5 LLM中预先训练的权重进行初始化。为了更好地满足多模态理解的要求，我们将一维RoPE（Rotary Position Embedding, 旋转位置嵌入）修改为我们的与绝对时间对齐的多模态旋转位置嵌入。</p>
<p><strong>视觉编码器：</strong> Qwen2.5-VL的视觉编码器采用了重新设计的视觉Transformer（ViT）架构。在结构上，我们结合了2D-RoPE和窗口注意力来支持原生输入分辨率，同时加速了整个视觉编码器的计算。在训练和推理过程中，输入图像的高度和宽度被调整到28的倍数，然后被输入ViT。视觉编码器通过将图像分割成步长为14的块来处理图像，生成一组图像特征。我们在第2.1.1节中提供了对视觉编码器的更详细的介绍。</p>
<p><strong>基于MLP的视觉语言合并：</strong> 为了解决长图像特征序列带来的效率挑战，我们采用一种简单而有效的方法来压缩特征序列，然后将其输入大型语言模型（LLM）。具体来说，我们不是直接使用ViT提取的原始块特征，而是首先将四个块特征的空间相邻集进行分组。然后，将这些分组的特征连接起来，并通过一个两级多层感知器（MLP），将它们投射到一个与LLM中使用的文本嵌入对齐的维度中。该方法不仅降低了计算成本，而且为动态压缩不同长度的图像特征序列提供了一种灵活的方法。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250311161912268.png" class="" title="image-20250311161912268">

<h4 id="2-1-1-快速和高效的视觉编码器"><a href="#2-1-1-快速和高效的视觉编码器" class="headerlink" title="2.1.1 快速和高效的视觉编码器"></a>2.1.1 快速和高效的视觉编码器</h4><p>视觉编码器在多模态大语言模型（MLLMs）中起着关键的作用。为了解决由于原生分辨率输入而导致的训练和推理过程中的计算负载不平衡所带来的挑战，我们重新设计了视觉变压器（ViT）架构。一个关键的问题来自于与处理不同大小的图像相关的二次计算复杂度。为了缓解这种情况，我们在大多数层中引入了窗口注意力，以确保计算成本与块的数量呈线性扩展，而不是二次方。在我们的架构中，只有四个层采用了完全的自我注意，而其余的层利用了窗口注意力，最大窗口大小为112×112（对应于8个×8块）。对小于112×112的区域不进行填充，保持其原始分辨率。这种设计允许模型在原生输入分辨率下进行，避免了不必要的缩放或失真。</p>
<p>对于位置编码，我们采用二维旋转位置嵌入（RoPE）来有效地捕获二维空间中的空间关系。此外，为了更好地处理视频输入，我们将我们的方法扩展到3D块分区。具体来说，我们使用14个×14个图像块作为基本单元，与传统的静态图像ViTs相一致。对于视频数据，两个连续的帧被分组在一起，显著减少了输入到语言模型中的token的数量。这种设计不仅保持了与现有架构的兼容性，而且提高了处理顺序视频数据的效率。</p>
<p>为了简化整体的网络结构，我们将ViT体系结构与大型语言模型（LLMs）的设计原则更紧密地对齐。具体来说，我们采用RMSNorm（Zhang &amp;Sennrich，2019）进行归一化，并采用SwiGLU（Dauphin et al.，2017）作为激活函数。这些选择提高了计算效率和模型的视觉和语言组件之间的兼容性。</p>
<p>在训练方面，我们从头开始训练重新设计的ViT。培训过程包括几个阶段，包括CLIP预训练、视觉-语言对齐和端到端微调。为了确保在不同的输入分辨率之间的鲁棒性，我们在训练阶段采用了原生分辨率的动态采样。图像根据其原始的纵横比进行随机采样，使模型能够有效地推广到不同分辨率的输入。这种方法不仅提高了模型的适应性，而且确保了跨不同大小的视觉数据的稳定和有效的训练。</p>
<h4 id="2-1-2-原生动态分辨率和帧率"><a href="#2-1-2-原生动态分辨率和帧率" class="headerlink" title="2.1.2 原生动态分辨率和帧率"></a>2.1.2 原生动态分辨率和帧率</h4><p>Qwen2.5-VL在空间和时间维度上都引入了进步，以有效地处理不同的多模态输入。</p>
<p>在空间域中，Qwen2.5-VL动态地将不同大小的图像转换为具有相应长度的标记序列。与传统的坐标规格化方法不同，我们的模型直接使用输入图像的实际尺寸来表示边界框、点和其他空间特征。这允许模型固有地学习比例信息，提高其处理不同分辨率图像的能力。</p>
<p>对于视频输入，Qwen2.5-VL结合了动态帧率（FPS）训练和绝对时间编码。通过适应可变帧率，该模型可以更好地捕捉视频内容的时间动态。不同于其他方法通过引入文本时间戳或使用额外的头部来实现时间对齐，我们提出了一种新颖且高效的策略，直接将 MRoPE ID 与时间戳对齐。这种方法允许模型通过时间维度id之间的间隔来理解时间的节奏，而不需要任何额外的计算。</p>
<h4 id="2-1-3-多模态旋转位置嵌入与绝对时间对齐"><a href="#2-1-3-多模态旋转位置嵌入与绝对时间对齐" class="headerlink" title="2.1.3 多模态旋转位置嵌入与绝对时间对齐"></a>2.1.3 多模态旋转位置嵌入与绝对时间对齐</h4><p>位置嵌入对于在视觉和语言模式中建模顺序数据都是至关重要的。基于Qwen2-VL中引入的多模态旋转位置嵌入（MRoPE），我们扩展了其功能，以更好地处理视频中的时间信息。</p>
<p>Qwen2-VL中的MRoPE将位置嵌入分解为三个不同的分量：时间、高度和宽度，以有效地建模多模态输入。对于文本输入，所有三个组件都使用相同的位置id，使得MRoPE在功能上等同于传统的1D RoPE（Su et al.，2024）。对于图像，时间ID在视觉token之间保持不变，而唯一的ID则根据每个token在图像中的空间位置分配给高度和宽度组件。当处理被视为帧序列的视频时，每一帧的时间ID都有增量，而高度和宽度组件遵循与静态图像相同的分配模式。</p>
<p>然而，在Qwen2-VL中，MRoPE中的时间位置id与输入帧数联系在一起，这并没有解释内容变化的速度或视频内事件的绝对时间。为了解决这一限制，Qwen2.5-VL引入了一个关键的改进：将MRoPE的时间组件与绝对时间对齐。如图1所示，通过利用时间id之间的间隔，该模型能够学习具有不同FPS采样率的视频之间一致的时间对齐。</p>
<h3 id="2-2-预训练"><a href="#2-2-预训练" class="headerlink" title="2.2 预训练"></a>2.2 预训练</h3><p>在本节中，我们首先描述了预训练数据集的构建，然后概述了整个训练管道和配置。</p>
<h4 id="2-2-1预训练数据集"><a href="#2-2-1预训练数据集" class="headerlink" title="2.2.1预训练数据集"></a>2.2.1预训练数据集</h4><p>与Qwen2-VL相比，我们显著增加了预训练数据的数量，从1.2万亿tokens增加到大约4万亿tokens。我们的预训练数据集是通过多种方法的组合来构建的，包括原始的网络数据，合成数据等。该数据集包括各种各样的多模态数据，如图像标题、交错图像-文本数据、光学字符识别（OCR）数据、视觉知识（如名人、地标、植物和动物识别）、多模态学术问题、位置数据、文档解析数据、视频描述、视频定位和基于代理的交互数据。在整个训练过程中，我们在不同的阶段仔细调整了这些数据类型的组成和比例，以优化学习结果。</p>
<p><strong>交错图像文本数据：</strong> 交错图像文本数据对多模式学习至关重要，提供三个关键的好处： </p>
<ol>
<li>使上下文学习同时视觉和文本线索（Alayrac等，2022）；</li>
<li>当没有图像时保持强大的文本功能（Lin et al.，2024）；</li>
<li>包含广泛的一般信息。</li>
</ol>
<p>然而，许多可用的交错数据缺乏有意义的文本-图像关联，而且往往是嘈杂的，限制了它在复杂推理和创造性生成方面的有用性。</p>
<p>为了解决这些挑战，我们开发了一个用于评分和清洗数据的管道，确保只使用高质量的、相关的交错数据。我们的过程包括两个步骤：标准数据清洗（Li et al.，2024e），然后是使用内部评估模型的四阶段评分系统。评分标准包括：</p>
<ol>
<li>纯文本质量，</li>
<li>图像文本相关性，</li>
<li>图像文本互补性，</li>
<li>信息密度平衡。</li>
</ol>
<p>这种细致的方法提高了模型执行复杂推理和生成连贯的多模态内容的能力。</p>
<p>以下是对这些图像-文本评分标准的描述：</p>
<p>图像-文本相关性：得分越高表示图像和文本之间的联系更强，图像有意义地补充、解释或扩展文本，而不仅仅是装饰它。</p>
<p>信息互补性：得分越高，表示图像和文本之间的互补性信息越大。每一个都应该提供独特的细节，共同创造一个完整的叙述。</p>
<p>信息密度的平衡：得分越高，意味着图像和文本之间的信息分布越平衡，避免过多的文本或图像信息，确保两者之间的适当平衡。</p>
<p><strong>具有绝对位置坐标的对齐数据：</strong> 我们采用具有绝对位置坐标的原生分辨率训练，目的是实现更准确的感知世界。相反，相对坐标不能有效地表示图像中物体的原始大小和位置。为了解决这个限制，Qwen2.5-VL在训练过程中使用基于输入图像的实际尺寸的坐标值来表示边界框和点。该方法确保了模型能够更好地捕捉目标的真实尺度和空间关系，从而提高了目标检测和定位等任务的性能。</p>
<p>为了提升模型的泛化对齐能力，我们构建了一个综合数据集，其中包含带有指代表达的边界框（bounding boxes）和关键点（points），数据来源包括公开数据集和专有数据。我们的方法涉及将数据合成为多种格式，包括 XML，JSON 以及自定义格式，并采用了一系列技术，例如 Copy-Paste 增强（Ghiasi 等，2021），以及利用现成的模型（off-the-shelf models）进行数据合成，如 Grounding DINO（Liu 等，2023c）和 SAM（Kirillov 等，2023）。这种方法有助于更稳健地评估和提升模型的对齐能力（grounding abilities）。</p>
<p>为了提高模型在开放词汇表检测方面的性能，我们将训练数据集扩展到包括超过10,000个对象类别。此外，为了提高模型在极端目标检测场景中的有效性，我们在查询中合成了不存在的对象类别，并为每个对象构建了包含多个实例的图像数据。</p>
<p>为了确保优越的基于点的对象对齐能力，我们构建了一个包含公开可用数据和合成数据的综合点数据集。具体来说，数据来源包括来自 PixMo（Deitke 等，2024）的公开指示和计数数据、公开可访问的物体对齐数据（来自物体检测和实例分割任务），以及通过自动化管道合成的精确指向特定图像细节的指示数据。</p>
<p><strong>文档全方位解析数据：</strong> 为了训练Qwen2.5-VL，我们合成了大量的文档数据语料库。解析文档内容的传统方法通常依赖于单独的模型来处理布局分析、文本提取、图表解释和插图处理。相比之下，Qwen2.5- VL的设计旨在使通用模型具有解析、理解和转换文档格式的全面功能。具体来说，我们在文档中加入了多种多样的元素，如表格、图表、方程式、自然或合成图像、乐谱和化学公式。这些元素统一采用HTML格式，它将布局框信息和插图描述集成到HTML标签结构中。我们还根据典型的阅读顺序丰富了文档布局，并在基于HTML的标注真值中包含了与每个模块（如段落和图表）对应的坐标。这种创新方法使得任何文档的完整信息，包括其布局、文本、图表和插图，都能够以标准化和统一的方式进行表示。因此，Qwen2.5-VL 实现了多模态文档元素的无缝集成，从而促进了更高效和准确的文档理解与转化。</p>
<p>下面是Qwen VL HTML格式：</p>
<figure class="highlight html"><figcaption><span>Qwen VL HTML Format</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"># paragraph</span><br><span class="line"><span class="tag">&lt;<span class="name">p</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> content <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line"># table</span><br><span class="line"><span class="tag">&lt;<span class="name">style</span>&gt;</span><span class="language-css"><span class="selector-tag">table</span>&#123;id&#125; style</span><span class="tag">&lt;/<span class="name">style</span>&gt;</span><span class="tag">&lt;<span class="name">table</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span> <span class="attr">class</span>=<span class="string">&quot;table&#123;id&#125;&quot;</span>&gt;</span> table content</span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"># chart</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;chart&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span> /&gt;</span><span class="tag">&lt;<span class="name">table</span>&gt;</span> chart content</span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"># formula</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;formula&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span> /&gt;</span> <span class="tag">&lt;<span class="name">div</span>&gt;</span> formula</span><br><span class="line">content <span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"># image caption</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;image caption&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span> /&gt;</span><span class="tag">&lt;<span class="name">p</span>&gt;</span> image</span><br><span class="line">caption <span class="tag">&lt;/<span class="name">p</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"># image ocr</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;image ocr&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span> /&gt;</span><span class="tag">&lt;<span class="name">p</span>&gt;</span> image ocr</span><br><span class="line"><span class="tag">&lt;/<span class="name">p</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"># music sheet</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;music sheet&quot;</span> <span class="attr">format</span>=<span class="string">&quot;abc notation&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1</span></span></span><br><span class="line"><span class="string"><span class="tag">x2 y2&quot;</span> /&gt;</span> <span class="tag">&lt;<span class="name">div</span>&gt;</span> music sheet content <span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"># chemical formula content</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;chemical formula&quot;</span> <span class="attr">format</span>=<span class="string">&quot;smile&quot;</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1 x2 y2&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">img</span> <span class="attr">data-bbox</span>=<span class="string">&quot;x1 y1</span></span></span><br><span class="line"><span class="string"><span class="tag">x2 y2&quot;</span> /&gt;</span> <span class="tag">&lt;<span class="name">div</span>&gt;</span> chemical formula content <span class="tag">&lt;/<span class="name">div</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这种格式确保所有文档元素都以结构化和可访问的方式表示，使Qwen2.5-VL能够高效地处理和理解。</p>
<p><strong>OCR 数据：<strong>我们收集并整理了来自不同来源的数据，以提升 OCR（光学字符识别）性能，其中包括</strong>合成数据、开源数据和内部收集的数据</strong>。</p>
<ul>
<li><strong>合成数据</strong> 由视觉文本生成引擎生成，能够模拟真实环境中的高质量文本图像。</li>
<li><strong>多语言支持</strong>：为了支持更广泛的语言并增强多语言能力，我们引入了一个大规模的<strong>多语言 OCR 数据集</strong>，涵盖<strong>法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语</strong>等多种语言。</li>
<li><strong>数据多样性与质量控制</strong>：数据集经过精心筛选，确保多样性和高质量，结合了<strong>高质量的合成图像</strong>和<strong>真实世界的自然场景图像</strong>，从而提升模型在不同语言环境中的适应性，并增强其对不同文本外观和环境条件的鲁棒性。</li>
</ul>
<p><strong>图表类数据：<strong>我们使用 <strong>matplotlib、seaborn 和 plotly</strong> 等可视化库合成了 <strong>100 万个样本</strong>，涵盖</strong>柱状图、关系图和热图</strong>等多种图表类型。</p>
<p><strong>表格类数据：<strong>我们利用离线端到端表格识别模型</strong>处理了 600 万个真实世界的表格样本</strong>，并进一步筛选出高质量数据，<strong>剔除低置信度表格、重叠表格以及单元格密度不足的表格</strong>。</p>
<p><strong>视频数据：</strong></p>
<ul>
<li><strong>FPS 适配</strong>：为了增强对不同帧率（FPS）视频数据的理解能力，我们在训练过程中<strong>动态采样 FPS</strong>，以确保训练数据集中 FPS 分布更加均衡。</li>
<li><strong>长视频字幕生成</strong>：对于<strong>时长超过半小时</strong>的视频，我们<strong>专门构建了一套长视频字幕数据集</strong>，通过<strong>目标合成管道</strong>生成<strong>多帧字幕</strong>，以提高模型对长视频内容的理解能力。</li>
<li><strong>视频对齐数据</strong>：针对视频数据的时间对齐问题，我们提供了<strong>秒级格式（second-based format）<strong>和</strong>时-分-秒-帧（hmsf）格式</strong>的时间戳，以确保模型能够准确理解和输出不同格式的时间信息。</li>
</ul>
<p><strong>智能体数据（Agent Data）：<strong>我们增强了 <strong>Qwen2.5-VL</strong> 的</strong>感知和决策能力</strong>，以构建其智能体（Agent）能力。</p>
<p><strong>感知能力（Perception）：</strong></p>
<ul>
<li>我们收集了<strong>移动端、网页端和桌面端</strong>的截图数据。</li>
<li>通过<strong>合成数据引擎</strong>生成<strong>截图描述（captions）<strong>和</strong>UI 元素对齐标注（grounding annotations）</strong>。</li>
<li><strong>截图描述任务</strong>（Caption Task）：帮助 Qwen2.5-VL 理解<strong>图形界面</strong>。</li>
<li><strong>对齐任务</strong>（Grounding Task）：帮助 Qwen2.5-VL <strong>关联 UI 元素的外观和功能</strong>。</li>
</ul>
<p><strong>决策能力（Decision-Making）：</strong></p>
<ul>
<li><strong>操作格式统一</strong>：我们将<strong>移动端、网页端和桌面端的操作</strong>统一为<strong>函数调用格式</strong>，并构建一个共享的<strong>操作空间（Action Space）</strong>。</li>
<li>多步操作数据：<ul>
<li>收集<strong>开源数据</strong>和<strong>基于虚拟环境的智能体框架（Wang et al., 2025; 2024b;c）合成的多步操作轨迹</strong>，并将其<strong>转换为函数格式</strong>。</li>
<li>通过<strong>人工和模型标注（Xu et al., 2024）</strong>，为每一步操作生成<strong>推理过程</strong>。</li>
</ul>
</li>
<li>推理数据构建流程：<ol>
<li><strong>给定一个真实操作（Ground-Truth Operation）</strong>，我们在截图上<strong>高亮显示</strong>该操作。</li>
<li><strong>提供全局查询信息</strong>，以及<strong>操作前后的截图</strong>，要求标注人员<strong>编写推理内容</strong>，解释该操作的意图。</li>
<li><strong>使用基于模型的筛选器</strong>，剔除低质量的推理内容。</li>
</ol>
</li>
</ul>
<p>这一推理内容的加入，能够防止 <strong>Qwen2.5-VL</strong> <strong>过拟合</strong>于<strong>真实操作数据</strong>，并使其在<strong>真实世界场景中更加鲁棒（robust）</strong>。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250312104636009.png" class="" title="image-20250312104636009">

<h4 id="2-2-2-训练方案（Training-Recipe）"><a href="#2-2-2-训练方案（Training-Recipe）" class="headerlink" title="2.2.2 训练方案（Training Recipe）"></a>2.2.2 训练方案（Training Recipe）</h4><p>我们从零开始训练了一个 <strong>Vision Transformer (ViT)</strong>，其中<strong>视觉编码器</strong>（Vision Encoder）使用 <strong>DataComp（Gadre et al., 2023）</strong> 和部分<strong>自有数据集</strong>进行初始化，而<strong>语言模型组件</strong>（LLM）则基于<strong>预训练的 Qwen2.5 大语言模型（Yang et al., 2024a）</strong> 进行初始化。</p>
<p>如表 2 所示，<strong>预训练过程分为三个阶段</strong>，每个阶段采用不同的数据配置和训练策略，以<strong>逐步增强模型能力</strong>。</p>
<p><strong>第一阶段：ViT 预训练，增强视觉-语言对齐能力</strong></p>
<ul>
<li>仅训练 <strong>Vision Transformer (ViT)</strong>，使其更好地与<strong>语言模型（LLM）对齐</strong>，为多模态理解奠定基础。</li>
<li>主要数据来源：<ul>
<li><strong>图像字幕数据（Image Captions）</strong></li>
<li><strong>视觉知识数据（Visual Knowledge）</strong></li>
<li><strong>OCR 数据（光学字符识别数据）</strong></li>
</ul>
</li>
<li>这些数据经过精心选择，以提升 ViT <strong>提取有意义视觉表征</strong>的能力，并确保其能<strong>有效融入文本信息</strong>。</li>
</ul>
<p><strong>第二阶段：解冻所有参数，训练多模态数据，提升复杂视觉处理能力</strong></p>
<ul>
<li><strong>解冻全部模型参数</strong>，在更<strong>丰富的多模态数据</strong>上训练，增强模型处理复杂视觉信息的能力。</li>
<li>引入更加复杂、需要推理能力的数据集，包括：<ul>
<li><strong>交错数据（Interleaved Data）</strong></li>
<li><strong>多任务学习数据集（Multi-task Learning Datasets）</strong></li>
<li><strong>视觉问答（VQA，Visual Question Answering）</strong></li>
<li><strong>多模态数学（Multimodal Mathematics）</strong></li>
<li><strong>基于智能体的任务（Agent-based Tasks）</strong></li>
<li><strong>视频理解数据（Video Understanding）</strong></li>
<li><strong>纯文本数据（Pure-text Datasets）</strong></li>
</ul>
</li>
<li>这一阶段旨在<strong>强化模型视觉与语言的深层次联系</strong>，使其能够处理<strong>更加复杂的多模态任务</strong>。</li>
</ul>
<p><strong>第三阶段：增强长序列推理能力，加入视频与智能体数据</strong></p>
<ul>
<li>进一步增强模型在<strong>长序列推理、视频和智能体任务</strong>上的能力。</li>
<li>在训练中<strong>增加序列长度</strong>，使模型能<strong>处理更长的上下文</strong>，特别是涉及<strong>长距离依赖和复杂推理的任务</strong>。</li>
</ul>
<p><strong>优化训练效率的策略</strong></p>
<p>由于训练过程中<strong>图像大小和文本长度的变化</strong>会导致<strong>计算负载不均衡</strong>，我们采取了<strong>优化策略</strong>来提升训练效率：</p>
<ul>
<li><strong>主要计算负载</strong>来自于 <strong>LLM（语言模型）和 Vision Encoder（视觉编码器）</strong>。</li>
<li>由于 <strong>视觉编码器的参数较少</strong>，并且我们引入了 <strong>窗口注意力（Window Attention）</strong> 来进一步<strong>降低计算需求</strong>，所以主要优化点是<strong>均衡 LLM 计算负载</strong>。</li>
<li>具体方法：<ul>
<li><strong>动态打包（Dynamically Packed）</strong> 数据样本，根据输入的<strong>序列长度</strong>分配给 LLM，确保计算负载均衡。</li>
<li><strong>第一、二阶段</strong>：数据序列长度<strong>统一设定为 8,192</strong>。</li>
<li><strong>第三阶段</strong>：序列长度<strong>提升至 32,768</strong>，以适应模型<strong>处理长序列任务的能力</strong>。</li>
</ul>
</li>
</ul>
<h3 id="2-3-后训练（Post-training）"><a href="#2-3-后训练（Post-training）" class="headerlink" title="2.3 后训练（Post-training）"></a>2.3 后训练（Post-training）</h3><p>Qwen2.5-VL 的后训练对齐框架采用 <strong>双阶段优化范式</strong>，包括 <strong>监督微调（Supervised Fine-Tuning, SFT）</strong> 和 <strong>直接偏好优化（Direct Preference Optimization, DPO）</strong>（Rafailov 等，2023）。这种<strong>分层对齐策略</strong>结合了<strong>参数高效的领域自适应</strong>和<strong>人类偏好蒸馏</strong>，分别针对<strong>表征对齐（representational grounding）</strong> 和<strong>行为优化（behavioral refinement）</strong> 进行优化，以满足不同的训练目标。</p>
<p><strong>监督微调（SFT）</strong></p>
<p>SFT 旨在通过<strong>针对性的指令优化</strong>，弥合<strong>预训练表征</strong>与<strong>下游任务需求</strong>之间的差距。在这一阶段，我们采用 <strong>ChatML 格式</strong>（OpenAI，2024）来构造指令跟随数据，这种格式<strong>有意区别于预训练数据模式</strong>，但仍保持与 Qwen2-VL（Wang 等，2024e）的架构一致性。这种格式转换实现了三个关键调整：</p>
<ol>
<li><strong>显式对话角色标注</strong>，支持多模态回合式交互；</li>
<li><strong>在文本指令中结构化地注入视觉嵌入</strong>，确保模型能够结合视觉信息理解任务；</li>
<li><strong>通过格式感知的数据打包，保持跨模态的位置信息关系</strong>，使模型能够理解视觉元素与文本之间的对应关系。</li>
</ol>
<p>通过在这种增强格式下，向模型提供<strong>精心挑选的多模态指令-响应对（instruction-response pairs）</strong>，SFT <strong>可以高效地传递知识，同时保持预训练特征的完整性</strong>。</p>
<h4 id="2-3-1-指令数据"><a href="#2-3-1-指令数据" class="headerlink" title="2.3.1 指令数据"></a>2.3.1 指令数据</h4><p>在<strong>监督微调（SFT）阶段，使用精心构建的数据集来提升模型在不同模态下的指令跟随能力。该数据集包含约200万条数据</strong>，其中<strong>纯文本数据占比50%</strong>，<strong>多模态数据占比50%</strong>，后者包括<strong>图文和视频文本</strong>的组合。多模态数据的引入使模型能够有效处理复杂输入。需要注意的是，虽然<strong>纯文本数据和多模态数据的数量相等</strong>，但由于多模态数据<strong>包含视觉和时间信息</strong>，其在训练过程中会消耗<strong>显著更多的 token 和计算资源</strong>。</p>
<p>该数据集主要由<strong>中英文数据</strong>组成，并补充了一定量的<strong>多语言数据</strong>，以支持更广泛的语言多样性。数据集结构设计涵盖不同层次的<strong>对话复杂度</strong>，包括<strong>单轮对话</strong>和<strong>多轮对话</strong>。这些对话进一步结合了从<strong>单张图片输入到多张图片序列</strong>的各种场景，以模拟真实的对话动态。</p>
<p>数据来源主要包括<strong>开源数据集</strong>，并辅以<strong>购买的精选数据</strong>和<strong>在线查询数据</strong>，从而确保数据集的<strong>广泛覆盖性</strong>和<strong>代表性</strong>。</p>
<p>为了适应多种应用场景，数据集中包含多个<strong>专门子集</strong>，如：</p>
<ul>
<li><strong>通用视觉问答（VQA）</strong></li>
<li><strong>图像描述（Image Captioning）</strong></li>
<li><strong>数学问题求解</strong></li>
<li><strong>编程任务</strong></li>
<li><strong>安全相关查询</strong></li>
</ul>
<p>此外，还构建了专门的数据集，以提升模型在<strong>文档与光学字符识别（Doc &amp; OCR）</strong>、<strong>目标定位（Grounding）</strong>、<strong>视频分析</strong>和<strong>智能体交互（Agent Interactions）</strong> 等领域的能力。有关数据的详细信息可在论文的相关章节中找到。</p>
<p>这种<strong>结构化且多样化</strong>的数据集设计，确保 SFT 阶段能够<strong>有效对齐预训练表示</strong>，使模型能够更好地适应<strong>多模态下游任务</strong>的需求，从而具备更强的<strong>上下文理解</strong>和<strong>任务执行</strong>能力。</p>
<h4 id="2-3-2-数据过滤流水线"><a href="#2-3-2-数据过滤流水线" class="headerlink" title="2.3.2 数据过滤流水线"></a>2.3.2 数据过滤流水线</h4><p>训练数据的质量是影响<strong>视觉-语言模型</strong>性能的关键因素。<strong>开源数据集</strong>和<strong>合成数据集</strong>通常存在较大的<strong>变异性</strong>，其中可能包含<strong>噪声数据、冗余数据或低质量样本</strong>。因此，必须进行严格的数据清理和过滤，以解决这些问题。<strong>低质量数据</strong>可能导致<strong>预训练表示</strong>与下游任务的要求无法良好对齐，从而降低模型处理复杂多模态任务的能力。因此，确保数据质量至关重要，以实现<strong>稳健</strong>且<strong>可靠</strong>的模型性能。</p>
<p><strong>两阶段数据过滤流水线：<strong>为了解决这些挑战，我们构建了一条</strong>两阶段数据过滤流水线</strong>，以系统性地提升<strong>监督微调（SFT）</strong> 数据集的质量。该流水线包括以下两个阶段：</p>
<p><strong>第一阶段：领域特定分类</strong>，在第一阶段，我们使用<strong>Qwen2-VL-Instag</strong>（由<strong>Qwen2-VL-72B</strong>衍生而来的<strong>专业分类模型</strong>）对<strong>问答（QA）对</strong>进行<strong>分层分类</strong>。该模型将 QA 对划分为<strong>八个主要领域</strong>（如<strong>编程</strong>和<strong>规划</strong>），并进一步细分为<strong>30个子类别</strong>。</p>
<p>例如，<strong>编程（Coding）</strong> 这一主要领域下包含以下子类别：</p>
<ul>
<li><strong>Code_Debugging（代码调试）</strong></li>
<li><strong>Code_Generation（代码生成）</strong></li>
<li><strong>Code_Translation（代码翻译）</strong></li>
<li><strong>Code_Understanding（代码理解）</strong></li>
</ul>
<p>这种<strong>分层结构</strong>有助于<strong>领域感知</strong>和<strong>子领域感知</strong>的过滤策略，使流水线能够针对不同类别的数据<strong>优化数据清理流程</strong>。从而<strong>提高 SFT 数据集的质量和相关性</strong>。</p>
<p><strong>第二阶段：领域定制化过滤，<strong>在第二阶段，我们实施</strong>领域定制化过滤</strong>，结合<strong>基于规则</strong>和<strong>基于模型</strong>的方法，以全面提升数据质量。由于<strong>文档处理、光学字符识别（OCR）、视觉目标定位（Visual Grounding）<strong>等领域的多样性，每个领域可能需要</strong>独特的过滤策略</strong>。以下是这些领域中采用的通用过滤策略概述：</p>
<p><strong>1. 基于规则的过滤（Rule-Based Filtering）</strong></p>
<ul>
<li>采用<strong>预定义的启发式规则</strong>来<strong>剔除低质量或存在问题的数据</strong>。</li>
<li>针对<strong>文档处理、OCR、视觉目标定位</strong>等任务，识别并移除<strong>重复模式</strong>，以防止模型学习过程受损，并确保最优性能。</li>
<li>剔除<strong>不完整、被截断或格式错误</strong>的响应，这些问题在<strong>合成数据集</strong>和<strong>多模态任务</strong>中较为常见。</li>
<li>为了保持<strong>数据的相关性</strong>并符合<strong>伦理标准</strong>，会移除<strong>无关或可能导致有害输出</strong>的查询和答案。</li>
<li>这种<strong>结构化方法</strong>确保数据集符合<strong>伦理准则</strong>，并满足<strong>特定任务的要求</strong>。</li>
</ul>
<p><strong>2. 基于模型的过滤（Model-Based Filtering）</strong></p>
<ul>
<li>进一步利用<strong>Qwen2.5-VL 系列</strong>训练的<strong>奖励模型（Reward Models）</strong> 对数据集进行精细筛选。</li>
<li>对多模态 QA 对进行多维度评估，其中：<ul>
<li><strong>查询（Query）<strong>会被评估其</strong>复杂度和相关性</strong>，确保保留<strong>适当具有挑战性且上下文相关的示例</strong>。</li>
<li><strong>答案（Answer）<strong>则依据</strong>正确性、完整性、清晰度、相关性和有用性</strong>等指标进行筛选。</li>
<li>在<strong>视觉相关任务</strong>中，会特别关注<strong>视觉信息的准确解读与应用</strong>，确保答案能够<strong>正确利用视觉信息</strong>。</li>
</ul>
</li>
</ul>
<p>这种<strong>多维评分机制</strong>确保<strong>只有高质量的数据</strong>才会进入 SFT 阶段，从而进一步提升模型的性能和泛化能力。</p>
<h4 id="2-3-3-基于拒绝采样的增强推理"><a href="#2-3-3-基于拒绝采样的增强推理" class="headerlink" title="2.3.3 基于拒绝采样的增强推理"></a>2.3.3 <strong>基于拒绝采样的增强推理</strong></h4><p>为了<strong>补充结构化数据过滤流水线</strong>，我们采用<strong>拒绝采样（Rejection Sampling）<strong>策略来优化数据集，并提升</strong>视觉-语言模型（VLM）<strong>的推理能力。该方法在</strong>需要复杂推理</strong>的任务中尤为关键，例如<strong>数学问题求解、代码生成</strong>以及<strong>特定领域的视觉问答（VQA）</strong>。</p>
<p>研究表明，<strong>链式思维（Chain-of-Thought, CoT）推理（Wei et al., 2022）能显著提升模型的推理能力</strong>（DeepSeek-AI et al., 2024）。我们的<strong>后训练实验</strong>也证实了这一点，进一步凸显了<strong>结构化推理过程</strong>对<strong>高质量推理结果</strong>的重要性。</p>
<p><strong>拒绝采样流程：<strong>该过程始于</strong>带有标准答案的高质量数据集</strong>，其中包含需要<strong>多步推理</strong>的任务，例如<strong>数学问题求解、代码生成和特定领域的 VQA</strong>。然后，我们使用<strong>Qwen2.5-VL 模型的中间版本</strong>对<strong>模型生成的回答</strong>与<strong>标准答案</strong>进行比对，<strong>仅保留</strong>模型输出与期望答案<strong>匹配</strong>的样本，从而确保数据集仅由<strong>高质量、准确的示例</strong>组成。</p>
<p>为了进一步提高数据质量，我们施加<strong>额外约束</strong>，以过滤掉不理想的输出，具体包括：</p>
<ul>
<li><strong>避免代码混用（code-switching）</strong>：排除含有不同语言混杂的回答，确保语言一致性。</li>
<li><strong>控制输出长度</strong>：去除<strong>过长</strong>的回答，避免冗余信息影响推理质量。</li>
<li><strong>剔除重复模式</strong>：移除<strong>重复句式</strong>或<strong>模式化表达</strong>，保证回答的<strong>清晰性</strong>和<strong>连贯性</strong>。</li>
</ul>
<p>这些标准有助于确保<strong>CoT 推理过程的清晰度和连贯性</strong>，从而提高下游任务的性能。</p>
<p><strong>视觉-语言模型中的 CoT 挑战与优化：<strong>在视觉-语言模型中应用</strong>CoT 推理</strong>存在一项核心挑战：模型必须<strong>同时依赖文本和视觉模态</strong>，但<strong>中间推理步骤可能无法充分整合视觉信息</strong>，具体表现为：</p>
<ul>
<li><strong>忽略关键视觉线索</strong>（例如没有参考图片中的关键信息）。</li>
<li><strong>错误解读视觉内容</strong>（例如误判图像中的物体或文字）。</li>
</ul>
<p>为了解决这些问题，我们开发了<strong>基于规则</strong>和<strong>基于模型</strong>的过滤策略，以<strong>验证中间推理步骤的准确性</strong>。这些机制确保<strong>每一步推理过程都能有效结合视觉和文本信息</strong>。然而，实现<strong>最佳的模态对齐</strong>仍是一个<strong>持续优化的挑战</strong>，需要进一步的技术突破。</p>
<p><strong>拒绝采样对模型推理能力的提升：<strong>通过</strong>迭代优化数据集</strong>并<strong>移除低质量或错误样本</strong>，拒绝采样显著提升了模型的<strong>推理能力</strong>。该方法确保模型学习<strong>高质量、高保真度</strong>的示例，使其能够在<strong>复杂任务</strong>中表现得更加<strong>准确</strong>和<strong>连贯</strong>。</p>
<p>这种方法不仅<strong>增强了模型对多步推理任务的处理能力</strong>，还<strong>为未来的视觉-语言建模改进奠定了基础</strong>。</p>
<h4 id="2-3-4-训练方案"><a href="#2-3-4-训练方案" class="headerlink" title="2.3.4 训练方案"></a>2.3.4 <strong>训练方案</strong></h4><p>Qwen2.5-VL 的<strong>后训练过程</strong>包括两个阶段：</p>
<ol>
<li><strong>监督微调（Supervised Fine-Tuning, SFT）</strong></li>
<li><strong>直接偏好优化（Direct Preference Optimization, DPO）</strong></li>
</ol>
<p>在这两个阶段中，<strong>视觉变换器（Vision Transformer, ViT）参数保持冻结</strong>。</p>
<p><strong>SFT 阶段（监督微调）：<strong>在 SFT 阶段，模型在</strong>多样化的多模态数据</strong>上进行<strong>微调</strong>，这些数据包括：</p>
<ul>
<li><strong>图文数据对（image-text pairs）</strong></li>
<li><strong>视频数据（video）</strong></li>
<li><strong>纯文本数据（pure text）</strong></li>
</ul>
<p>数据来源涵盖：</p>
<ul>
<li><strong>通用 VQA 数据</strong>（General Visual Question Answering）</li>
<li><strong>拒绝采样数据（Rejection Sampling）</strong></li>
<li><strong>专业领域数据集</strong>（如<strong>文档识别（Document and OCR）</strong>、<strong>目标定位（Grounding）</strong>、<strong>视频分析（Video）<strong>和</strong>智能体相关任务（Agent-related tasks）</strong>）</li>
</ul>
<p><strong>DPO 阶段（直接偏好优化）：<strong>DPO 阶段专注于</strong>图文数据</strong>和<strong>纯文本数据</strong>，主要使用<strong>偏好数据（Preference Data）<strong>来优化模型，使其更加符合</strong>人类偏好</strong>。</p>
<ul>
<li><strong>每个样本仅处理一次</strong>，确保优化过程高效。</li>
<li>该阶段的目标：<ul>
<li>提升<strong>跨模态推理能力</strong></li>
<li><strong>优化特定任务表现</strong></li>
<li><strong>对齐用户意图</strong>，使模型生成的答案更符合用户需求</li>
</ul>
</li>
</ul>
<p>这种<strong>高效的训练流程</strong>在<strong>提升模型能力</strong>的同时，确保其与<strong>人类偏好保持一致</strong>。</p>
<hr>
<h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><p>在本节中，我们首先介绍整体模型，并将其与当前最先进（SoTA）模型进行比较。随后，我们评估该模型在各个子能力上的表现。</p>
<h3 id="3-1-与先进模型比较"><a href="#3-1-与先进模型比较" class="headerlink" title="3.1 与先进模型比较"></a>3.1 与先进模型比较</h3><img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314140802625.png" class="" title="image-20250314140802625">

<p>本实验部分评估了 Qwen2.5-VL 在多个数据集上的表现，并将其与当前最先进（SoTA）模型进行比较，包括 <strong>Claude-3.5-Sonnet-0620</strong>（Anthropic, 2024a）、<strong>GPT-4o-0513</strong>（OpenAI, 2024）、<strong>InternVL2.5</strong>（Chen et al., 2024d），以及不同规模的 <strong>Qwen2-VL</strong>（Wang et al., 2024e）。</p>
<p><strong>大学水平任务：</strong> 在大学水平的问题上，<strong>Qwen2.5-VL-72B</strong> 在 <strong>MMMU</strong>（Yue et al., 2023）上取得了 <strong>70.2</strong> 分。<br> 在更高难度的 <strong>MMMU-Pro</strong>（Yue et al., 2024）上，该模型得分 <strong>51.1</strong>，超越了此前的开源 SoTA 模型，并达到了 <strong>接近 GPT-4o 的性能</strong>。</p>
<p><strong>数学相关任务：</strong></p>
<ul>
<li>在 <strong>MathVista</strong>（Lu et al., 2024）上，Qwen2.5-VL-72B <strong>得分 74.8</strong>，超越了此前开源 SoTA 记录（72.3）。</li>
<li>在 <strong>MATH-Vision</strong>（Wang et al., 2024d）上，该模型 <strong>得分 38.1</strong>。</li>
<li>在 <strong>MathVerse</strong>（Zhang et al., 2024c）上，Qwen2.5-VL-72B <strong>得分 57.6</strong>，表现与其他领先模型具有竞争力。</li>
</ul>
<p><strong>通用视觉问答（VQA）任务：</strong></p>
<ul>
<li>在 <strong>MMBench-EN</strong>（Liu et al., 2023d）上，该模型 <strong>得分 88.6</strong>，略微超越了此前最高分 <strong>88.3</strong>。</li>
<li>在 <strong>MuirBench</strong>（Wang et al., 2024a）上 <strong>得分 70.7</strong>，在 <strong>BLINK</strong>（Fu et al., 2024c）上 <strong>得分 64.4</strong>，均表现优异。</li>
<li>在多语言视觉问答任务 <strong>MTVQA</strong>（Tang et al., 2024）中，Qwen2.5-VL-72B <strong>得分 31.7</strong>，展现出强大的多语言文本识别能力。</li>
<li>在主观评测任务中：<ul>
<li><strong>MMVet</strong>（Yu et al., 2024）上得分 <strong>76.2</strong></li>
<li><strong>MM-MT-Bench</strong>（Agrawal et al., 2024）上得分 <strong>7.6</strong><br> 这表明 Qwen2.5-VL-72B 在<strong>自然对话体验和用户满意度</strong>方面表现出色。</li>
</ul>
</li>
</ul>
<h3 id="3-2-纯文本任务的表现"><a href="#3-2-纯文本任务的表现" class="headerlink" title="3.2 纯文本任务的表现"></a>3.2 纯文本任务的表现</h3><p>为了全面评估指令微调模型在 <strong>纯文本任务</strong> 上的表现，我们选取了多个具有代表性的基准测试（详见 <strong>表 4</strong>），涵盖以下领域：</p>
<ul>
<li><strong>通用任务</strong>（Wang et al., 2024j; Gema et al., 2024; White et al., 2024）</li>
<li><strong>数学与科学任务</strong>（Rein et al., 2023; Hendrycks et al., 2021; Cobbe et al., 2021）</li>
<li><strong>编程任务</strong>（Chen et al., 2021; Cassano et al., 2023）</li>
<li><strong>对齐任务</strong>（Zhou et al., 2023）</li>
</ul>
<p>我们将 <strong>Qwen2.5-VL</strong> 与 <strong>多种同规模的大型语言模型（LLMs）</strong> 进行了对比。结果表明，<strong>Qwen2.5-VL 不仅在多模态任务上达到了最先进（SoTA）水平，同时在纯文本任务上也展现出了领先的性能</strong>，证明了其在<strong>多种评测标准下的多功能性和稳健性</strong>。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314142212215.png" class="" title="image-20250314142212215">

<hr>
<h3 id="3-3-定量结果"><a href="#3-3-定量结果" class="headerlink" title="3.3 定量结果"></a>3.3 定量结果</h3><h4 id="3-3-1-通用视觉问答（VQA）"><a href="#3-3-1-通用视觉问答（VQA）" class="headerlink" title="3.3.1 通用视觉问答（VQA）"></a>3.3.1 通用视觉问答（VQA）</h4><p>为了全面评估模型在 <strong>通用视觉问答（VQA）</strong> 和 <strong>对话</strong> 方面的能力，我们在多个 <strong>多样化数据集</strong> 上进行了广泛实验。<strong>如表 3 所示</strong>，Qwen2.5-VL 在 <strong>VQA 任务、主观评测、多语言场景和多图像问题</strong> 上均展现出 <strong>最先进（SoTA）</strong> 的表现。</p>
<p>具体而言，该模型在以下 <strong>基准数据集</strong> 上表现卓越：</p>
<ul>
<li><strong>MMBench 系列</strong>（Liu et al., 2023d）</li>
<li><strong>MMStar</strong>（Chen et al., 2024c）</li>
<li><strong>MME</strong>（Fu et al., 2023）</li>
<li><strong>MuirBench</strong>（Wang et al., 2024a）</li>
<li><strong>BLINK</strong>（Fu et al., 2024c）</li>
<li><strong>CRPE</strong>（Wang et al., 2024h）</li>
<li><strong>HallBench</strong>（Guan et al., 2023）</li>
<li><strong>MTVQA</strong>（Tang et al., 2024）</li>
<li><strong>MME-RealWorld</strong>（Zhang et al., 2024f）</li>
<li><strong>MMVet</strong>（Yu et al., 2024）</li>
<li><strong>MM-MT-Bench</strong>（Agrawal et al., 2024）</li>
</ul>
<p><strong>视觉细节理解与推理能力：</strong></p>
<ul>
<li>在 <strong>MMBench-EN-V1.1</strong> 数据集上，<strong>Qwen2.5-VL-72B</strong> 取得 <strong>88.4%</strong> 的准确率，超越了 <strong>InternVL2.5（78B）</strong> 和 <strong>Claude-3.5 Sonnet-0620</strong> 等 <strong>最先进模型</strong>。</li>
<li>在 <strong>MMStar</strong> 数据集上，Qwen2.5-VL 得分 <strong>70.8%</strong>，领先于其他主流模型。</li>
</ul>
<p>这些结果表明，<strong>Qwen2.5-VL 具有强大的适应性，能够在多种语言环境下保持高性能</strong>。</p>
<p><strong>高分辨率真实场景适应能力：</strong></p>
<ul>
<li>在 <strong>MME-RealWorld</strong> 基准测试中，Qwen2.5-VL 取得 <strong>63.2</strong> 分，展现出<strong>卓越的现实环境适应能力</strong>。</li>
<li>在 <strong>MuirBench</strong> 数据集中，该模型在 <strong>多图像理解任务</strong> 上得分 <strong>70.7</strong>，进一步证明其 <strong>优越的泛化能力</strong>。</li>
</ul>
<p>综合来看，这些结果表明，Qwen2.5-VL 在 <strong>通用视觉问答（VQA）任务</strong> 中 <strong>表现出极强的多功能性和高效性</strong>。</p>
<p><strong>小规模模型的竞争力：</strong></p>
<p>值得注意的是，即使是<strong>较小规模</strong> 的 <strong>Qwen2.5-VL-7B</strong> 和 <strong>Qwen2.5-VL-3B</strong> 也表现出 <strong>极具竞争力的性能</strong>：</p>
<ul>
<li><strong>Qwen2.5-VL-7B</strong> 在 <strong>MMStar</strong> 数据集上取得 <strong>63.9%</strong> 的得分。</li>
<li><strong>Qwen2.5-VL-3B</strong> 在 <strong>MMStar</strong> 数据集上得分 <strong>55.9%</strong>。</li>
</ul>
<p>这表明 <strong>Qwen2.5-VL 的架构不仅强大，而且具有良好的可扩展性，在减少参数的情况下依然能保持出色性能</strong>。</p>
<h4 id="3-3-2-文档理解与-OCR"><a href="#3-3-2-文档理解与-OCR" class="headerlink" title="3.3.2 文档理解与 OCR"></a>3.3.2 文档理解与 OCR</h4><p>我们在 <strong>OCR、图表和文档理解</strong> 相关的多个基准数据集上评估了 Qwen2.5-VL 模型的性能。<strong>表 5</strong> 展示了 Qwen2.5-VL 各版本与最先进（SoTA）模型在以下 <strong>OCR 相关基准</strong> 上的对比：</p>
<ul>
<li><strong>AI2D</strong>（Kembhavi et al., 2016）</li>
<li><strong>TextVQA</strong>（Singh et al., 2019）</li>
<li><strong>DocVQA</strong>（Mathew et al., 2021b）</li>
<li><strong>InfoVQA</strong>（Mathew et al., 2021a）</li>
<li><strong>ChartQA</strong>（Masry et al., 2022）</li>
<li><strong>CharXiv</strong>（Wang et al., 2024k）</li>
<li><strong>SEED-Bench-2-Plus</strong>（Li et al., 2024b）</li>
<li><strong>OCRBench</strong>（Liu et al., 2023e）</li>
<li><strong>OCRBench_v2</strong>（Fu et al., 2024b）</li>
<li><strong>CC-OCR</strong>（Yang et al., 2024b）</li>
<li><strong>OmniDocBench</strong>（Ouyang et al., 2024）</li>
<li><strong>VCR</strong>（Zhang et al., 2024e）</li>
</ul>
<p><strong>OCR 解析任务：</strong></p>
<p>对于 <strong>多场景、多语言以及不同类型文档（如手写文本、表格、图表、化学公式、数学表达式）</strong> 的 OCR 解析任务，Qwen2.5-VL-72B <strong>在 CC-OCR 和 OmniDocBench 数据集上刷新了最先进记录（SoTA）</strong>。这一成果得益于<strong>精心挑选的训练数据</strong> 和 <strong>LLM 强大的文本解析能力</strong>。</p>
<p><strong>OCR 相关理解任务：</strong></p>
<p>在 <strong>场景文本、图表、示意图及文档理解</strong>相关的基准测试中，Qwen2.5-VL 取得了<strong>卓越的性能</strong>，展现出 <strong>出色的文档理解能力</strong>。</p>
<ul>
<li>在 <strong>综合 OCR 理解基准</strong>（如 OCRBench和InfoVQA）上，Qwen2.5-VL-72B取得了<strong>显著领先的成绩</strong>，远超 <strong>InternVL2.5-78B</strong> 等强劲对手。</li>
<li>在<strong>涵盖图表、地图、网页等文本密集型场景</strong> 的 <strong>SEED-Bench-2-Plus</strong> 数据集中，Qwen2.5-VL-72B <strong>表现出色</strong>，进一步验证了其卓越的文档处理能力。</li>
</ul>
<p><strong>OCR 综合基准测试：</strong></p>
<p>在 <strong>OCRBench_v2</strong> 这一涵盖 <strong>OCR 解析与理解</strong> 任务的综合基准上，Qwen2.5-VL <strong>同样实现了领先的性能</strong>。值得注意的是，相较于 <strong>Gemini 1.5-Pro</strong>，Qwen2.5-VL 在 <strong>英文 OCR 任务上提升了 9.6%</strong>，在 <strong>中文 OCR 任务上更是提升了 20.6%</strong>，大幅超越当前最先进模型。</p>
<p>这些结果表明，Qwen2.5-VL <strong>在文档理解和 OCR 任务上达到了行业领先水平</strong>，具备强大的<strong>跨语言文档解析和文本理解能力</strong>。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314143316009.png" class="" title="image-20250314143316009">

<h4 id="3-3-3-空间理解"><a href="#3-3-3-空间理解" class="headerlink" title="3.3.3 空间理解"></a><strong>3.3.3 空间理解</strong></h4><p>理解空间关系对于开发能够像人类一样 <strong>解释和交互世界</strong> 的 AI 模型至关重要。在<strong>大规模视觉-语言模型（LVLMs）</strong> 中，<strong>视觉定位（visual grounding）</strong> 使 AI 能够根据 <strong>自然语言查询或描述</strong>在图像中<strong>精确地定位和识别</strong>特定的 <strong>物体、区域或元素</strong>。这一能力超越了传统的目标检测，不仅能够检测物体，还能在 <strong>视觉内容与语言上下文之间建立语义关系</strong>，从而实现更<strong>细粒度和上下文感知的视觉推理</strong>。</p>
<p>我们评估了 Qwen2.5-VL 在以下<strong>视觉定位基准</strong>上的表现：</p>
<ul>
<li><strong>指代表达理解基准（Referring Expression Comprehension）</strong>（Kazemzadeh et al., 2014; Mao et al., 2016）</li>
<li><strong>自然环境中的目标检测（Object Detection in the Wild, ODinW）</strong>（Li et al., 2022b）</li>
<li><strong>自构建的点定位基准（Point Grounding Benchmark）</strong></li>
<li><strong>物体计数基准（CountBench）</strong>（Paiss et al., 2023）</li>
</ul>
<p>我们将 <strong>Qwen2.5-VL</strong> 的 <strong>视觉定位能力</strong>与其他先进的<strong>大规模视觉-语言模型（LVLMs）</strong> 进行了比较，包括：</p>
<ul>
<li><strong>Gemini</strong></li>
<li><strong>Grounding-DINO</strong>（Liu et al., 2023c）</li>
<li><strong>Molmo</strong>（Deitke et al., 2024）</li>
<li><strong>InternVL2.5</strong></li>
</ul>
<p><strong>领先的视觉定位能力：</strong></p>
<p>Qwen2.5-VL 在 <strong>框定位（box-grounding）、点定位（point-grounding）和物体计数（counting）</strong> 任务上均取得 <strong>领先成绩</strong>。</p>
<ul>
<li><strong>双重定位能力（框+点）</strong><br> Qwen2.5-VL <strong>同时具备框定位和点定位能力</strong>，可以理解、定位并推理图像中特定部分的<strong>细节信息</strong>。</li>
<li><strong>开放词汇目标检测（Open-Vocabulary Object Detection）</strong><br> 在 <strong>ODinW-13 数据集</strong> 上，Qwen2.5-VL 取得了 <strong>43.1 mAP</strong> 的优异成绩，<strong>超过了大多数 LVLMs</strong>，并<strong>迅速缩小了通用模型与专业模型之间的差距</strong>。</li>
<li><strong>点定位能力突破</strong><br> 传统的 <strong>目标检测</strong> 主要依赖 <strong>边界框（bounding box）</strong>，但有些物体的 <strong>细节</strong> 很难用 <strong>矩形框表示</strong>。Qwen2.5-VL <strong>解锁了点定位能力</strong>，可以<strong>精准地定位</strong> 物体的<strong>特定细节</strong>，这一点在过去的检测方法中是 <strong>难以实现的</strong>。</li>
<li><strong>强大的计数能力</strong><br> 在 <strong>CountBench</strong> 基准测试中，Qwen2.5-VL <strong>采用“先检测再计数”（detect then count）的提示方式</strong>，Qwen2.5-VL-72B <strong>取得了 93.6% 的领先准确率</strong>，展现了出色的 <strong>目标计数能力</strong>。</li>
</ul>
<h4 id="3-3-4-视频理解与定位"><a href="#3-3-4-视频理解与定位" class="headerlink" title="3.3.4 视频理解与定位"></a>3.3.4 视频理解与定位</h4><p>我们对Qwen2.5-VL模型在<strong>视频理解与定位</strong>任务上的表现进行了全面评估，采用的基准测试涵盖<strong>时长从数秒到数小时</strong>的视频。<strong>表 8</strong>展示了 <strong>Qwen2.5-VL 系列模型</strong>与<strong>顶级商业模型</strong>在以下<strong>视频基准测试</strong>上的性能对比：</p>
<ul>
<li><strong>Video-MME</strong>（Fu et al., 2024a）</li>
<li><strong>Video-MMMU</strong>（Hu et al., 2025）</li>
<li><strong>MMVU</strong>（Zhao et al., 2025）</li>
<li><strong>MVBench</strong>（Li et al., 2024d）</li>
<li><strong>MMBench-Video</strong>（Fang et al., 2024）</li>
<li><strong>LongVideoBench</strong>（Wu et al., 2024a）</li>
<li><strong>EgoSchema</strong>（Mangalam et al., 2023）</li>
<li><strong>PerceptionTest</strong>（Patraucean et al., 2024）</li>
<li><strong>MLVU</strong>（Zhou et al., 2024）</li>
<li><strong>LVBench</strong>（Wang et al., 2024g）</li>
<li><strong>TempCompass</strong>（Liu et al., 2024c）</li>
<li><strong>Charades-STA</strong>（Gao et al., 2017）</li>
</ul>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314144415902.png" class="" title="image-20250314144415902">

<p><strong>领先的长视频理解能力：</strong></p>
<p>在评估<strong>长视频理解</strong>的LVBench和MLVU基准测试中，Qwen2.5-VL-72B <strong>显著超越 GPT-4o</strong>，表现出<strong>卓越的长时视频推理和问答能力</strong>。</p>
<p><strong>时间敏感视频理解的突破：</strong></p>
<p>Qwen2.5-VL 采用 <strong>同步MRoPE（Synchronized MRoPE）</strong> 技术，增强了模型在<strong>时间敏感视频理解</strong>方面的能力，包括：</p>
<ul>
<li><strong>时间戳引用（timestamp referencing）</strong></li>
<li><strong>时间定位（temporal grounding）</strong></li>
<li><strong>密集字幕生成（dense captioning）</strong></li>
<li><strong>其他视频推理功能</strong></li>
</ul>
<p><strong>精准的视频事件定位能力：</strong></p>
<p>在 <strong>Charades-STA 数据集</strong>（用于评估 AI <strong>准确定位视频中的事件或活动</strong>的能力）上，Qwen2.5-VL-72B 取得了 <strong>50.9 mIoU</strong> 的优秀成绩，<strong>超越 GPT-4o</strong>。</p>
<p><strong>视频处理限制：</strong></p>
<p>在所有基准测试中，我们限制 <strong>每个视频的最大分析帧数为768</strong>，并确保<strong>视频 token总数不超过 24,576</strong>，以保证高效计算。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314144806604.png" class="" title="image-20250314144806604">

<h4 id="3-3-5-智能体（Agent）"><a href="#3-3-5-智能体（Agent）" class="headerlink" title="3.3.5 智能体（Agent）"></a>3.3.5 智能体（Agent）</h4><p>多模态模型的 <strong>智能体能力</strong>（Agent Capabilities）至关重要，它使模型能够 <strong>有效地与现实世界设备交互</strong>。我们从多个方面评估了 <strong>Qwen2.5-VL 的智能体能力</strong>。</p>
<p><strong>GUI 元素定位能力评估</strong></p>
<ul>
<li><strong>ScreenSpot（Cheng et al., 2024）</strong></li>
<li><strong>ScreenSpot Pro（Li et al., 2025a）</strong></li>
</ul>
<p><strong>离线评估（Offline Evaluation）</strong></p>
<ul>
<li><strong>Android Control（Li et al., 2024f）</strong></li>
</ul>
<p><strong>在线评估（Online Evaluation）</strong></p>
<ul>
<li><strong>AndroidWorld（Rawles et al., 2024）</strong></li>
<li><strong>MobileMiniWob++（Rawles et al., 2024）</strong></li>
<li><strong>OSWorld（Xie et al., 2025）</strong></li>
</ul>
<p><strong>模型对比</strong></p>
<p>我们将 <strong>Qwen2.5-VL-72B</strong> 与 <strong>其他顶级模型</strong> 进行了对比，包括：</p>
<ul>
<li><strong>GPT-4o（OpenAI, 2024）</strong></li>
<li><strong>Gemini 2.0（Deepmind, 2024）</strong></li>
<li><strong>Claude（Anthropic, 2024b）</strong></li>
<li><strong>Aguvis-72B（Xu et al., 2024）</strong></li>
<li><strong>Qwen2-VL-72B（Wang et al., 2024e）</strong></li>
</ul>
<p>评估结果见<strong>表 9</strong>。</p>
<p><strong>Qwen2.5-VL-72B 在 GUI 定位基准测试中的卓越表现</strong></p>
<ul>
<li>在 <strong>ScreenSpot 基准测试</strong> 中，Qwen2.5-VL-72B <strong>达到了 87.1% 的准确率</strong>，显著领先 <strong>Gemini 2.0（84.0%）和 Claude（83.0%）</strong>。</li>
<li>在 <strong>ScreenSpot Pro 基准测试</strong> 中，Qwen2.5-VL-72B <strong>达到了 43.6% 的准确率</strong>，<strong>远超</strong> Aguvis-72B（23.6%）和其前代 Qwen2-VL-72B（1.6%）。</li>
</ul>
<p>这些出色的 <strong>GUI 元素定位能力</strong> 使得 <strong>Qwen2.5-VL-72B</strong> 在 <strong>所有离线评估任务</strong> 中都 <strong>大幅领先</strong> 其他基线模型。</p>
<p><strong>在线评估中的出色表现：</strong></p>
<p>在 <strong>AndroidWorld 和 MobileMiniWob++</strong> 任务中，<strong>部分基线模型</strong> 由于 <strong>有限的 GUI 定位能力</strong>，难以完成任务。因此，我们采用 <strong>Set-of-Mark（SoM）</strong> 方法为这些模型提供辅助标记输入。</p>
<p><strong>主要发现：</strong></p>
<ul>
<li><strong>Qwen2.5-VL-72B 在 AndroidWorld 和 MobileMiniWob++ 任务中显著超越基线模型</strong>。</li>
<li><strong>在 OSWorld 任务中，即使不借助辅助标记，Qwen2.5-VL-72B 也能达到与基线模型相当的性能</strong>。</li>
</ul>
<p>这一结果表明：<br>  <strong>Qwen2.5-VL-72B 具备强大的环境适应能力</strong>，可以在<strong>真实且动态的环境</strong>中作为<strong>智能体（Agent）执行任务</strong>，无需额外辅助信息。</p>
<img src="/2025/03/11/Qwen2-5VL%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%80%BB%E7%BB%93/image-20250314155533200.png" class="" title="image-20250314155533200">

<hr>
<h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4 结论"></a>4 结论</h2><p>我们推出了 <strong>Qwen2.5-VL</strong>，这是一款 <strong>最先进的视觉-语言（Vision-Language）模型系列</strong>，在 <strong>多模态理解和交互方面</strong> 取得了 <strong>重大突破</strong>。</p>
<p>Qwen2.5-VL 具备 <strong>增强的视觉识别、目标定位、文档解析以及长视频理解能力</strong>，能够在 <strong>静态与动态任务</strong> 中均表现出色。</p>
<p><strong>核心技术创新</strong></p>
<p>✅ <strong>原生动态分辨率处理（Dynamic-Resolution Processing）</strong> 和 <strong>绝对时间编码（Absolute Time Encoding）</strong>，确保对多种输入类型的稳健处理。<br> ✅ <strong>Window Attention 机制</strong>，在 <strong>降低计算成本</strong> 的同时，仍 <strong>保持高分辨率的精准解析</strong>。</p>
<p><strong>广泛的应用场景</strong></p>
<p>Qwen2.5-VL 适用于 <strong>从边缘 AI 设备到高性能计算（HPC）等多种场景</strong>。</p>
<ul>
<li><strong>旗舰模型 Qwen2.5-VL-72B</strong>：<br> 📌 <strong>在文档和图表理解任务上匹敌甚至超越</strong> <strong>GPT-4o 和 Claude 3.5 Sonnet</strong>。<br> 📌 在 <strong>纯文本任务上仍保持强劲性能</strong>。</li>
<li><strong>小型模型 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B</strong>：<br> 📌 在相同规模的竞争对手中表现更优，兼顾 <strong>高效性与多功能性</strong>。</li>
</ul>
<p><strong>设立新标杆，推动智能交互</strong></p>
<p>Qwen2.5-VL <strong>重新定义了视觉-语言模型的行业基准</strong>，在 <strong>泛化能力</strong> 和<strong>任务执行能力</strong>上均表现卓越。其 <strong>创新突破</strong>为<strong>更智能、更具交互性的 AI 系统</strong>奠定了基础，进一步<strong>拉近AI感知能力与真实世界应用之间的距离</strong>。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="Vit"><a href="#Vit" class="headerlink" title="Vit"></a>Vit</h3><p><strong>Vision Transformer (ViT) 是具体算法还是一类算法？</strong></p>
<p>简单来说，<strong>Vision Transformer（ViT）</strong> 是 <strong>一种具体的算法</strong>，但它也代表了一类<strong>基于 Transformer 结构的视觉处理方法</strong>。</p>
<h4 id="ViT是什么？"><a href="#ViT是什么？" class="headerlink" title="ViT是什么？"></a><strong>ViT是什么？</strong></h4><p>ViT 是 <strong>一种用于处理图像的深度学习模型</strong>，它由 Google 在 2020 年提出（论文 “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words</a>“）。<br>它的核心思想是：<strong>用 Transformer 代替 CNN 进行图像处理</strong>，从而带来更强的全局信息建模能力。</p>
<blockquote>
<p><strong>类比</strong>：<br>传统的 CNN（卷积神经网络）就像<strong>看局部细节</strong>，一块一块地分析图片，而 ViT 更像<strong>从整体来看图</strong>，一次性理解整个画面。</p>
</blockquote>
<h4 id="ViT-的核心原理"><a href="#ViT-的核心原理" class="headerlink" title="ViT 的核心原理"></a><strong>ViT 的核心原理</strong></h4><p>ViT 其实借鉴了 NLP（自然语言处理）中的 Transformer 结构，但对图像数据做了一些特别的处理：</p>
<ol>
<li><strong>把图片切成小块（Patch Embedding）</strong><ul>
<li>假设输入图片是 <strong>256×256 像素</strong>，ViT 会把它<strong>切割成多个 16×16 小块</strong>，然后把这些小块转换成向量。</li>
<li><strong>类比</strong>：就像把一张照片分成多个拼图块。</li>
</ul>
</li>
<li><strong>用 Transformer 处理这些小块</strong><ul>
<li>这些小块的向量会输入 Transformer，经过<strong>自注意力（Self-Attention）</strong> 机制计算它们之间的关系。</li>
<li><strong>类比</strong>：就像在阅读一段文章时，每个词都可以和其他词有联系，ViT 也能理解每个图像块与其他块的关系。</li>
</ul>
</li>
<li><strong>最后做分类或其他任务</strong><ul>
<li>经过 Transformer 处理后，ViT 会输出一个结果，比如“这张图片里有一只猫”或者“这是某个人的照片”。</li>
</ul>
</li>
</ol>
<h4 id="ViT-和-CNN（传统卷积神经网络）的对比"><a href="#ViT-和-CNN（传统卷积神经网络）的对比" class="headerlink" title="ViT 和 CNN（传统卷积神经网络）的对比"></a><strong>ViT 和 CNN（传统卷积神经网络）的对比</strong></h4><table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>CNN（卷积神经网络）</strong></th>
<th><strong>ViT（视觉 Transformer）</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>处理方式</strong></td>
<td>逐层卷积提取局部特征</td>
<td>直接学习全局关系</td>
</tr>
<tr>
<td><strong>适合的任务</strong></td>
<td>小数据量、标准视觉任务</td>
<td>大数据量、复杂视觉任务</td>
</tr>
<tr>
<td><strong>计算量</strong></td>
<td>较小，训练快</td>
<td>计算量大，需要更强硬件</td>
</tr>
<tr>
<td><strong>依赖数据量</strong></td>
<td>小数据也能训练得不错</td>
<td>需要大量数据（如 ImageNet）</td>
</tr>
</tbody></table>
<p><strong>总结</strong>：</p>
<ul>
<li>CNN 适合<strong>小数据集</strong>，比如检测人脸、识别车牌等。</li>
<li>ViT 更适合<strong>大规模数据</strong>，比如理解复杂图片、自动驾驶等。</li>
</ul>
<h4 id="ViT-只是一种算法，还是一类算法？"><a href="#ViT-只是一种算法，还是一类算法？" class="headerlink" title="ViT 只是一种算法，还是一类算法？"></a><strong>ViT 只是一种算法，还是一类算法？</strong></h4><p>ViT 本身是<strong>一个具体的算法</strong>，但由于它的成功，现在很多变种都基于 ViT 改进，比如：</p>
<ul>
<li><strong>DeiT</strong>（Data-efficient ViT）：优化数据使用，减少训练需求</li>
<li><strong>Swin Transformer</strong>：增加层次结构，提高性能</li>
<li><strong>BEiT</strong>（BERT for images）：借鉴 NLP 预训练方法，提高效果</li>
</ul>
<p>因此，ViT <strong>既是一种具体的算法，也可以看作是 Transformer 视觉模型的代表</strong>。</p>
<h4 id="适合小白的总结"><a href="#适合小白的总结" class="headerlink" title="适合小白的总结"></a><strong>适合小白的总结</strong></h4><ul>
<li>ViT 是用 Transformer 来分析图像的模型，相比 CNN 能更好地理解全局信息。</li>
<li>但是它需要<strong>大量数据和计算资源</strong>，所以一般用于大规模视觉任务。</li>
<li>ViT 本身是<strong>一个具体的算法</strong>，但它的思想已经影响了一大类<strong>基于 Transformer 的视觉算法</strong>。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/qwen2-5/" rel="tag"># qwen2.5</a>
              <a href="/tags/qwen2-5-vl/" rel="tag"># qwen2.5 vl</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/03/07/TTS%E4%B9%8BSpark-TTS%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/" rel="prev" title="TTS之Spark-TTS介绍与使用">
                  <i class="fa fa-angle-left"></i> TTS之Spark-TTS介绍与使用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/15/TTS%E4%B9%8Bfish-speech/" rel="next" title="TTS之fish-speech">
                  TTS之fish-speech <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">366k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">22:09</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
