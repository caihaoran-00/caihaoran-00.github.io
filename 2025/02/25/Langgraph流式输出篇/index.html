<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言前面介绍了langgrph并给出了几个基础demo，鉴于langgraph的流式输出篇幅不小，为避免单个博客过于臃肿，故新开一个博客进行流式输出的介绍。 流式传输对于增强基于 LLM 构建的应用程序的响应能力至关重要。通过逐步显示输出，流式传输可显著改善用户体验 ，尤其是在LLM 的延迟方面。">
<meta property="og:type" content="article">
<meta property="og:title" content="Langgraph流式输出篇">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/02/25/Langgraph%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%E7%AF%87/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言前面介绍了langgrph并给出了几个基础demo，鉴于langgraph的流式输出篇幅不小，为避免单个博客过于臃肿，故新开一个博客进行流式输出的介绍。 流式传输对于增强基于 LLM 构建的应用程序的响应能力至关重要。通过逐步显示输出，流式传输可显著改善用户体验 ，尤其是在LLM 的延迟方面。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-02-25T07:26:09.000Z">
<meta property="article:modified_time" content="2025-02-27T08:10:26.748Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="langgraph">
<meta property="article:tag" content="langchain">
<meta property="article:tag" content="agent">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/02/25/Langgraph%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%E7%AF%87/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/02/25/Langgraph%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%E7%AF%87/","path":"2025/02/25/Langgraph流式输出篇/","title":"Langgraph流式输出篇"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Langgraph流式输出篇 | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%9F%BA%E7%A1%80%E7%A4%BA%E4%BE%8B-%E5%BA%95%E5%BA%A7%EF%BC%8C%E9%9D%9E%E6%B5%81%E5%BC%8F%E7%89%88%E6%9C%AC"><span class="nav-number">2.1.</span> <span class="nav-text">一、基础示例(底座，非流式版本)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E6%B5%81%E5%BC%8F%E4%BC%A0%E8%BE%93%E7%8A%B6%E6%80%81%E4%B8%8B%E7%9A%84%E6%89%80%E6%9C%89%E5%80%BC%EF%BC%88stream-mode-%E2%80%9Dvalues%E2%80%9D%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">二、流式传输状态下的所有值（stream_mode&#x3D;”values”）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%9D%A5%E8%87%AA%E8%8A%82%E7%82%B9%E7%9A%84%E6%B5%81%E7%8A%B6%E6%80%81%E6%9B%B4%E6%96%B0%EF%BC%88stream-mode-%E2%80%9Dupdates%E2%80%9D%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">三、来自节点的流状态更新（stream_mode&#x3D;”updates”）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%B5%81%E8%B0%83%E8%AF%95%E4%BA%8B%E4%BB%B6%EF%BC%88stream-mode-%E2%80%9Ddebug%E2%80%9D%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">四、流调试事件（stream_mode&#x3D;”debug”）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E2%9C%A8%E6%B5%81%E5%BC%8Ftokens%EF%BC%88stream-mode-%E2%80%9Dmessages%E2%80%9D%EF%BC%89"><span class="nav-number">2.5.</span> <span class="nav-text">五、✨流式tokens（stream_mode&#x3D;”messages”）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E6%B5%81%E5%BC%8F%E4%BC%A0%E8%BE%93%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%EF%BC%88stream-mode-%E2%80%9Dcustom%E2%80%9D%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">六、流式传输自定义数据（stream_mode&#x3D;”custom”）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E9%85%8D%E7%BD%AE%E5%A4%9A%E7%A7%8D%E6%B5%81%E6%A8%A1%E5%BC%8F"><span class="nav-number">4.</span> <span class="nav-text">七、配置多种流模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E2%9C%A8%E9%99%84%E5%BD%95%EF%BC%9A%E6%B5%81%E5%BC%8F%E4%BC%A0%E8%BE%93LLM-tokens%E6%89%A9%E5%B1%95"><span class="nav-number">5.</span> <span class="nav-text">✨附录：流式传输LLM tokens扩展</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">6.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/02/25/Langgraph%E6%B5%81%E5%BC%8F%E8%BE%93%E5%87%BA%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Langgraph流式输出篇 | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Langgraph流式输出篇
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-02-25 15:26:09" itemprop="dateCreated datePublished" datetime="2025-02-25T15:26:09+08:00">2025-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-27 16:10:26" itemprop="dateModified" datetime="2025-02-27T16:10:26+08:00">2025-02-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/code/" itemprop="url" rel="index"><span itemprop="name">code</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>12 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://caihaoran-00.github.io/2025/02/10/langgraph%E4%BB%8B%E7%BB%8D%E5%8F%8Ademo/#more">前面</a>介绍了langgrph并给出了几个基础demo，鉴于langgraph的流式输出篇幅不小，为避免单个博客过于臃肿，故新开一个博客进行流式输出的介绍。</p>
<p>流式传输对于增强基于 LLM 构建的应用程序的响应能力至关重要。通过逐步显示输出，流式传输可显著改善用户体验 ，尤其是在LLM 的延迟方面。</p>
<span id="more"></span>

<hr>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>LangGraph 内置了一流的流式传输支持。有几种不同的方法可以从图形运行中流式传输输出：</p>
<ul>
<li><code>&quot;values&quot;</code>：每一步之后发出状态中的所有值(每次状态更新都会输出)</li>
<li><code>&quot;updates&quot;</code>：仅发出节点名称和每个步骤后节点返回的更新。如果在同一步骤中进行多项更新（例如运行多个节点），则这些更新将分别发出（只会输出状态的增量更新，并且带上当前更新对应的节点名）</li>
<li><code>&quot;custom&quot;</code>：使用从内部节点发出自定义数据<code>StreamWriter</code></li>
<li><code>&quot;messages&quot;</code>：逐个标记地发出 LLM 消息以及节点内任何 LLM 调用的元数据</li>
<li><code>&quot;debug&quot;</code>：针对每个步骤发出包含尽可能多信息的调试事件</li>
</ul>
<h3 id="一、基础示例-底座，非流式版本"><a href="#一、基础示例-底座，非流式版本" class="headerlink" title="一、基础示例(底座，非流式版本)"></a>一、基础示例(底座，非流式版本)</h3><figure class="highlight python"><figcaption><span>howtostream_basic.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">refine_topic</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;topic&quot;</span>: state[<span class="string">&quot;topic&quot;</span>] + <span class="string">&quot; and cats&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_joke</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: <span class="string">f&quot;This is a joke about <span class="subst">&#123;state[<span class="string">&#x27;topic&#x27;</span>]&#125;</span>&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(refine_topic)</span><br><span class="line">    .add_node(generate_joke)</span><br><span class="line">    .add_edge(START, <span class="string">&quot;refine_topic&quot;</span>)</span><br><span class="line">    .add_edge(<span class="string">&quot;refine_topic&quot;</span>, <span class="string">&quot;generate_joke&quot;</span>)</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = graph.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>, <span class="string">&#x27;joke&#x27;</span>: <span class="string">&#x27;This is a joke about ice cream and cats&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li><code>result = graph.invoke(&#123;&quot;topic&quot;: &quot;ice cream&quot;&#125;)</code>，<code>graph.invoke(&#123;...&#125;)</code> 返回的是 <strong>整个 <code>state</code></strong>，所以 <code>result</code> 就是最终的 <code>state</code></li>
<li><code>state</code> <strong>是贯穿整个执行流程的数据结构</strong>，它会被 <strong>不断更新和合并</strong></li>
</ul>
<hr>
<h3 id="二、流式传输状态下的所有值（stream-mode-”values”）"><a href="#二、流式传输状态下的所有值（stream-mode-”values”）" class="headerlink" title="二、流式传输状态下的所有值（stream_mode&#x3D;”values”）"></a>二、流式传输状态下的所有值（stream_mode&#x3D;”values”）</h3><figure class="highlight python"><figcaption><span>howtostream.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;https://langchain-ai.github.io/langgraph/how-tos/streaming/&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">refine_topic</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;topic&quot;</span>: state[<span class="string">&quot;topic&quot;</span>] + <span class="string">&quot; and cats&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_joke</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: <span class="string">f&quot;This is a joke about <span class="subst">&#123;state[<span class="string">&#x27;topic&#x27;</span>]&#125;</span>&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(refine_topic)</span><br><span class="line">    .add_node(generate_joke)</span><br><span class="line">    .add_edge(START, <span class="string">&quot;refine_topic&quot;</span>)</span><br><span class="line">    .add_edge(<span class="string">&quot;refine_topic&quot;</span>, <span class="string">&quot;generate_joke&quot;</span>)</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> graph.stream(</span><br><span class="line">    &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;values&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>, <span class="string">&#x27;joke&#x27;</span>: <span class="string">&#x27;This is a joke about ice cream and cats&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>可见，指定<code>stream_mode=&quot;values&quot;</code>时，每次状态更新都会输出，顺序分别为：</p>
<ul>
<li><p><strong>初始状态</strong> <code>&#123; &quot;topic&quot;: &quot;ice cream&quot; &#125;</code></p>
</li>
<li><p><strong><code>refine_topic</code> 处理后的状态</strong> <code>&#123; &quot;topic&quot;: &quot;ice cream and cats&quot; &#125;</code></p>
</li>
<li><p><strong><code>generate_joke</code> 处理后的状态</strong> <code>&#123; &quot;topic&quot;: &quot;ice cream and cats&quot;, &quot;joke&quot;: &quot;This is a joke about ice cream and cats&quot; &#125;</code></p>
</li>
</ul>
<hr>
<h3 id="三、来自节点的流状态更新（stream-mode-”updates”）"><a href="#三、来自节点的流状态更新（stream-mode-”updates”）" class="headerlink" title="三、来自节点的流状态更新（stream_mode&#x3D;”updates”）"></a>三、来自节点的流状态更新（stream_mode&#x3D;”updates”）</h3><p>只需将<code>stream_mode=&quot;updates&quot;</code>-&gt;<code>stream_mode=&quot;values&quot;</code>，输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;refine_topic&#x27;</span>: &#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">&#x27;generate_joke&#x27;</span>: &#123;<span class="string">&#x27;joke&#x27;</span>: <span class="string">&#x27;This is a joke about ice cream and cats&#x27;</span>&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>注意：该行为<strong>不会输出初始输入的状态</strong>，因为 <code>updates</code> 只跟踪 <strong>计算过程中产生的增量更新</strong>，而初始状态不是由任何节点计算出来的。</p>
<hr>
<h3 id="四、流调试事件（stream-mode-”debug”）"><a href="#四、流调试事件（stream-mode-”debug”）" class="headerlink" title="四、流调试事件（stream_mode&#x3D;”debug”）"></a>四、流调试事件（stream_mode&#x3D;”debug”）</h3><p>只需将<code>stream_mode=&quot;debug&quot;</code>-&gt;<code>stream_mode=&quot;updates&quot;</code>，输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;task&#x27;</span>, <span class="string">&#x27;timestamp&#x27;</span>: <span class="string">&#x27;2025-02-25T09:20:45.414185+00:00&#x27;</span>, <span class="string">&#x27;step&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;payload&#x27;</span>: &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;fb7332bb-5540-51c3-4f71-147275e42ada&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;refine_topic&#x27;</span>, <span class="string">&#x27;input&#x27;</span>: &#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream&#x27;</span>&#125;, <span class="string">&#x27;triggers&#x27;</span>: [<span class="string">&#x27;start:refine_topic&#x27;</span>]&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;task_result&#x27;</span>, <span class="string">&#x27;timestamp&#x27;</span>: <span class="string">&#x27;2025-02-25T09:20:45.415185+00:00&#x27;</span>, <span class="string">&#x27;step&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;payload&#x27;</span>: &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;fb7332bb-5540-51c3-4f71-147275e42ada&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;refine_topic&#x27;</span>, <span class="string">&#x27;error&#x27;</span>: <span class="literal">None</span>, <span class="string">&#x27;result&#x27;</span>: [(<span class="string">&#x27;topic&#x27;</span>, <span class="string">&#x27;ice cream and cats&#x27;</span>)], <span class="string">&#x27;interrupts&#x27;</span>: []&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;task&#x27;</span>, <span class="string">&#x27;timestamp&#x27;</span>: <span class="string">&#x27;2025-02-25T09:20:45.415185+00:00&#x27;</span>, <span class="string">&#x27;step&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;payload&#x27;</span>: &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;5f676a5d-4bb5-2231-1a86-4f87a3943648&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;generate_joke&#x27;</span>, <span class="string">&#x27;input&#x27;</span>: &#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>&#125;, <span class="string">&#x27;triggers&#x27;</span>: [<span class="string">&#x27;refine_topic&#x27;</span>]&#125;&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;task_result&#x27;</span>, <span class="string">&#x27;timestamp&#x27;</span>: <span class="string">&#x27;2025-02-25T09:20:45.415185+00:00&#x27;</span>, <span class="string">&#x27;step&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;payload&#x27;</span>: &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;5f676a5d-4bb5-2231-1a86-4f87a3943648&#x27;</span>, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;generate_joke&#x27;</span>, <span class="string">&#x27;error&#x27;</span>: <span class="literal">None</span>, <span class="string">&#x27;result&#x27;</span>: [(<span class="string">&#x27;joke&#x27;</span>, <span class="string">&#x27;This is a joke about ice cream and cats&#x27;</span>)], <span class="string">&#x27;interrupts&#x27;</span>: []&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>使用它来流式**传输调试事件，**其中包含每个步骤的尽可能多的信息。包括有关计划执行的任务以及任务执行结果的信息。</p>
<hr>
<h3 id="五、✨流式tokens（stream-mode-”messages”）"><a href="#五、✨流式tokens（stream-mode-”messages”）" class="headerlink" title="五、✨流式tokens（stream_mode&#x3D;”messages”）"></a>五、✨流式tokens（stream_mode&#x3D;”messages”）</h3><figure class="highlight python"><figcaption><span>howtostream_tokens.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;https://langchain-ai.github.io/langgraph/how-tos/streaming/&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置大模型参数</span></span><br><span class="line">LLM_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;api_key&quot;</span>: <span class="string">&quot;sk-****************&quot;</span>,</span><br><span class="line">    <span class="string">&quot;base_url&quot;</span>: <span class="string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen2.5-7b-instruct&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化大模型</span></span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model=LLM_CONFIG[<span class="string">&quot;model&quot;</span>],</span><br><span class="line">    openai_api_key=LLM_CONFIG[<span class="string">&quot;api_key&quot;</span>],</span><br><span class="line">    openai_api_base=LLM_CONFIG[<span class="string">&quot;base_url&quot;</span>],</span><br><span class="line">    streaming=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">refine_topic</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;topic&quot;</span>: state[<span class="string">&quot;topic&quot;</span>] + <span class="string">&quot; and cats&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_joke</span>(<span class="params">state: State</span>):</span><br><span class="line">    llm_response = llm.invoke(</span><br><span class="line">        [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Generate a joke about <span class="subst">&#123;state[<span class="string">&#x27;topic&#x27;</span>]&#125;</span>&quot;</span>&#125;</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: llm_response.content&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(refine_topic)</span><br><span class="line">    .add_node(generate_joke)</span><br><span class="line">    .add_edge(START, <span class="string">&quot;refine_topic&quot;</span>)</span><br><span class="line">    .add_edge(<span class="string">&quot;refine_topic&quot;</span>, <span class="string">&quot;generate_joke&quot;</span>)</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> message_chunk, metadata <span class="keyword">in</span> graph.stream(</span><br><span class="line">    &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;messages&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    <span class="keyword">if</span> message_chunk.content:</span><br><span class="line">        <span class="built_in">print</span>(message_chunk.content, end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;metadata: <span class="subst">&#123;metadata&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Why| did| the| cat choose the ice| cream over the fish|?</span><br><span class="line"></span><br><span class="line">Because the ice| cream was purr|-fectly chilled|!|</span><br><span class="line"></span><br><span class="line">metadata: </span><br><span class="line">&#123;<span class="string">&#x27;langgraph_step&#x27;</span>: <span class="number">2</span>,</span><br><span class="line"> <span class="string">&#x27;langgraph_node&#x27;</span>: <span class="string">&#x27;generate_joke&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;langgraph_triggers&#x27;</span>: [<span class="string">&#x27;refine_topic&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;langgraph_path&#x27;</span>: (<span class="string">&#x27;__pregel_pull&#x27;</span>, <span class="string">&#x27;generate_joke&#x27;</span>), </span><br><span class="line"> <span class="string">&#x27;langgraph_checkpoint_ns&#x27;</span>: <span class="string">&#x27;generate_joke:4a4e35fb-1c05-53cf-2898-e23e9d2784ad&#x27;</span>, 	  <span class="string">&#x27;checkpoint_ns&#x27;</span>: <span class="string">&#x27;generate_joke:4a4e35fb-1c05-53cf-2898-e23e9d2784ad&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ls_provider&#x27;</span>: <span class="string">&#x27;openai&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ls_model_name&#x27;</span>: <span class="string">&#x27;qwen2.5-7b-instruct&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ls_model_type&#x27;</span>: <span class="string">&#x27;chat&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;ls_temperature&#x27;</span>: <span class="number">0.7</span>&#125;</span><br></pre></td></tr></table></figure>

<p><strong>(1) <code>langgraph_step: 2</code></strong></p>
<ul>
<li>当前执行的步骤编号。</li>
<li>这里是 <code>2</code>，表示 <code>generate_joke</code> 是 <strong>图执行的第二步</strong>（第一步是 <code>refine_topic</code>）。</li>
</ul>
<p><strong>(2) <code>langgraph_node: &#39;generate_joke&#39;</code></strong></p>
<ul>
<li>当前正在执行的 <strong>节点名称</strong>。</li>
<li>这里是 <code>&quot;generate_joke&quot;</code>，表示代码正在执行 <code>generate_joke</code> 这个节点。</li>
</ul>
<p><strong>(3) <code>langgraph_triggers: [&#39;refine_topic&#39;]</code></strong></p>
<ul>
<li><strong>触发当前节点的前序节点</strong>，即 <code>generate_joke</code> 是由哪些节点的执行结果触发的。</li>
<li>这里是 <code>[&#39;refine_topic&#39;]</code>，表示 <code>generate_joke</code> 是 <strong><code>refine_topic</code> 计算完成后触发的</strong>。</li>
</ul>
<p><strong>(4) <code>langgraph_path: (&#39;__pregel_pull&#39;, &#39;generate_joke&#39;)</code></strong></p>
<ul>
<li>记录 <strong>执行路径</strong>，表示 <code>generate_joke</code> 是如何被调用的。</li>
<li><code>__pregel_pull</code> 可能是 <code>langgraph</code> 内部用于数据拉取的机制。</li>
</ul>
<p><strong>(5) <code>langgraph_checkpoint_ns: &#39;generate_joke:4a4e35fb-1c05-53cf-2898-e23e9d2784ad&#39;</code></strong></p>
<ul>
<li><code>checkpoint_ns</code>（命名空间）用于 <strong>存储和恢复计算状态</strong>，确保在系统崩溃或中断时可以继续执行。</li>
<li><code>4a4e35fb-1c05-53cf-2898-e23e9d2784ad</code> 是一个 <strong>唯一的 UUID</strong>，用于标识这个 checkpoint。</li>
</ul>
<p><strong>(6) <code>checkpoint_ns: &#39;generate_joke:4a4e35fb-1c05-53cf-2898-e23e9d2784ad&#39;</code></strong></p>
<ul>
<li>和 <code>langgraph_checkpoint_ns</code> 一样，表示该节点的 <strong>计算状态可以被存储和恢复</strong>，用于 <code>langgraph</code> 的 checkpoint 机制。</li>
</ul>
<p><strong>(7) <code>ls_provider: &#39;openai&#39;</code></strong></p>
<ul>
<li><strong>Language Service 提供商</strong>，这里是 <code>openai</code>，表示调用的 LLM 是 OpenAI 系的模型。</li>
</ul>
<p><strong>(8) <code>ls_model_name: &#39;qwen2.5-7b-instruct&#39;</code></strong></p>
<ul>
<li><strong>使用的语言模型名称</strong>，这里是 <code>qwen2.5-7b-instruct</code>（通义千问 2.5-7B 指导模型）。</li>
<li>说明 <code>generate_joke</code> 这个节点使用了 <strong><code>qwen2.5-7b-instruct</code></strong> 来生成笑话。</li>
</ul>
<p><strong>(9) <code>ls_model_type: &#39;chat&#39;</code></strong></p>
<ul>
<li><strong>模型类型</strong>，这里是 <code>chat</code>，表示使用的是 <strong>对话模型</strong>（如 ChatGPT 这种）。</li>
</ul>
<p><strong>(10) <code>ls_temperature: 0.7</code></strong></p>
<ul>
<li>LLM 生成文本的温度参数：<ul>
<li>0.0 → <strong>最确定性</strong>（生成最可能的文本）</li>
<li>1.0 → <strong>最随机</strong>（生成更有创意的文本）</li>
<li>这里 <code>0.7</code> 代表 <strong>平衡创造性和确定性</strong>，适合生成带点随机性的笑话。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>字段</strong></th>
<th><strong>含义</strong></th>
</tr>
</thead>
<tbody><tr>
<td><code>langgraph_step</code></td>
<td>当前执行的步骤编号（第几步）</td>
</tr>
<tr>
<td><code>langgraph_node</code></td>
<td>当前执行的节点名称</td>
</tr>
<tr>
<td><code>langgraph_triggers</code></td>
<td>触发此节点的前序节点</td>
</tr>
<tr>
<td><code>langgraph_path</code></td>
<td>计算的执行路径</td>
</tr>
<tr>
<td><code>langgraph_checkpoint_ns</code></td>
<td>计算的 Checkpoint 存储标识</td>
</tr>
<tr>
<td><code>checkpoint_ns</code></td>
<td>与 <code>langgraph_checkpoint_ns</code> 类似，确保状态可恢复</td>
</tr>
<tr>
<td><code>ls_provider</code></td>
<td>语言模型提供商（如 OpenAI）</td>
</tr>
<tr>
<td><code>ls_model_name</code></td>
<td>具体使用的 LLM 名称（如 <code>qwen2.5-7b-instruct</code>）</td>
</tr>
<tr>
<td><code>ls_model_type</code></td>
<td>LLM 的类别（如 <code>chat</code>）</td>
</tr>
<tr>
<td><code>ls_temperature</code></td>
<td>LLM 生成内容的随机性（0.0&#x3D;确定性，1.0&#x3D;高随机性）</td>
</tr>
</tbody></table>
<p>这个 <code>metadata</code> 主要是 <strong>用于跟踪 <code>langgraph</code> 的执行过程和 LLM 推理信息</strong>，在 <strong>调试、日志记录、故障恢复</strong> 时非常有用。</p>
<hr>
<h2 id="六、流式传输自定义数据（stream-mode-”custom”）"><a href="#六、流式传输自定义数据（stream-mode-”custom”）" class="headerlink" title="六、流式传输自定义数据（stream_mode&#x3D;”custom”）"></a>六、流式传输自定义数据（stream_mode&#x3D;”custom”）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"><span class="keyword">from</span> langgraph.types <span class="keyword">import</span> StreamWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">refine_topic</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;topic&quot;</span>: state[<span class="string">&quot;topic&quot;</span>] + <span class="string">&quot; and cats&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_joke</span>(<span class="params">state: State, writer: StreamWriter</span>):</span><br><span class="line">    writer(&#123;<span class="string">&quot;custom_key&quot;</span>: <span class="string">&quot;Writing custom data while generating a joke&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: <span class="string">f&quot;This is a joke about <span class="subst">&#123;state[<span class="string">&#x27;topic&#x27;</span>]&#125;</span>&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(refine_topic)</span><br><span class="line">    .add_node(generate_joke)</span><br><span class="line">    .add_edge(START, <span class="string">&quot;refine_topic&quot;</span>)</span><br><span class="line">    .add_edge(<span class="string">&quot;refine_topic&quot;</span>, <span class="string">&quot;generate_joke&quot;</span>)</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> graph.stream(</span><br><span class="line">    &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;custom&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;custom_key&#x27;</span>: <span class="string">&#x27;Writing custom data while generating a joke&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>满脸问号？这是什么？有什么用？🤓（过几天再补）</p>
<hr>
<h2 id="七、配置多种流模式"><a href="#七、配置多种流模式" class="headerlink" title="七、配置多种流模式"></a>七、配置多种流模式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"><span class="keyword">from</span> langgraph.types <span class="keyword">import</span> StreamWriter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">refine_topic</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;topic&quot;</span>: state[<span class="string">&quot;topic&quot;</span>] + <span class="string">&quot; and cats&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_joke</span>(<span class="params">state: State, writer: StreamWriter</span>):</span><br><span class="line">    writer(&#123;<span class="string">&quot;custom_key&quot;</span>: <span class="string">&quot;Writing custom data while generating a joke&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: <span class="string">f&quot;This is a joke about <span class="subst">&#123;state[<span class="string">&#x27;topic&#x27;</span>]&#125;</span>&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(refine_topic)</span><br><span class="line">    .add_node(generate_joke)</span><br><span class="line">    .add_edge(START, <span class="string">&quot;refine_topic&quot;</span>)</span><br><span class="line">    .add_edge(<span class="string">&quot;refine_topic&quot;</span>, <span class="string">&quot;generate_joke&quot;</span>)</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> stream_mode, chunk <span class="keyword">in</span> graph.stream(</span><br><span class="line">    &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;ice cream&quot;</span>&#125;,</span><br><span class="line">    stream_mode=[<span class="string">&quot;updates&quot;</span>, <span class="string">&quot;custom&quot;</span>],</span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Stream mode: <span class="subst">&#123;stream_mode&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(chunk,end=<span class="string">&quot;|&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Stream mode: updates</span><br><span class="line">&#123;<span class="string">&#x27;refine_topic&#x27;</span>: &#123;<span class="string">&#x27;topic&#x27;</span>: <span class="string">&#x27;ice cream and cats&#x27;</span>&#125;&#125;|</span><br><span class="line"></span><br><span class="line">Stream mode: custom</span><br><span class="line">&#123;<span class="string">&#x27;custom_key&#x27;</span>: <span class="string">&#x27;Writing custom data while generating a joke&#x27;</span>&#125;|</span><br><span class="line"></span><br><span class="line">Stream mode: updates</span><br><span class="line">&#123;<span class="string">&#x27;generate_joke&#x27;</span>: &#123;<span class="string">&#x27;joke&#x27;</span>: <span class="string">&#x27;This is a joke about ice cream and cats&#x27;</span>&#125;&#125;|</span><br></pre></td></tr></table></figure>

<p>不明所以，到底有什么用啊？</p>
<hr>
<h2 id="✨附录：流式传输LLM-tokens扩展"><a href="#✨附录：流式传输LLM-tokens扩展" class="headerlink" title="✨附录：流式传输LLM tokens扩展"></a>✨附录：流式传输LLM tokens扩展</h2><p>正式给示例前，先给出一套流式传输LLM tokens的模板：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: State</span>):</span><br><span class="line">    model.invoke(...)</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">graph = (</span><br><span class="line">    StateGraph(State)</span><br><span class="line">    .add_node(call_model)</span><br><span class="line">    ...</span><br><span class="line">    .<span class="built_in">compile</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> msg, metadata <span class="keyword">in</span> graph.stream(inputs, stream_mode=<span class="string">&quot;messages&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(msg)</span><br></pre></td></tr></table></figure>

<p>流式输出的元组为（message chunk, metadata）:</p>
<ul>
<li>message chunk是LLM流式传输的tokens</li>
<li>metadata是一个字典，其中包含有关调用LLM的节点以及LLM调用元数据的信息</li>
</ul>
<p><strong>好的，让我们开始吧：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#example&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> START, StateGraph, MessagesState</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置大模型参数</span></span><br><span class="line">LLM_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;api_key&quot;</span>: <span class="string">&quot;sk-xxxxxxxxx&quot;</span>,</span><br><span class="line">    <span class="string">&quot;base_url&quot;</span>: <span class="string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen2.5-7b-instruct&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化大模型</span></span><br><span class="line">joke_model = ChatOpenAI(</span><br><span class="line">    model=LLM_CONFIG[<span class="string">&quot;model&quot;</span>],</span><br><span class="line">    openai_api_key=LLM_CONFIG[<span class="string">&quot;api_key&quot;</span>],</span><br><span class="line">    openai_api_base=LLM_CONFIG[<span class="string">&quot;base_url&quot;</span>],</span><br><span class="line">    streaming=<span class="literal">True</span>,</span><br><span class="line">    tags=[<span class="string">&quot;joke&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">poem_model = ChatOpenAI(</span><br><span class="line">    model=LLM_CONFIG[<span class="string">&quot;model&quot;</span>],</span><br><span class="line">    openai_api_key=LLM_CONFIG[<span class="string">&quot;api_key&quot;</span>],</span><br><span class="line">    openai_api_base=LLM_CONFIG[<span class="string">&quot;base_url&quot;</span>],</span><br><span class="line">    streaming=<span class="literal">True</span>,</span><br><span class="line">    tags=[<span class="string">&quot;poem&quot;</span>]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line">    poem: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state, config</span>):</span><br><span class="line">    topic = state[<span class="string">&quot;topic&quot;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Writing joke...&quot;</span>)</span><br><span class="line">    <span class="comment"># Note: Passing the config through explicitly is required for python &lt; 3.11</span></span><br><span class="line">    <span class="comment"># Since context var support wasn&#x27;t added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks</span></span><br><span class="line">    joke_response = <span class="keyword">await</span> joke_model.ainvoke(</span><br><span class="line">        [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Write a joke about <span class="subst">&#123;topic&#125;</span>&quot;</span>&#125;],</span><br><span class="line">        config,</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n\nWriting poem...&quot;</span>)</span><br><span class="line">    poem_response = <span class="keyword">await</span> poem_model.ainvoke(</span><br><span class="line">        [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Write a short poem about <span class="subst">&#123;topic&#125;</span>&quot;</span>&#125;],</span><br><span class="line">        config,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: joke_response.content, <span class="string">&quot;poem&quot;</span>: poem_response.content&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = StateGraph(State).add_node(call_model).add_edge(START, <span class="string">&quot;call_model&quot;</span>).<span class="built_in">compile</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg, metadata <span class="keyword">in</span> graph.astream(</span><br><span class="line">            &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;cats&quot;</span>&#125;,</span><br><span class="line">            stream_mode=<span class="string">&quot;messages&quot;</span>,</span><br><span class="line">    ):</span><br><span class="line">        <span class="keyword">if</span> msg.content:</span><br><span class="line">            <span class="built_in">print</span>(msg.content, end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Writing joke...</span><br><span class="line">Why| don|&#x27;t| cats play poker in| the jungle? Because| there are too many| cheetahs|!|</span><br><span class="line"></span><br><span class="line">Writing poem...</span><br><span class="line">Wh|isk|ered| wonders, soft and| fine,</span><br><span class="line">In shadows| they take their time|.</span><br><span class="line">With silent p|aws and gentle pur|rs,</span><br><span class="line">They rule| the night with quiet| powers.</span><br><span class="line"></span><br><span class="line">Eyes| like embers in| the dark,</span><br><span class="line">Where|ver they go,| light does mark.</span><br><span class="line">|A flick of tail|, a gentle nap|,</span><br><span class="line">In dreams they| chase the little map|.</span><br><span class="line"></span><br><span class="line">Mice and| feathers, toys and| play,</span><br><span class="line">Yet when| the day begins to| sway,</span><br><span class="line">They find| a cozy spot to| rest,</span><br><span class="line">Guarding| dreams with silent jest|.</span><br><span class="line"></span><br><span class="line">So let them| roam, let them| explore,</span><br><span class="line">In every| home, they bring| much more.</span><br><span class="line">For| in their grace,| we find our peace|,</span><br><span class="line">And in their| eyes, a world| of ceaseless peace|.|</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong></p>
<ul>
<li><code>ainvoke(...)</code> 由于 <code>streaming=True</code>，不会等到<strong>整个</strong>笑话生成完才返回，而是<strong>逐步返回 token</strong></li>
<li><code>graph.astream()</code> 负责<strong>逐步接收</strong> <code>ainvoke(...)</code> 返回的数据</li>
<li>每当 <code>ainvoke(...)</code> 生成新 token，<code>astream()</code> 就会<strong>读取并输出</strong>它</li>
<li>流式输出并不依赖 <code>return</code> 语句</li>
<li><code>return</code> 只是在 <code>call_model()</code> 结束时返回最终完整的 <code>State</code>，但 <code>astream()</code> 在此之前已经接收了 <code>ainvoke(...)</code> 的流式输出</li>
</ul>
<p><strong>执行顺序总结：</strong></p>
<ol>
<li><p><code>&quot;Writing joke...&quot;</code> 立即打印。</p>
</li>
<li><p><code>joke_model.ainvoke(...)</code>流式返回，<code>graph.astream()逐步打印：</code></p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Why| <span class="type">did</span>| <span class="type">the</span>| <span class="type">cat</span> join the book| <span class="type">club</span>?</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>joke_model.ainvoke(...)</code> 结束，<code>print(&quot;\n\nWriting poem...&quot;)</code> 执行。</p>
</li>
<li><p><code>poem_model.ainvoke(...)</code>流式返回，<code>graph.astream()</code>逐步打印：</p>
<figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wh|<span class="type">isk</span>|<span class="type">ered</span>| <span class="type">dancers</span>, silent and| <span class="type">sleek</span>,</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>poem_model.ainvoke(...)</code> 结束，<code>return &#123;&quot;joke&quot;: ..., &quot;poem&quot;: ...&#125;</code> 执行，完整 <code>State</code> 结果返回。</p>
</li>
</ol>
<p>好的，我们可以看到，现在是从所有LLM调用中流式传输tokens，现在如果我们<strong>只想流式传输特定LLM调用的tokens</strong>，该怎么做呢？—&gt;使用流式传输的元数据（metadata）以及之前添加在LLM中的标签（tags）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">for</span> msg, metadata <span class="keyword">in</span> graph.astream(</span><br><span class="line">    &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;cats&quot;</span>&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;messages&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    <span class="keyword">if</span> msg.content <span class="keyword">and</span> <span class="string">&quot;joke&quot;</span> <span class="keyword">in</span> metadata.get(<span class="string">&quot;tags&quot;</span>, []):</span><br><span class="line">        <span class="built_in">print</span>(msg.content, end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Writing joke...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Writing poem...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>🤺说好的过滤掉指定LLM调用呢？这都过滤掉了？打印metadata发现其中并不包含tags标签，emmmm好吧留个坑，用到时候再深究吧。</p>
<p><strong>那如果不使用langchain&#x2F;Langgraph实现相同的功能，怎么做？</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langgraph.constants <span class="keyword">import</span> START</span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AsyncOpenAI</span><br><span class="line"></span><br><span class="line">LLM_CONFIG = &#123;</span><br><span class="line">    <span class="string">&quot;api_key&quot;</span>: <span class="string">&quot;sk-******************&quot;</span>,</span><br><span class="line">    <span class="string">&quot;base_url&quot;</span>: <span class="string">&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: <span class="string">&quot;qwen2.5-7b-instruct&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">openai_client = AsyncOpenAI(</span><br><span class="line">    api_key=LLM_CONFIG[<span class="string">&quot;api_key&quot;</span>],</span><br><span class="line">    base_url=LLM_CONFIG[<span class="string">&quot;base_url&quot;</span>],</span><br><span class="line">)</span><br><span class="line">model_name = LLM_CONFIG[<span class="string">&quot;model&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    topic: <span class="built_in">str</span></span><br><span class="line">    joke: <span class="built_in">str</span></span><br><span class="line">    poem: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">stream_tokens</span>(<span class="params">model_name: <span class="built_in">str</span>, messages: <span class="built_in">list</span>[<span class="built_in">dict</span>]</span>):</span><br><span class="line">    response = <span class="keyword">await</span> openai_client.chat.completions.create(</span><br><span class="line">        messages=messages, model=model_name, stream=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    role = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">        delta = chunk.choices[<span class="number">0</span>].delta</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> delta.role <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            role = delta.role</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> delta.content:</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">&quot;role&quot;</span>: role, <span class="string">&quot;content&quot;</span>: delta.content&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state, config, writer</span>):</span><br><span class="line">    topic = state[<span class="string">&quot;topic&quot;</span>]</span><br><span class="line">    joke = <span class="string">&quot;&quot;</span></span><br><span class="line">    poem = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Writing joke...&quot;</span>)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg_chunk <span class="keyword">in</span> stream_tokens(</span><br><span class="line">        model_name, [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Write a joke about <span class="subst">&#123;topic&#125;</span>&quot;</span>&#125;]</span><br><span class="line">    ):</span><br><span class="line">        joke += msg_chunk[<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        metadata = &#123;**config[<span class="string">&quot;metadata&quot;</span>], <span class="string">&quot;tags&quot;</span>: [<span class="string">&quot;joke&quot;</span>]&#125;</span><br><span class="line">        chunk_to_stream = (msg_chunk, metadata)</span><br><span class="line">        writer(chunk_to_stream)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n\nWriting poem...&quot;</span>)</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg_chunk <span class="keyword">in</span> stream_tokens(</span><br><span class="line">        model_name, [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">f&quot;Write a short poem about <span class="subst">&#123;topic&#125;</span>&quot;</span>&#125;]</span><br><span class="line">    ):</span><br><span class="line">        poem += msg_chunk[<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        metadata = &#123;**config[<span class="string">&quot;metadata&quot;</span>], <span class="string">&quot;tags&quot;</span>: [<span class="string">&quot;poem&quot;</span>]&#125;</span><br><span class="line">        chunk_to_stream = (msg_chunk, metadata)</span><br><span class="line">        writer(chunk_to_stream)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;joke&quot;</span>: joke, <span class="string">&quot;poem&quot;</span>: poem&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph = StateGraph(State).add_node(call_model).add_edge(START, <span class="string">&quot;call_model&quot;</span>).<span class="built_in">compile</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg, metadata <span class="keyword">in</span> graph.astream(</span><br><span class="line">            &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;cats&quot;</span>&#125;,</span><br><span class="line">            stream_mode=<span class="string">&quot;custom&quot;</span>,</span><br><span class="line">    ):</span><br><span class="line">        <span class="built_in">print</span>(msg[<span class="string">&quot;content&quot;</span>], end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;metadata=<span class="subst">&#123;metadata&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    asyncio.run(main())</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Writing joke...</span><br><span class="line">Why| don|<span class="string">&#x27;t| cats play poker in| the jungle? Too| many cheetah|s!|</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Writing poem...</span></span><br><span class="line"><span class="string">Wh|isk|ered| nobles of the| night,</span></span><br><span class="line"><span class="string">Silent| paws on velvet| light.</span></span><br><span class="line"><span class="string">Eyes| that gleam like| stars above,</span></span><br><span class="line"><span class="string">M|ajestic, in| the moon&#x27;</span>s soft| glove.</span><br><span class="line"></span><br><span class="line">Soft fur| whispers <span class="keyword">with</span> each step|,</span><br><span class="line">A mystery wrapped| <span class="keyword">in</span> a napkin|-wrapped keep.</span><br><span class="line">|In shadows, they| weave <span class="keyword">and</span> play,</span><br><span class="line">|Guardians of dreams|, come what may|.</span><br><span class="line"></span><br><span class="line">With a flick| of tail so grand|,</span><br><span class="line">They rule their| realm, without a| band.</span><br><span class="line">Cats|, <span class="keyword">in</span> their own| regal way,</span><br><span class="line">|Are the lords of| every fray.|</span><br><span class="line"></span><br><span class="line">metadata=&#123;<span class="string">&#x27;langgraph_step&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&#x27;langgraph_node&#x27;</span>: <span class="string">&#x27;call_model&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;langgraph_triggers&#x27;</span>: [<span class="string">&#x27;start:call_model&#x27;</span>],</span><br><span class="line">          <span class="string">&#x27;langgraph_path&#x27;</span>: (<span class="string">&#x27;__pregel_pull&#x27;</span>, <span class="string">&#x27;call_model&#x27;</span>),</span><br><span class="line">          <span class="string">&#x27;langgraph_checkpoint_ns&#x27;</span>: <span class="string">&#x27;call_model:317202cd-c031-a780-9998-423c3ae28e18&#x27;</span>, </span><br><span class="line">          <span class="string">&#x27;tags&#x27;</span>: [<span class="string">&#x27;poem&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>嘿，这里有tags，那么如果想过滤特定的LLM调用，怎么做呢：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">for</span> msg, metadata <span class="keyword">in</span> graph.astream(</span><br><span class="line">            &#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;cats&quot;</span>&#125;,</span><br><span class="line">            stream_mode=<span class="string">&quot;custom&quot;</span>,</span><br><span class="line">    ):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;poem&quot;</span> <span class="keyword">in</span> metadata.get(<span class="string">&quot;tags&quot;</span>, []):</span><br><span class="line">            <span class="built_in">print</span>(msg[<span class="string">&quot;content&quot;</span>], end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Writing joke...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Writing poem...</span><br><span class="line">Wh|isk|ered| whispers <span class="keyword">in</span> the night|,</span><br><span class="line">Silent shadows|, soft <span class="keyword">and</span> light|.</span><br><span class="line">Paws pad| on the carpeted| floor,</span><br><span class="line">Myst|ical eyes, a| gentle roar.</span><br><span class="line"></span><br><span class="line">F|ur <span class="keyword">as</span> night,| <span class="keyword">with</span> stars aglow|,</span><br><span class="line">Tail flicks|, a secret they| bestow.</span><br><span class="line">In| their gaze, mysteries| lie,</span><br><span class="line">Cats|, the poets of| the sky.</span><br><span class="line"></span><br><span class="line">With| a purr so| deep <span class="keyword">and</span> true,</span><br><span class="line">|They mend the soul|, make it new|.</span><br><span class="line">Guardians of| the quiet peace,</span><br><span class="line">|Cats, <span class="keyword">in</span>| their own graceful cease|.</span><br></pre></td></tr></table></figure>

<p>对味了😁。</p>
<hr>
<p>这里给的简单的流式输出示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line">openai.api_key = <span class="string">&quot;YOUR_API_KEY&quot;</span>  <span class="comment"># 替换为你的API密钥</span></span><br><span class="line"></span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">    model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>,</span><br><span class="line">    messages=[&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;请用中文写一个简短的励志句子&quot;</span>&#125;],</span><br><span class="line">    stream=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">    content = chunk.choices[<span class="number">0</span>].delta.get(<span class="string">&quot;content&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(content, end=<span class="string">&#x27;&#x27;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/streaming/">https://langchain-ai.github.io/langgraph/how-tos/streaming/</a></li>
<li><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#example">https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#example</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"># LLM</a>
              <a href="/tags/langgraph/" rel="tag"># langgraph</a>
              <a href="/tags/langchain/" rel="tag"># langchain</a>
              <a href="/tags/agent/" rel="tag"># agent</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/02/21/cam-senseVoice%E6%9E%84%E5%BB%BA%E5%BE%AE%E6%9C%8D%E5%8A%A1/" rel="prev" title="cam+senseVoice构建微服务">
                  <i class="fa fa-angle-left"></i> cam+senseVoice构建微服务
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/03/04/ASR-LLM-TTS%E6%9E%84%E5%BB%BA%E6%99%BA%E8%83%BD%E8%AF%AD%E9%9F%B3%E5%AF%B9%E8%AF%9D%E6%9C%BA%E5%99%A8%E4%BA%BA/" rel="next" title="ASR+LLM+TTS构建智能语音对话机器人">
                  ASR+LLM+TTS构建智能语音对话机器人 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">131k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:55</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
