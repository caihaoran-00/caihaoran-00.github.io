<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言前面如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）浅浅的使用Unsloth微调了DeepSeek-R1-Distill-Llama-8B模型，在了解Unsloth框架时，发现还有示例使用GRPO将普通大语言模型转化为推理大语言模型，有趣，咱们一起看一下吧。">
<meta property="og:type" content="article">
<meta property="og:title" content="Unsloth:Phi-4+GRPO将通用模型微调成推理模型">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言前面如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）浅浅的使用Unsloth微调了DeepSeek-R1-Distill-Llama-8B模型，在了解Unsloth框架时，发现还有示例使用GRPO将普通大语言模型转化为推理大语言模型，有趣，咱们一起看一下吧。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-08T09:13:40.000Z">
<meta property="article:modified_time" content="2025-04-28T07:07:11.002Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="unsloth">
<meta property="article:tag" content="grpo">
<meta property="article:tag" content="phi-4">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/","path":"2025/04/08/Unsloth-Phi-4-GRPO将通用模型微调成推理模型/","title":"Unsloth:Phi-4+GRPO将通用模型微调成推理模型"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Unsloth:Phi-4+GRPO将通用模型微调成推理模型 | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95"><span class="nav-number">3.</span> <span class="nav-text">问题记录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">4.</span> <span class="nav-text">参考链接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">5.</span> <span class="nav-text">附录</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">98</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">113</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Unsloth:Phi-4+GRPO将通用模型微调成推理模型 | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Unsloth:Phi-4+GRPO将通用模型微调成推理模型
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-04-08 17:13:40" itemprop="dateCreated datePublished" datetime="2025-04-08T17:13:40+08:00">2025-04-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-28 15:07:11" itemprop="dateModified" datetime="2025-04-28T15:07:11+08:00">2025-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/code/" itemprop="url" rel="index"><span itemprop="name">code</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>7.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面<a href="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/#%E5%89%8D%E8%A8%80">如何将你的DeepSeek-R1微调成某个领域的专家（实战篇）</a>浅浅的使用<code>Unsloth</code>微调了<code>DeepSeek-R1-Distill-Llama-8B</code>模型，在了解<code>Unsloth</code>框架时，发现还有示例使用<code>GRPO</code>将普通大语言模型转化为推理大语言模型，有趣，咱们一起看一下吧。</p>
<span id="more"></span>

<hr>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>首先，先克隆模型到本地：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://huggingface.co/unsloth/phi-4-unsloth-bnb-4bit</span><br></pre></td></tr></table></figure>

<p>耐心等待，然后打开你的<code>jupyter notebook</code>(关于环境配置和部分代码解释请看<a href="https://caihaoran-00.github.io/2025/03/25/%E5%A6%82%E4%BD%95%E5%B0%86%E4%BD%A0%E7%9A%84DeepSeek-R1%E5%BE%AE%E8%B0%83%E6%88%90%E6%9F%90%E4%B8%AA%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%93%E5%AE%B6%EF%BC%88%E5%AE%9E%E6%88%98%E7%AF%87%EF%BC%89/#%E5%89%8D%E8%A8%80">这里</a>)：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>

<p><strong>第一块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel, is_bfloat16_supported</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">max_seq_length = <span class="number">512</span> <span class="comment"># Can increase for longer reasoning traces</span></span><br><span class="line">lora_rank = <span class="number">16</span> <span class="comment"># Larger rank = smarter, but slower</span></span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;../phi-4-unsloth-bnb-4bit&quot;</span>,	 <span class="comment"># 根据自己的路径来</span></span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    load_in_4bit = <span class="literal">True</span>, <span class="comment"># False for LoRA 16bit</span></span><br><span class="line">    fast_inference = <span class="literal">True</span>, <span class="comment"># Enable vLLM fast inference</span></span><br><span class="line">    max_lora_rank = lora_rank,</span><br><span class="line">    gpu_memory_utilization = <span class="number">0.7</span>, <span class="comment"># Reduce if out of memory</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = lora_rank, <span class="comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span></span><br><span class="line">    target_modules = [<span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>,],</span><br><span class="line">    lora_alpha = lora_rank,</span><br><span class="line">    use_gradient_checkpointing = <span class="string">&quot;unsloth&quot;</span>, <span class="comment"># Enable long context finetuning</span></span><br><span class="line">    random_state = <span class="number">3407</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这段代码利用 <code>unsloth</code> 库，高效地加载了一个预先被 4 位量化的 <code>Phi-4</code> 大语言模型，并为其配置了 <code>LoRA</code> 适配器。通过指定 <code>LoRA</code> 的秩、目标模块和 <code>alpha</code> 值，并启用<code>梯度检查点</code>，代码为后续使用较少计算资源进行模型微调做好了准备。最终得到的 model 对象是一个<code>PEFT 模型</code>，可以直接用于基于 <code>LoRA</code> 的训练流程。</p>
<p><strong>第二块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load and prep dataset</span></span><br><span class="line">SYSTEM_PROMPT = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Respond in the following format:</span></span><br><span class="line"><span class="string">&lt;reasoning&gt;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">&lt;/reasoning&gt;</span></span><br><span class="line"><span class="string">&lt;answer&gt;</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">&lt;/answer&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">XML_COT_FORMAT = <span class="string">&quot;&quot;&quot;\</span></span><br><span class="line"><span class="string">&lt;reasoning&gt;</span></span><br><span class="line"><span class="string">&#123;reasoning&#125;</span></span><br><span class="line"><span class="string">&lt;/reasoning&gt;</span></span><br><span class="line"><span class="string">&lt;answer&gt;</span></span><br><span class="line"><span class="string">&#123;answer&#125;</span></span><br><span class="line"><span class="string">&lt;/answer&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_xml_answer</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    answer = text.split(<span class="string">&quot;&lt;answer&gt;&quot;</span>)[-<span class="number">1</span>]</span><br><span class="line">    answer = answer.split(<span class="string">&quot;&lt;/answer&gt;&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> answer.strip()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_hash_answer</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span> | <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;####&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> text:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> text.split(<span class="string">&quot;####&quot;</span>)[<span class="number">1</span>].strip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># uncomment middle messages for 1-shot prompting</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_gsm8k_questions</span>(<span class="params">split = <span class="string">&quot;train&quot;</span></span>) -&gt; Dataset:</span><br><span class="line">    data = load_dataset(<span class="string">&#x27;openai/gsm8k&#x27;</span>, <span class="string">&#x27;main&#x27;</span>)[split] <span class="comment"># type: ignore</span></span><br><span class="line">    data = data.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123; <span class="comment"># type: ignore</span></span><br><span class="line">        <span class="string">&#x27;prompt&#x27;</span>: [</span><br><span class="line">            &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;system&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: SYSTEM_PROMPT&#125;,</span><br><span class="line">            &#123;<span class="string">&#x27;role&#x27;</span>: <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;content&#x27;</span>: x[<span class="string">&#x27;question&#x27;</span>]&#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">&#x27;answer&#x27;</span>: extract_hash_answer(x[<span class="string">&#x27;answer&#x27;</span>])</span><br><span class="line">    &#125;) <span class="comment"># type: ignore</span></span><br><span class="line">    <span class="keyword">return</span> data <span class="comment"># type: ignore</span></span><br><span class="line"></span><br><span class="line">dataset = get_gsm8k_questions()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reward functions</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">correctness_reward_func</span>(<span class="params">prompts, completions, answer, **kwargs</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&#x27;content&#x27;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    q = prompts[<span class="number">0</span>][-<span class="number">1</span>][<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">    extracted_responses = [extract_xml_answer(r) <span class="keyword">for</span> r <span class="keyword">in</span> responses]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>*<span class="number">20</span>, <span class="string">f&quot;Question:\n<span class="subst">&#123;q&#125;</span>&quot;</span>, <span class="string">f&quot;\nAnswer:\n<span class="subst">&#123;answer[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nResponse:\n<span class="subst">&#123;responses[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nExtracted:\n<span class="subst">&#123;extracted_responses[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">2.0</span> <span class="keyword">if</span> r == a <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> r, a <span class="keyword">in</span> <span class="built_in">zip</span>(extracted_responses, answer)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">int_reward_func</span>(<span class="params">completions, **kwargs</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&#x27;content&#x27;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    extracted_responses = [extract_xml_answer(r) <span class="keyword">for</span> r <span class="keyword">in</span> responses]</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">0.5</span> <span class="keyword">if</span> r.isdigit() <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> r <span class="keyword">in</span> extracted_responses]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">strict_format_reward_func</span>(<span class="params">completions, **kwargs</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Reward function that checks if the completion has a specific format.&quot;&quot;&quot;</span></span><br><span class="line">    pattern = <span class="string">r&quot;^&lt;reasoning&gt;\n.*?\n&lt;/reasoning&gt;\n&lt;answer&gt;\n.*?\n&lt;/answer&gt;\n$&quot;</span></span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    matches = [re.<span class="keyword">match</span>(pattern, r) <span class="keyword">for</span> r <span class="keyword">in</span> responses]</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">0.5</span> <span class="keyword">if</span> <span class="keyword">match</span> <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">soft_format_reward_func</span>(<span class="params">completions, **kwargs</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Reward function that checks if the completion has a specific format.&quot;&quot;&quot;</span></span><br><span class="line">    pattern = <span class="string">r&quot;&lt;reasoning&gt;.*?&lt;/reasoning&gt;\s*&lt;answer&gt;.*?&lt;/answer&gt;&quot;</span></span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    matches = [re.<span class="keyword">match</span>(pattern, r) <span class="keyword">for</span> r <span class="keyword">in</span> responses]</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">0.5</span> <span class="keyword">if</span> <span class="keyword">match</span> <span class="keyword">else</span> <span class="number">0.0</span> <span class="keyword">for</span> <span class="keyword">match</span> <span class="keyword">in</span> matches]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_xml</span>(<span class="params">text</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    count = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">if</span> text.count(<span class="string">&quot;&lt;reasoning&gt;\n&quot;</span>) == <span class="number">1</span>:</span><br><span class="line">        count += <span class="number">0.125</span></span><br><span class="line">    <span class="keyword">if</span> text.count(<span class="string">&quot;\n&lt;/reasoning&gt;\n&quot;</span>) == <span class="number">1</span>:</span><br><span class="line">        count += <span class="number">0.125</span></span><br><span class="line">    <span class="keyword">if</span> text.count(<span class="string">&quot;\n&lt;answer&gt;\n&quot;</span>) == <span class="number">1</span>:</span><br><span class="line">        count += <span class="number">0.125</span></span><br><span class="line">        count -= <span class="built_in">len</span>(text.split(<span class="string">&quot;\n&lt;/answer&gt;\n&quot;</span>)[-<span class="number">1</span>])*<span class="number">0.001</span></span><br><span class="line">    <span class="keyword">if</span> text.count(<span class="string">&quot;\n&lt;/answer&gt;&quot;</span>) == <span class="number">1</span>:</span><br><span class="line">        count += <span class="number">0.125</span></span><br><span class="line">        count -= (<span class="built_in">len</span>(text.split(<span class="string">&quot;\n&lt;/answer&gt;&quot;</span>)[-<span class="number">1</span>]) - <span class="number">1</span>)*<span class="number">0.001</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xmlcount_reward_func</span>(<span class="params">completions, **kwargs</span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    contents = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    <span class="keyword">return</span> [count_xml(c) <span class="keyword">for</span> c <span class="keyword">in</span> contents]</span><br></pre></td></tr></table></figure>

<p>这段代码为基于 <code>GSM8k</code> 数据集<code>微调 LLM</code> 做准备，特别是针对需要模型生成带有推理过程（Chain-of-Thought）和答案的特定 XML 格式输出的任务。它加载数据、将其转换为适合模型的对话格式，并定义了多种奖励函数。根据答案正确性、数字格式、XML 结构符合度（严格或宽松）以及结构完整性等多个维度来评估和指导模型的学习。</p>
<p><strong>第三块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> GRPOConfig, GRPOTrainer</span><br><span class="line">training_args = GRPOConfig(</span><br><span class="line">    use_vllm = <span class="literal">True</span>, <span class="comment"># use vLLM for fast inference!</span></span><br><span class="line">    learning_rate = <span class="number">5e-6</span>,</span><br><span class="line">    adam_beta1 = <span class="number">0.9</span>,</span><br><span class="line">    adam_beta2 = <span class="number">0.99</span>,</span><br><span class="line">    weight_decay = <span class="number">0.1</span>,</span><br><span class="line">    warmup_ratio = <span class="number">0.1</span>,</span><br><span class="line">    lr_scheduler_type = <span class="string">&quot;cosine&quot;</span>,</span><br><span class="line">    optim = <span class="string">&quot;paged_adamw_8bit&quot;</span>,</span><br><span class="line">    logging_steps = <span class="number">1</span>,</span><br><span class="line">    bf16 = is_bfloat16_supported(),</span><br><span class="line">    fp16 = <span class="keyword">not</span> is_bfloat16_supported(),</span><br><span class="line">    per_device_train_batch_size = <span class="number">1</span>,</span><br><span class="line">    gradient_accumulation_steps = <span class="number">1</span>, <span class="comment"># Increase to 4 for smoother training</span></span><br><span class="line">    num_generations = <span class="number">6</span>, <span class="comment"># Decrease if out of memory</span></span><br><span class="line">    max_prompt_length = <span class="number">256</span>,</span><br><span class="line">    max_completion_length = <span class="number">200</span>,</span><br><span class="line">    <span class="comment"># num_train_epochs = 1, # Set to 1 for a full training run</span></span><br><span class="line">    max_steps = <span class="number">100</span>,</span><br><span class="line">    save_steps = <span class="number">250</span>,</span><br><span class="line">    max_grad_norm = <span class="number">0.1</span>,</span><br><span class="line">    report_to = <span class="string">&quot;none&quot;</span>, <span class="comment"># Can use Weights &amp; Biases</span></span><br><span class="line">    output_dir = <span class="string">&quot;outputs&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这个 GRPOConfig 配置了一个旨在进行 <strong>内存高效</strong> 且 <strong>快速</strong>（利用 vLLM）的 GRPO 偏好优化训练过程。它使用了 8 位分页优化器、混合精度（优先 bf16）、小批次大小（但注释建议增加梯度累积）、学习率预热和余弦衰减、梯度裁剪等常见技术。训练被设置为一个较短的运行（100 步），并配置了内部生成过程的参数（生成数量、最大长度）。日志记录非常频繁，但外部报告被禁用。这个配置适合在资源受限的环境下进行实验或快速迭代。</p>
<p><strong>第四块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">trainer = GRPOTrainer(</span><br><span class="line">model = model,</span><br><span class="line">processing_class = tokenizer,</span><br><span class="line">reward_funcs = [</span><br><span class="line">xmlcount_reward_func,</span><br><span class="line">soft_format_reward_func,</span><br><span class="line">strict_format_reward_func,</span><br><span class="line">int_reward_func,</span><br><span class="line">correctness_reward_func,</span><br><span class="line">],</span><br><span class="line">args = training_args,</span><br><span class="line">train_dataset = dataset,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<p>这段代码使用前面定义的模型、分词器、奖励函数和训练参数来实例化并启动 GRPOTrainer。整合了前面所有的准备工作（模型加载与 PEFT 配置、数据准备、奖励函数定义、训练参数设置），创建了一个 GRPOTrainer 实例。然后通过调用 trainer.train()，启动了一个基于偏好优化的训练过程。该过程旨在微调 LoRA 适配器，使基础模型（Phi-4）在保持其原有能力的同时，更好地生成符合 GSM8k 数据集要求、具有特定 XML 格式（包含推理过程和答案）、且答案尽可能正确的响应。训练过程利用了多种奖励信号来从不同角度指导模型的学习。</p>
<p><strong>第五块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">text = tokenizer.apply_chat_template([</span><br><span class="line">&#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span> : <span class="string">&quot;Which is bigger? 9.11 or 9.9?&quot;</span>&#125;,</span><br><span class="line">], tokenize = <span class="literal">False</span>, add_generation_prompt = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> SamplingParams</span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">temperature = <span class="number">0.8</span>,</span><br><span class="line">top_p = <span class="number">0.95</span>,</span><br><span class="line">max_tokens = <span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line">output = model.fast_generate(</span><br><span class="line">[text],</span><br><span class="line">sampling_params = sampling_params,</span><br><span class="line">lora_request = <span class="literal">None</span>,</span><br><span class="line">)[<span class="number">0</span>].outputs[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p>这行代码的作用是在打印 原始模型输出的output 变量的值，也就是原始模型生成的最终文本响应。</p>
<p><strong>第六块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">model.save_lora(<span class="string">&quot;grpo_saved_lora&quot;</span>)</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template([</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span> : SYSTEM_PROMPT&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span> : <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span> : <span class="string">&quot;Which is bigger? 9.11 or 9.9?&quot;</span>&#125;,</span><br><span class="line">], tokenize = <span class="literal">False</span>, add_generation_prompt = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> SamplingParams</span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature = <span class="number">0.8</span>,</span><br><span class="line">    top_p = <span class="number">0.95</span>,</span><br><span class="line">    max_tokens = <span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line">output = model.fast_generate(</span><br><span class="line">    text,</span><br><span class="line">    sampling_params = sampling_params,</span><br><span class="line">    lora_request = model.load_lora(<span class="string">&quot;grpo_saved_lora&quot;</span>),</span><br><span class="line">)[<span class="number">0</span>].outputs[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p>保存刚才通过<code>GRPO</code>训练得到的<code>LoRA</code>权重，然后加载<code>LoRA</code>权重，输入相同的问题做直观对比。</p>
<p><strong>第七块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Merge to 16bit</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_merged(<span class="string">&quot;model&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_16bit&quot;</span>,)</span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_merged(<span class="string">&quot;hf/model&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_16bit&quot;</span>, token = <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Merge to 4bit</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_merged(<span class="string">&quot;model&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_4bit&quot;</span>,)</span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_merged(<span class="string">&quot;hf/model&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_4bit&quot;</span>, token = <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Just LoRA adapters</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_merged(<span class="string">&quot;model&quot;</span>, tokenizer, save_method = <span class="string">&quot;lora&quot;</span>,)</span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_merged(<span class="string">&quot;hf/model&quot;</span>, tokenizer, save_method = <span class="string">&quot;lora&quot;</span>, token = <span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>保存融合（LoRA的）模型或仅保存LoRA权重，也可以选择push到hf仓库，用于vLLM部署。</p>
<p><strong>第八块：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save to 8bit Q8_0</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer,)</span><br><span class="line"><span class="comment"># Remember to go to https://huggingface.co/settings/tokens for a token!</span></span><br><span class="line"><span class="comment"># And change hf to your username!</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_gguf(<span class="string">&quot;hf/model&quot;</span>, tokenizer, token = <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to 16bit GGUF</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;f16&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_gguf(<span class="string">&quot;hf/model&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;f16&quot;</span>, token = <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to q4_k_m GGUF</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.save_pretrained_gguf(<span class="string">&quot;model&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;q4_k_m&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: model.push_to_hub_gguf(<span class="string">&quot;hf/model&quot;</span>, tokenizer, quantization_method = <span class="string">&quot;q4_k_m&quot;</span>, token = <span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to multiple GGUF options - much faster if you want multiple!</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.push_to_hub_gguf(</span><br><span class="line">        <span class="string">&quot;hf/model&quot;</span>, <span class="comment"># Change hf to your username!</span></span><br><span class="line">        tokenizer,</span><br><span class="line">        quantization_method = [<span class="string">&quot;q4_k_m&quot;</span>, <span class="string">&quot;q8_0&quot;</span>, <span class="string">&quot;q5_k_m&quot;</span>,],</span><br><span class="line">        token = <span class="string">&quot;&quot;</span>,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>与第七块类似，只是保存gguf格式以用于ollama或llama.cpp。</p>
<hr>
<h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><p>首先就是诡异的<code>loss</code>和 ：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Step 	Training Loss 	reward 	reward<span class="built_in">_</span>std 	completion<span class="built_in">_</span>length 	kl 	rewards / xmlcount<span class="built_in">_</span>reward<span class="built_in">_</span>func 	rewards / soft<span class="built_in">_</span>format<span class="built_in">_</span>reward<span class="built_in">_</span>func 	rewards / strict<span class="built_in">_</span>format<span class="built_in">_</span>reward<span class="built_in">_</span>func 	rewards / int<span class="built_in">_</span>reward<span class="built_in">_</span>func 	rewards / correctness<span class="built_in">_</span>reward<span class="built_in">_</span>func</span><br><span class="line">1 	0.000000 	0.125000 	0.000000 	200.000000 	0.000000 	0.125000 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">2 	0.000000 	0.104167 	0.051031 	200.000000 	0.000000 	0.104167 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">3 	0.000000 	0.125000 	0.000000 	200.000000 	0.000097 	0.125000 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">4 	0.000000 	0.125000 	0.000000 	200.000000 	0.000133 	0.125000 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">5 	0.000000 	-0.163500 	0.078355 	192.666672 	0.000136 	-0.163500 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">6 	0.000000 	0.104167 	0.051031 	200.000000 	0.000076 	0.104167 	0.000000 	0.000000 	0.000000 	0.000000</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Training Loss (训练损失):</strong> 这是最<strong>关键和可疑</strong>的一点。训练损失几乎全程为 0.000000 (除了少数几个极小的非零值)。在正常的训练过程中，损失值应该是一个反映模型预测与目标之间差异的正数，并且会随着训练进行而波动（通常是下降）</li>
<li><strong>格式奖励 (soft_format, strict_format):</strong> 这两项一直为零：模型生成的响应<strong>完全没有</strong>满足哪怕是宽松的 XML 格式要求。这说明模型在学习遵循 <reasoning>…</reasoning><answer>…</answer> 结构方面非常困难。</li>
<li><strong>整数奖励 (int_reward):</strong> 大部分时间为零，偶尔有非零值。这表示模型偶尔能在（可能格式错误的）输出中生成看起来像数字的内容，但大多数时候不行。非零值经常与非零的 correctness_reward 同时出现，这符合逻辑（正确答案通常是数字）。</li>
<li><strong>正确性奖励 (correctness_reward):</strong> 同样，大部分时间为零，表明模型很少能输出<strong>格式正确且答案数值也正确</strong>的结果。偶尔出现的非零值（如 1.0, 1.333）是怎么回事？回顾 correctness_reward_func，它对每个正确的回答奖励 2.0。这里的日志记录的是<strong>平均值</strong>。假设 num_generations&#x3D;6，如果 6 个生成结果中有 3 个是正确的，平均奖励就是 (3 * 2.0 + 3 * 0.0) &#x2F; 6 &#x3D; 1.0。如果 4 个正确，就是 (4 * 2.0 + 2 * 0.0) &#x2F; 6 &#x3D; 1.333。所以这些非零值表示模型在某些步骤中，生成的多个响应里有部分是完全正确的，但<strong>绝大多数步骤里，一个正确的都没有</strong>。</li>
</ul>
<p>在unsloth的git仓库也看到有人说loss为0这个事情，但并没有个具体的答案（只是让试试其他方式）。但是，看输出结果是有作用的，<strong>微调前输出：</strong></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;9.11 is bigger than 9.9. When comparing decimal numbers, you compare them digit by digit from left to right. Both numbers have 9 as the whole number part, so you compare the digits after the decimal point. The first digit after the decimal point in 9.11 is 1, and in 9.9, it is 9. Since 1 is less than 9, you move to the next digit. In 9.11, the next digit is 1, and in 9.9, it is implied to be 0 (since 9.9 is equivalent to 9.90). Therefore, 9.11 is greater than 9.90.&#x27;</span><br></pre></td></tr></table></figure>

<p><strong>微调后：</strong></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;reasoning&gt;;<span class="keyword">\nTo</span> determine which number is bigger between 9.11 and 9.9, we compare them starting from the leftmost digit.<span class="keyword">\n</span><span class="keyword">\n</span>1. Compare the whole number part before the decimal point:<span class="keyword">\n</span>   - Both numbers have 9 as the whole number part.<span class="keyword">\n</span><span class="keyword">\n</span>2. Compare the tenths place (the first digit after the decimal point):<span class="keyword">\n</span>   - 9.11 has a 1 in the tenths place.<span class="keyword">\n</span>   - 9.9 has a 9 in the tenths place.<span class="keyword">\n</span><span class="keyword">\nSince</span> 1 is less than 9, 9.11 is smaller than 9.9 at this point. There is no need to compare further digits because the comparison at the tenths place already determines which number is larger.<span class="keyword">\n</span>&lt;/reasoning&gt;;<span class="keyword">\n</span>&lt;answer&gt;;<span class="keyword">\n</span>9.9 is bigger than 9.11.<span class="keyword">\n</span>&lt;/answer&gt;;&#x27;</span><br></pre></td></tr></table></figure>

<p>可以看出作用还是很明显的，而且答对了。</p>
<p>调整下<strong>第三块</strong>的训练参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> GRPOConfig, GRPOTrainer</span><br><span class="line">training_args = GRPOConfig(</span><br><span class="line">    use_vllm = <span class="literal">True</span>, </span><br><span class="line">    learning_rate = <span class="number">5e-6</span>,</span><br><span class="line">    adam_beta1 = <span class="number">0.9</span>,</span><br><span class="line">    adam_beta2 = <span class="number">0.99</span>,</span><br><span class="line">    weight_decay = <span class="number">0.1</span>,</span><br><span class="line">    warmup_ratio = <span class="number">0.1</span>,</span><br><span class="line">    lr_scheduler_type = <span class="string">&quot;cosine&quot;</span>,</span><br><span class="line">    optim = <span class="string">&quot;paged_adamw_8bit&quot;</span>,</span><br><span class="line">    logging_steps = <span class="number">1</span>,</span><br><span class="line">    bf16 = is_bfloat16_supported(),</span><br><span class="line">    fp16 = <span class="keyword">not</span> is_bfloat16_supported(),</span><br><span class="line">    per_device_train_batch_size = <span class="number">1</span>,</span><br><span class="line">    gradient_accumulation_steps = <span class="number">4</span>,  <span class="comment"># this</span></span><br><span class="line">    num_generations = <span class="number">8</span>, 	<span class="comment"># this</span></span><br><span class="line">    max_prompt_length = <span class="number">256</span>,</span><br><span class="line">    max_completion_length = <span class="number">200</span>,</span><br><span class="line">    <span class="comment"># num_train_epochs = 1, </span></span><br><span class="line">    max_steps = <span class="number">300</span>,  <span class="comment"># this</span></span><br><span class="line">    save_steps = <span class="number">250</span>,</span><br><span class="line">    max_grad_norm = <span class="number">0.1</span>,</span><br><span class="line">    report_to = <span class="string">&quot;none&quot;</span>, </span><br><span class="line">    output_dir = <span class="string">&quot;outputs&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>更改了三个地方：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">gradient_accumulation_steps</span> = <span class="number">4</span>,  <span class="comment"># this</span></span><br><span class="line"><span class="attr">num_generations</span> = <span class="number">8</span>, 	<span class="comment"># this</span></span><br><span class="line"><span class="attr">max_steps</span> = <span class="number">300</span>,  <span class="comment"># this</span></span><br></pre></td></tr></table></figure>

<p>训练的中间数据见文章最后的附录，此处只做文字总结：</p>
<ul>
<li>总体比修改前正常多了</li>
<li>loss前60条也只是仅有稀疏并很小的数，后面就正常了</li>
<li>soft_format_reward_func和strict_format_reward_func依旧全是0，其他的奖励函数都要比修改前正常</li>
</ul>
<p>但是吧（哈哈哈哈，又来但是了🤪），微调后的输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;reasoning&gt;</span><br><span class="line">9.11 has a higher value in the tenths place compared to 9.9. Specifically, 9.11 has 1 in the tenths place, while 9.9 has 9 in the tenths place. When comparing the numbers, 9.9 is larger because 9 in the tenths place is greater than 1 in the tenths place.</span><br><span class="line">&lt;/reasoning&gt;</span><br><span class="line">&lt;answer&gt;</span><br><span class="line">9.9</span><br><span class="line">&lt;/answer&gt;</span><br></pre></td></tr></table></figure>

<p>答案是对了，但是推理过程：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与 9.9 相比，9.11 在十分位上的值更高。具体来说，9.11 的十分位是 1，而 9.9 的十分位是 9。当比较这两个数时，9.9 更大，因为十分位上的 9 大于十分位上的 1。</span><br></pre></td></tr></table></figure>

<p>第一句话错了，但后续又对了😂，但是也合理，因为几个奖励函数并没有验证推理的正确性，而且一般可能也不看推理过程，只看结果的正确性。而且我多运行几次，发现有时候答案给的是错的，说明现在的结果并不稳定，后续在实际案例中再做深入优化吧。</p>
<p>有一个很大的疑问：为什么soft_format_reward_func和strict_format_reward_func依旧全是0？不能理解</p>
<p>我们看下代码</p>
<hr>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ol>
<li><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb">https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb</a></li>
<li>l</li>
<li><a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth/issues/1761">https://github.com/unslothai/unsloth/issues/1761</a></li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><strong>原训练参数</strong></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Training Loss</th>
<th>reward</th>
<th>reward_std</th>
<th>completion_length</th>
<th>kl</th>
<th>rewards &#x2F; xmlcount_reward_func</th>
<th>rewards &#x2F; soft_format_reward_func</th>
<th>rewards &#x2F; strict_format_reward_func</th>
<th>rewards &#x2F; int_reward_func</th>
<th>rewards &#x2F; correctness_reward_func</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>2</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000097</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>4</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000133</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>5</td>
<td>0.000000</td>
<td>-0.163500</td>
<td>0.078355</td>
<td>192.666672</td>
<td>0.000136</td>
<td>-0.163500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000076</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000145</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.077167</td>
<td>0.117167</td>
<td>195.666672</td>
<td>0.000278</td>
<td>0.077167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>0.044500</td>
<td>0.197184</td>
<td>200.000000</td>
<td>0.000168</td>
<td>0.044500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000143</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>11</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.000135</td>
<td>0.166667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>12</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.000212</td>
<td>0.083333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>13</td>
<td>0.000000</td>
<td>0.052167</td>
<td>0.178405</td>
<td>200.000000</td>
<td>0.000112</td>
<td>0.052167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>14</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000279</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>15</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000122</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>1.220333</td>
<td>1.200325</td>
<td>189.833344</td>
<td>0.000146</td>
<td>-0.029667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>17</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000166</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>18</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000092</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>19</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.000156</td>
<td>0.083333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>20</td>
<td>0.000000</td>
<td>0.020833</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000267</td>
<td>0.020833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>21</td>
<td>0.000000</td>
<td>0.041667</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.000127</td>
<td>0.041667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>22</td>
<td>0.000000</td>
<td>0.008167</td>
<td>0.301007</td>
<td>188.500000</td>
<td>0.000133</td>
<td>-0.075167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.000000</td>
</tr>
<tr>
<td>23</td>
<td>0.000000</td>
<td>0.085000</td>
<td>0.097980</td>
<td>195.333344</td>
<td>0.000142</td>
<td>0.085000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>24</td>
<td>0.000000</td>
<td>0.025833</td>
<td>0.161067</td>
<td>198.833344</td>
<td>0.000170</td>
<td>0.025833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>25</td>
<td>0.000000</td>
<td>-0.001500</td>
<td>0.199920</td>
<td>195.000000</td>
<td>0.000184</td>
<td>-0.001500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>26</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000220</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>27</td>
<td>0.000000</td>
<td>-0.005667</td>
<td>0.213240</td>
<td>197.833344</td>
<td>0.000164</td>
<td>-0.005667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>28</td>
<td>0.000000</td>
<td>1.290833</td>
<td>1.278981</td>
<td>178.000000</td>
<td>0.000168</td>
<td>0.040833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>29</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000088</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>30</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000286</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>31</td>
<td>0.000000</td>
<td>-0.124833</td>
<td>0.077288</td>
<td>164.666672</td>
<td>0.000193</td>
<td>-0.124833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>32</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000304</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>33</td>
<td>0.000000</td>
<td>0.384167</td>
<td>0.931063</td>
<td>188.500000</td>
<td>0.000516</td>
<td>-0.032500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>34</td>
<td>0.000000</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.000127</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>35</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000244</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>36</td>
<td>0.000000</td>
<td>0.069000</td>
<td>0.204612</td>
<td>200.000000</td>
<td>0.000110</td>
<td>0.069000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>37</td>
<td>0.000000</td>
<td>0.472167</td>
<td>0.850381</td>
<td>200.000000</td>
<td>0.000210</td>
<td>0.055500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>38</td>
<td>0.000000</td>
<td>-0.163167</td>
<td>0.198312</td>
<td>189.666672</td>
<td>0.000187</td>
<td>-0.163167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>39</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000111</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>40</td>
<td>0.000100</td>
<td>0.083333</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.001334</td>
<td>0.083333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>41</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000198</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>42</td>
<td>0.000000</td>
<td>0.051667</td>
<td>0.179629</td>
<td>200.000000</td>
<td>0.000188</td>
<td>0.051667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>43</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000151</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>44</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000257</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>45</td>
<td>0.000000</td>
<td>0.041667</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.001159</td>
<td>0.041667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>46</td>
<td>0.000000</td>
<td>0.780000</td>
<td>1.127370</td>
<td>194.166672</td>
<td>0.000144</td>
<td>-0.053333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.666667</td>
</tr>
<tr>
<td>47</td>
<td>0.000000</td>
<td>0.082167</td>
<td>0.037140</td>
<td>149.666672</td>
<td>0.000179</td>
<td>0.082167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>48</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000105</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>49</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000162</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>50</td>
<td>0.000000</td>
<td>0.073500</td>
<td>0.126149</td>
<td>199.166672</td>
<td>0.000169</td>
<td>0.073500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>51</td>
<td>0.000000</td>
<td>0.008333</td>
<td>0.131736</td>
<td>184.833344</td>
<td>0.000145</td>
<td>0.008333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>52</td>
<td>0.000200</td>
<td>0.083333</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.004634</td>
<td>0.083333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>53</td>
<td>0.000100</td>
<td>0.104167</td>
<td>0.051031</td>
<td>200.000000</td>
<td>0.002473</td>
<td>0.104167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>54</td>
<td>0.000000</td>
<td>0.040500</td>
<td>0.206982</td>
<td>200.000000</td>
<td>0.000196</td>
<td>0.040500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>55</td>
<td>0.000000</td>
<td>0.019333</td>
<td>0.090480</td>
<td>170.833344</td>
<td>0.000139</td>
<td>0.019333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>56</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000181</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>57</td>
<td>0.000000</td>
<td>0.078833</td>
<td>0.113085</td>
<td>197.000000</td>
<td>0.000471</td>
<td>0.078833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>58</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000076</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>59</td>
<td>0.000000</td>
<td>-0.013667</td>
<td>0.220845</td>
<td>199.500000</td>
<td>0.000208</td>
<td>-0.013667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>60</td>
<td>0.000000</td>
<td>-0.068000</td>
<td>0.044605</td>
<td>157.833344</td>
<td>0.000259</td>
<td>-0.068000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>61</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000202</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>62</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000790</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>63</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000166</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>64</td>
<td>0.000100</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.001505</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>65</td>
<td>0.000000</td>
<td>0.039667</td>
<td>0.209023</td>
<td>200.000000</td>
<td>0.000290</td>
<td>0.039667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>66</td>
<td>0.000000</td>
<td>0.094833</td>
<td>0.073893</td>
<td>195.500000</td>
<td>0.000080</td>
<td>0.094833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>67</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000139</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>68</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000889</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>69</td>
<td>0.000600</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.015919</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>70</td>
<td>0.000000</td>
<td>-0.029167</td>
<td>0.240278</td>
<td>189.333344</td>
<td>0.000252</td>
<td>-0.029167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>71</td>
<td>0.000000</td>
<td>0.562000</td>
<td>1.023501</td>
<td>176.000000</td>
<td>0.000180</td>
<td>-0.104667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.666667</td>
</tr>
<tr>
<td>72</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000189</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>73</td>
<td>0.000000</td>
<td>0.444500</td>
<td>0.928717</td>
<td>194.666672</td>
<td>0.000288</td>
<td>0.027833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>74</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000230</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>75</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000097</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>76</td>
<td>0.000000</td>
<td>-0.053167</td>
<td>0.066653</td>
<td>163.833344</td>
<td>0.000088</td>
<td>-0.053167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>77</td>
<td>0.000100</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.001454</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>78</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000308</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>79</td>
<td>0.000000</td>
<td>0.892167</td>
<td>1.240599</td>
<td>146.000000</td>
<td>0.000181</td>
<td>0.058833</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.666667</td>
</tr>
<tr>
<td>80</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000207</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>81</td>
<td>0.000000</td>
<td>0.833000</td>
<td>1.264935</td>
<td>165.166672</td>
<td>0.000162</td>
<td>-0.000333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.666667</td>
</tr>
<tr>
<td>82</td>
<td>0.000000</td>
<td>0.793833</td>
<td>1.218390</td>
<td>190.000000</td>
<td>0.000229</td>
<td>-0.039500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.666667</td>
</tr>
<tr>
<td>83</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000220</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>84</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000551</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>85</td>
<td>0.000000</td>
<td>-0.298000</td>
<td>0.217899</td>
<td>191.000000</td>
<td>0.000378</td>
<td>-0.298000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>86</td>
<td>0.000000</td>
<td>1.518333</td>
<td>1.290365</td>
<td>170.500000</td>
<td>0.000193</td>
<td>-0.148333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.333333</td>
<td>1.333333</td>
</tr>
<tr>
<td>87</td>
<td>0.000000</td>
<td>0.166667</td>
<td>0.064550</td>
<td>200.000000</td>
<td>0.000192</td>
<td>0.166667</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>88</td>
<td>0.000000</td>
<td>0.474833</td>
<td>0.856913</td>
<td>200.000000</td>
<td>0.000232</td>
<td>0.058167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>89</td>
<td>0.000000</td>
<td>-0.092167</td>
<td>0.239684</td>
<td>196.666672</td>
<td>0.000827</td>
<td>-0.092167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>90</td>
<td>0.000000</td>
<td>0.448833</td>
<td>0.955630</td>
<td>197.500000</td>
<td>0.000118</td>
<td>0.032167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>91</td>
<td>0.000000</td>
<td>0.377500</td>
<td>1.052700</td>
<td>178.833344</td>
<td>0.000211</td>
<td>-0.039167</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>92</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000142</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>93</td>
<td>0.000000</td>
<td>0.061333</td>
<td>0.222869</td>
<td>200.000000</td>
<td>0.000116</td>
<td>0.061333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>94</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000167</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>95</td>
<td>0.000000</td>
<td>0.480000</td>
<td>0.869569</td>
<td>198.666672</td>
<td>0.000403</td>
<td>0.063333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.083333</td>
<td>0.333333</td>
</tr>
<tr>
<td>96</td>
<td>0.000000</td>
<td>0.045500</td>
<td>0.178344</td>
<td>198.666672</td>
<td>0.000174</td>
<td>0.045500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>97</td>
<td>0.000000</td>
<td>-0.043333</td>
<td>0.261058</td>
<td>200.000000</td>
<td>0.000143</td>
<td>-0.043333</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>98</td>
<td>0.000000</td>
<td>-0.086500</td>
<td>0.094614</td>
<td>177.833344</td>
<td>0.000174</td>
<td>-0.086500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>99</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000116</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>100</td>
<td>0.000000</td>
<td>0.013000</td>
<td>0.233636</td>
<td>199.166672</td>
<td>0.000215</td>
<td>0.013000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
</tbody></table>
<p><strong>修改后的训练参数</strong></p>
<table>
<thead>
<tr>
<th>Step</th>
<th>Training Loss</th>
<th>reward</th>
<th>reward_std</th>
<th>completion_length</th>
<th>kl</th>
<th>rewards &#x2F; xmlcount_reward_func</th>
<th>rewards &#x2F; soft_format_reward_func</th>
<th>rewards &#x2F; strict_format_reward_func</th>
<th>rewards &#x2F; int_reward_func</th>
<th>rewards &#x2F; correctness_reward_func</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.000000</td>
<td>0.128906</td>
<td>0.011049</td>
<td>200.000000</td>
<td>0.000200</td>
<td>0.128906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>2</td>
<td>0.000000</td>
<td>0.091437</td>
<td>0.272227</td>
<td>198.281250</td>
<td>0.000199</td>
<td>0.013312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>0.299875</td>
<td>0.365889</td>
<td>197.125000</td>
<td>0.000217</td>
<td>0.065500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>4</td>
<td>0.000000</td>
<td>0.596656</td>
<td>0.471141</td>
<td>195.250000</td>
<td>0.000232</td>
<td>0.049781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>5</td>
<td>0.000100</td>
<td>0.121094</td>
<td>0.011049</td>
<td>200.000000</td>
<td>0.001695</td>
<td>0.121094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>0.127625</td>
<td>0.289390</td>
<td>199.562500</td>
<td>0.000245</td>
<td>0.018250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.062500</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>0.113188</td>
<td>0.350681</td>
<td>196.781250</td>
<td>0.000222</td>
<td>0.035062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>0.014656</td>
<td>0.104425</td>
<td>195.562500</td>
<td>0.000193</td>
<td>0.014656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>9</td>
<td>0.000100</td>
<td>0.361219</td>
<td>0.536969</td>
<td>190.156250</td>
<td>0.001660</td>
<td>0.048719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>0.087312</td>
<td>0.061786</td>
<td>197.750000</td>
<td>0.000293</td>
<td>0.087312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>11</td>
<td>0.000000</td>
<td>0.128906</td>
<td>0.011049</td>
<td>200.000000</td>
<td>0.000200</td>
<td>0.128906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>12</td>
<td>0.000100</td>
<td>0.508000</td>
<td>0.524590</td>
<td>183.156250</td>
<td>0.001624</td>
<td>0.039250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>13</td>
<td>0.000100</td>
<td>0.071063</td>
<td>0.093637</td>
<td>199.625000</td>
<td>0.001865</td>
<td>0.071063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>14</td>
<td>0.000000</td>
<td>0.078500</td>
<td>0.072087</td>
<td>193.343750</td>
<td>0.000217</td>
<td>0.078500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>15</td>
<td>0.000000</td>
<td>0.027813</td>
<td>0.119273</td>
<td>190.093750</td>
<td>0.000223</td>
<td>0.027812</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000884</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>17</td>
<td>0.000200</td>
<td>0.100125</td>
<td>0.051338</td>
<td>199.218750</td>
<td>0.004186</td>
<td>0.100125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>18</td>
<td>0.000000</td>
<td>0.047375</td>
<td>0.065061</td>
<td>193.781250</td>
<td>0.000229</td>
<td>0.047375</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>19</td>
<td>0.000000</td>
<td>0.129312</td>
<td>0.230931</td>
<td>195.593750</td>
<td>0.000208</td>
<td>0.051188</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>20</td>
<td>0.000000</td>
<td>0.266562</td>
<td>0.301932</td>
<td>185.843750</td>
<td>0.000446</td>
<td>0.110312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>21</td>
<td>0.000000</td>
<td>0.517438</td>
<td>0.537948</td>
<td>190.468750</td>
<td>0.000327</td>
<td>0.048688</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>22</td>
<td>0.000000</td>
<td>0.104937</td>
<td>0.411467</td>
<td>190.406250</td>
<td>0.000256</td>
<td>-0.051312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>23</td>
<td>0.000000</td>
<td>0.541125</td>
<td>0.657217</td>
<td>194.406250</td>
<td>0.000334</td>
<td>-0.005750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>24</td>
<td>0.000000</td>
<td>0.055250</td>
<td>0.159929</td>
<td>198.812500</td>
<td>0.000211</td>
<td>0.055250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>25</td>
<td>0.000000</td>
<td>0.049281</td>
<td>0.130569</td>
<td>197.437500</td>
<td>0.000147</td>
<td>0.049281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>26</td>
<td>0.000000</td>
<td>0.276562</td>
<td>0.416252</td>
<td>175.281250</td>
<td>0.000278</td>
<td>0.042188</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>27</td>
<td>0.000000</td>
<td>0.306031</td>
<td>0.362150</td>
<td>188.625000</td>
<td>0.000242</td>
<td>-0.006469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>28</td>
<td>0.000000</td>
<td>0.678031</td>
<td>0.786093</td>
<td>179.968750</td>
<td>0.000221</td>
<td>-0.025094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.140625</td>
<td>0.562500</td>
</tr>
<tr>
<td>29</td>
<td>0.000000</td>
<td>0.063125</td>
<td>0.114379</td>
<td>197.875000</td>
<td>0.000592</td>
<td>0.063125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>30</td>
<td>0.000000</td>
<td>0.335156</td>
<td>0.559257</td>
<td>187.250000</td>
<td>0.000207</td>
<td>0.022656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>31</td>
<td>0.000000</td>
<td>0.114281</td>
<td>0.030317</td>
<td>199.781250</td>
<td>0.000312</td>
<td>0.114281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>32</td>
<td>0.000000</td>
<td>0.020219</td>
<td>0.135688</td>
<td>192.562500</td>
<td>0.000204</td>
<td>0.020219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>33</td>
<td>0.000100</td>
<td>0.101625</td>
<td>0.043579</td>
<td>199.906250</td>
<td>0.001501</td>
<td>0.101625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>34</td>
<td>0.000000</td>
<td>-0.003125</td>
<td>0.064681</td>
<td>192.062500</td>
<td>0.000249</td>
<td>-0.003125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>35</td>
<td>0.000000</td>
<td>0.444406</td>
<td>0.383764</td>
<td>175.937500</td>
<td>0.001186</td>
<td>0.053781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.078125</td>
<td>0.312500</td>
</tr>
<tr>
<td>36</td>
<td>0.000000</td>
<td>0.073656</td>
<td>0.082237</td>
<td>199.093750</td>
<td>0.000238</td>
<td>0.073656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>37</td>
<td>0.000000</td>
<td>0.209313</td>
<td>0.321363</td>
<td>193.375000</td>
<td>0.000293</td>
<td>0.053062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>38</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000940</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>39</td>
<td>0.000500</td>
<td>0.097687</td>
<td>0.054421</td>
<td>200.000000</td>
<td>0.011868</td>
<td>0.097687</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>40</td>
<td>0.000100</td>
<td>0.193656</td>
<td>0.194189</td>
<td>199.875000</td>
<td>0.003135</td>
<td>0.115531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>41</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000438</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>42</td>
<td>0.000000</td>
<td>0.172844</td>
<td>0.221426</td>
<td>199.562500</td>
<td>0.001187</td>
<td>0.094719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>43</td>
<td>0.000000</td>
<td>0.080094</td>
<td>0.043903</td>
<td>193.687500</td>
<td>0.000243</td>
<td>0.080094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>44</td>
<td>0.000000</td>
<td>1.021969</td>
<td>0.336553</td>
<td>189.375000</td>
<td>0.000266</td>
<td>0.006344</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>45</td>
<td>0.000100</td>
<td>0.040000</td>
<td>0.096341</td>
<td>195.312500</td>
<td>0.001694</td>
<td>0.040000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>46</td>
<td>0.000000</td>
<td>0.459094</td>
<td>0.547510</td>
<td>188.281250</td>
<td>0.000386</td>
<td>0.068469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.078125</td>
<td>0.312500</td>
</tr>
<tr>
<td>47</td>
<td>0.000100</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.001254</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>48</td>
<td>0.000000</td>
<td>0.554219</td>
<td>0.265542</td>
<td>193.125000</td>
<td>0.000314</td>
<td>0.085469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>49</td>
<td>0.000000</td>
<td>0.566031</td>
<td>0.296579</td>
<td>192.343750</td>
<td>0.001164</td>
<td>0.019156</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>50</td>
<td>0.000000</td>
<td>0.149875</td>
<td>0.272207</td>
<td>188.500000</td>
<td>0.000295</td>
<td>0.071750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>51</td>
<td>0.000000</td>
<td>0.527062</td>
<td>0.533761</td>
<td>195.187500</td>
<td>0.000272</td>
<td>0.058312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>52</td>
<td>0.000000</td>
<td>0.088687</td>
<td>0.102707</td>
<td>199.875000</td>
<td>0.000253</td>
<td>0.088687</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>53</td>
<td>0.000000</td>
<td>0.499219</td>
<td>0.575142</td>
<td>195.750000</td>
<td>0.000369</td>
<td>0.030469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>54</td>
<td>0.000000</td>
<td>0.475094</td>
<td>0.830660</td>
<td>187.843750</td>
<td>0.000331</td>
<td>-0.009281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.375000</td>
</tr>
<tr>
<td>55</td>
<td>0.000000</td>
<td>0.114250</td>
<td>0.044400</td>
<td>200.000000</td>
<td>0.000202</td>
<td>0.114250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>56</td>
<td>0.000000</td>
<td>0.272125</td>
<td>0.321911</td>
<td>196.312500</td>
<td>0.000684</td>
<td>0.037750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>57</td>
<td>0.000000</td>
<td>0.318687</td>
<td>0.469326</td>
<td>199.062500</td>
<td>0.000417</td>
<td>0.084312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>58</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000310</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>59</td>
<td>0.000100</td>
<td>0.769313</td>
<td>0.569422</td>
<td>179.468750</td>
<td>0.001617</td>
<td>-0.011937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>60</td>
<td>0.000000</td>
<td>1.208313</td>
<td>1.032118</td>
<td>176.156250</td>
<td>0.000968</td>
<td>0.036438</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>61</td>
<td>0.000000</td>
<td>0.356281</td>
<td>0.527199</td>
<td>184.343750</td>
<td>0.001006</td>
<td>0.043781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>62</td>
<td>0.000100</td>
<td>0.187188</td>
<td>0.175893</td>
<td>200.000000</td>
<td>0.001395</td>
<td>0.109063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>63</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.000990</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>64</td>
<td>0.000000</td>
<td>0.132687</td>
<td>0.262365</td>
<td>196.125000</td>
<td>0.000882</td>
<td>0.054563</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>65</td>
<td>0.000000</td>
<td>0.618719</td>
<td>0.922293</td>
<td>196.468750</td>
<td>0.000420</td>
<td>0.071844</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>66</td>
<td>0.000100</td>
<td>0.129062</td>
<td>0.245166</td>
<td>199.031250</td>
<td>0.001289</td>
<td>0.050938</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>67</td>
<td>0.000000</td>
<td>0.623875</td>
<td>0.202422</td>
<td>191.031250</td>
<td>0.000446</td>
<td>0.077000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>68</td>
<td>0.000000</td>
<td>0.256937</td>
<td>0.283389</td>
<td>197.656250</td>
<td>0.000639</td>
<td>0.100687</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>69</td>
<td>0.000000</td>
<td>0.100562</td>
<td>0.035478</td>
<td>198.781250</td>
<td>0.000369</td>
<td>0.100562</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>70</td>
<td>0.000100</td>
<td>0.068187</td>
<td>0.049234</td>
<td>198.468750</td>
<td>0.003580</td>
<td>0.068188</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>71</td>
<td>0.000200</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.004849</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>72</td>
<td>0.000100</td>
<td>0.195063</td>
<td>0.198167</td>
<td>199.593750</td>
<td>0.002061</td>
<td>0.116937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>73</td>
<td>0.000100</td>
<td>0.389719</td>
<td>0.607224</td>
<td>188.937500</td>
<td>0.002278</td>
<td>-0.000906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.078125</td>
<td>0.312500</td>
</tr>
<tr>
<td>74</td>
<td>0.000400</td>
<td>0.890375</td>
<td>1.126343</td>
<td>169.031250</td>
<td>0.010070</td>
<td>0.031000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.171875</td>
<td>0.687500</td>
</tr>
<tr>
<td>75</td>
<td>0.000000</td>
<td>0.162719</td>
<td>0.234266</td>
<td>189.343750</td>
<td>0.000792</td>
<td>0.084594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>76</td>
<td>0.000200</td>
<td>0.759188</td>
<td>0.914893</td>
<td>179.562500</td>
<td>0.003830</td>
<td>-0.022063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>77</td>
<td>0.000000</td>
<td>1.034500</td>
<td>0.547051</td>
<td>191.843750</td>
<td>0.000724</td>
<td>0.018875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>78</td>
<td>0.000200</td>
<td>0.787781</td>
<td>0.882335</td>
<td>188.093750</td>
<td>0.003986</td>
<td>0.006531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.687500</td>
</tr>
<tr>
<td>79</td>
<td>0.000100</td>
<td>0.708563</td>
<td>0.402601</td>
<td>195.250000</td>
<td>0.001422</td>
<td>0.083562</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.500000</td>
</tr>
<tr>
<td>80</td>
<td>0.000100</td>
<td>1.131250</td>
<td>0.916285</td>
<td>179.687500</td>
<td>0.002825</td>
<td>0.006250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>0.875000</td>
</tr>
<tr>
<td>81</td>
<td>0.000100</td>
<td>0.858906</td>
<td>0.710950</td>
<td>190.000000</td>
<td>0.002542</td>
<td>-0.000469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.171875</td>
<td>0.687500</td>
</tr>
<tr>
<td>82</td>
<td>0.000100</td>
<td>0.589000</td>
<td>0.794487</td>
<td>195.281250</td>
<td>0.001541</td>
<td>0.042125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>83</td>
<td>0.000200</td>
<td>1.044719</td>
<td>0.874095</td>
<td>190.125000</td>
<td>0.003762</td>
<td>0.029094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>84</td>
<td>0.000000</td>
<td>0.267719</td>
<td>0.264266</td>
<td>198.718750</td>
<td>0.000571</td>
<td>0.111469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>85</td>
<td>0.000100</td>
<td>0.239563</td>
<td>0.300843</td>
<td>198.562500</td>
<td>0.001571</td>
<td>0.083312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>86</td>
<td>0.000200</td>
<td>0.561062</td>
<td>0.664822</td>
<td>187.062500</td>
<td>0.005404</td>
<td>0.014187</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>87</td>
<td>0.000200</td>
<td>0.235531</td>
<td>0.290269</td>
<td>198.718750</td>
<td>0.004947</td>
<td>0.079281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>88</td>
<td>0.000100</td>
<td>0.310937</td>
<td>0.339621</td>
<td>198.687500</td>
<td>0.001520</td>
<td>0.076563</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>89</td>
<td>0.000000</td>
<td>0.116719</td>
<td>0.045520</td>
<td>200.000000</td>
<td>0.000942</td>
<td>0.116719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>90</td>
<td>0.000000</td>
<td>0.620875</td>
<td>0.588485</td>
<td>196.750000</td>
<td>0.001108</td>
<td>0.074000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>91</td>
<td>0.000100</td>
<td>0.561594</td>
<td>0.242350</td>
<td>193.406250</td>
<td>0.002773</td>
<td>0.014719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>92</td>
<td>0.000300</td>
<td>1.209375</td>
<td>0.677630</td>
<td>172.593750</td>
<td>0.006277</td>
<td>0.037500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>93</td>
<td>0.000000</td>
<td>1.103844</td>
<td>0.657026</td>
<td>191.531250</td>
<td>0.001238</td>
<td>0.010094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.218750</td>
<td>0.875000</td>
</tr>
<tr>
<td>94</td>
<td>0.000300</td>
<td>0.729719</td>
<td>0.863541</td>
<td>191.687500</td>
<td>0.007389</td>
<td>0.026594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.140625</td>
<td>0.562500</td>
</tr>
<tr>
<td>95</td>
<td>0.000100</td>
<td>0.338375</td>
<td>0.438867</td>
<td>198.718750</td>
<td>0.001460</td>
<td>0.104000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>96</td>
<td>0.000100</td>
<td>0.520844</td>
<td>0.496037</td>
<td>196.718750</td>
<td>0.003298</td>
<td>0.052094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>97</td>
<td>0.000100</td>
<td>1.041531</td>
<td>0.726904</td>
<td>192.000000</td>
<td>0.002370</td>
<td>0.025906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>98</td>
<td>0.000200</td>
<td>1.205375</td>
<td>0.966042</td>
<td>181.812500</td>
<td>0.003860</td>
<td>0.002250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>0.937500</td>
</tr>
<tr>
<td>99</td>
<td>0.000100</td>
<td>0.169125</td>
<td>0.245326</td>
<td>199.031250</td>
<td>0.002583</td>
<td>0.091000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.015625</td>
<td>0.062500</td>
</tr>
<tr>
<td>100</td>
<td>0.000400</td>
<td>0.735563</td>
<td>0.874583</td>
<td>196.187500</td>
<td>0.008870</td>
<td>0.032437</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.140625</td>
<td>0.562500</td>
</tr>
<tr>
<td>101</td>
<td>0.000200</td>
<td>0.628906</td>
<td>0.459497</td>
<td>196.781250</td>
<td>0.003757</td>
<td>0.082031</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.109375</td>
<td>0.437500</td>
</tr>
<tr>
<td>102</td>
<td>0.000100</td>
<td>0.926781</td>
<td>0.323388</td>
<td>184.406250</td>
<td>0.002390</td>
<td>0.067406</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.171875</td>
<td>0.687500</td>
</tr>
<tr>
<td>103</td>
<td>0.000200</td>
<td>0.705688</td>
<td>0.840264</td>
<td>196.625000</td>
<td>0.006018</td>
<td>-0.013063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.562500</td>
</tr>
<tr>
<td>104</td>
<td>0.000300</td>
<td>0.312688</td>
<td>0.467491</td>
<td>197.875000</td>
<td>0.008158</td>
<td>0.078312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>105</td>
<td>0.000100</td>
<td>0.719250</td>
<td>0.528978</td>
<td>191.187500</td>
<td>0.003097</td>
<td>0.078625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.140625</td>
<td>0.500000</td>
</tr>
<tr>
<td>106</td>
<td>0.000200</td>
<td>0.257531</td>
<td>0.501718</td>
<td>194.812500</td>
<td>0.005807</td>
<td>-0.008094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.078125</td>
<td>0.187500</td>
</tr>
<tr>
<td>107</td>
<td>0.000100</td>
<td>0.565281</td>
<td>0.570627</td>
<td>195.437500</td>
<td>0.003359</td>
<td>0.096531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>108</td>
<td>0.000300</td>
<td>0.266188</td>
<td>0.252022</td>
<td>199.750000</td>
<td>0.007125</td>
<td>0.109937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.125000</td>
</tr>
<tr>
<td>109</td>
<td>0.000500</td>
<td>1.591406</td>
<td>0.531403</td>
<td>168.875000</td>
<td>0.013054</td>
<td>0.028906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>110</td>
<td>0.000600</td>
<td>0.971031</td>
<td>0.906340</td>
<td>191.937500</td>
<td>0.014373</td>
<td>0.033531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.187500</td>
<td>0.750000</td>
</tr>
<tr>
<td>111</td>
<td>0.000500</td>
<td>1.763906</td>
<td>0.517783</td>
<td>176.218750</td>
<td>0.013554</td>
<td>-0.032969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>112</td>
<td>0.000900</td>
<td>1.143687</td>
<td>0.746264</td>
<td>179.843750</td>
<td>0.021685</td>
<td>-0.028187</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>113</td>
<td>0.000200</td>
<td>0.125000</td>
<td>0.000000</td>
<td>200.000000</td>
<td>0.005879</td>
<td>0.125000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>114</td>
<td>0.000200</td>
<td>0.344187</td>
<td>0.294369</td>
<td>197.312500</td>
<td>0.004148</td>
<td>0.109812</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>115</td>
<td>0.000500</td>
<td>1.335250</td>
<td>0.234957</td>
<td>180.812500</td>
<td>0.011432</td>
<td>0.007125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>116</td>
<td>0.000200</td>
<td>0.386500</td>
<td>0.490854</td>
<td>199.031250</td>
<td>0.004324</td>
<td>0.074000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>117</td>
<td>0.000200</td>
<td>0.340062</td>
<td>0.296831</td>
<td>199.250000</td>
<td>0.003828</td>
<td>0.105687</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.046875</td>
<td>0.187500</td>
</tr>
<tr>
<td>118</td>
<td>0.000400</td>
<td>0.111500</td>
<td>0.025591</td>
<td>195.562500</td>
<td>0.009099</td>
<td>0.111500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr>
<td>119</td>
<td>0.000300</td>
<td>1.739094</td>
<td>0.297500</td>
<td>181.562500</td>
<td>0.007558</td>
<td>0.020344</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.375000</td>
</tr>
<tr>
<td>120</td>
<td>0.000400</td>
<td>0.648094</td>
<td>0.737686</td>
<td>192.750000</td>
<td>0.009983</td>
<td>0.085594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.500000</td>
</tr>
<tr>
<td>121</td>
<td>0.000500</td>
<td>0.730281</td>
<td>0.009011</td>
<td>183.875000</td>
<td>0.012388</td>
<td>0.105281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.500000</td>
</tr>
<tr>
<td>122</td>
<td>0.000800</td>
<td>1.746000</td>
<td>0.608225</td>
<td>171.625000</td>
<td>0.018913</td>
<td>0.027250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.375000</td>
</tr>
<tr>
<td>123</td>
<td>0.000800</td>
<td>0.573875</td>
<td>0.511113</td>
<td>182.593750</td>
<td>0.018763</td>
<td>0.105125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.093750</td>
<td>0.375000</td>
</tr>
<tr>
<td>124</td>
<td>0.000700</td>
<td>1.552563</td>
<td>0.787023</td>
<td>177.656250</td>
<td>0.018103</td>
<td>-0.009937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>125</td>
<td>0.000300</td>
<td>0.728563</td>
<td>0.063017</td>
<td>183.437500</td>
<td>0.008366</td>
<td>0.103563</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.125000</td>
<td>0.500000</td>
</tr>
<tr>
<td>126</td>
<td>0.000900</td>
<td>1.069187</td>
<td>0.322141</td>
<td>183.375000</td>
<td>0.021549</td>
<td>0.053562</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>127</td>
<td>0.000500</td>
<td>0.805375</td>
<td>0.602364</td>
<td>192.937500</td>
<td>0.011782</td>
<td>0.024125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>128</td>
<td>0.000500</td>
<td>1.450062</td>
<td>0.329752</td>
<td>168.656250</td>
<td>0.012539</td>
<td>0.043813</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.281250</td>
<td>1.125000</td>
</tr>
<tr>
<td>129</td>
<td>0.000400</td>
<td>1.082375</td>
<td>0.341948</td>
<td>181.781250</td>
<td>0.009770</td>
<td>0.066750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>130</td>
<td>0.000900</td>
<td>1.377656</td>
<td>1.025102</td>
<td>184.500000</td>
<td>0.023494</td>
<td>-0.028594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.281250</td>
<td>1.125000</td>
</tr>
<tr>
<td>131</td>
<td>0.000900</td>
<td>1.928344</td>
<td>0.267554</td>
<td>164.218750</td>
<td>0.022489</td>
<td>-0.024781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>132</td>
<td>0.000600</td>
<td>0.415750</td>
<td>0.500535</td>
<td>197.250000</td>
<td>0.015222</td>
<td>0.103250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.062500</td>
<td>0.250000</td>
</tr>
<tr>
<td>133</td>
<td>0.000700</td>
<td>1.313406</td>
<td>0.271812</td>
<td>181.031250</td>
<td>0.017767</td>
<td>-0.014719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>134</td>
<td>0.000700</td>
<td>0.844031</td>
<td>0.658927</td>
<td>191.562500</td>
<td>0.016444</td>
<td>0.062781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>135</td>
<td>0.000400</td>
<td>0.786500</td>
<td>0.732629</td>
<td>191.125000</td>
<td>0.010595</td>
<td>0.083375</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.140625</td>
<td>0.562500</td>
</tr>
<tr>
<td>136</td>
<td>0.000500</td>
<td>1.339406</td>
<td>0.580219</td>
<td>181.437500</td>
<td>0.012606</td>
<td>0.011281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>137</td>
<td>0.000700</td>
<td>1.576375</td>
<td>0.874083</td>
<td>175.968750</td>
<td>0.018491</td>
<td>0.013875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>138</td>
<td>0.000800</td>
<td>1.126188</td>
<td>0.632414</td>
<td>187.500000</td>
<td>0.019553</td>
<td>-0.061312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.875000</td>
</tr>
<tr>
<td>139</td>
<td>0.000800</td>
<td>0.863250</td>
<td>0.250897</td>
<td>174.500000</td>
<td>0.020898</td>
<td>0.097625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.562500</td>
</tr>
<tr>
<td>140</td>
<td>0.001800</td>
<td>2.337156</td>
<td>0.369841</td>
<td>146.125000</td>
<td>0.045088</td>
<td>-0.006594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>141</td>
<td>0.001200</td>
<td>1.179875</td>
<td>0.290548</td>
<td>187.250000</td>
<td>0.029070</td>
<td>0.008000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>142</td>
<td>0.002000</td>
<td>1.860031</td>
<td>0.651452</td>
<td>162.750000</td>
<td>0.050185</td>
<td>0.063156</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>143</td>
<td>0.001000</td>
<td>1.639469</td>
<td>0.696820</td>
<td>176.625000</td>
<td>0.023879</td>
<td>-0.001156</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>1.312500</td>
</tr>
<tr>
<td>144</td>
<td>0.001500</td>
<td>1.432656</td>
<td>0.235467</td>
<td>166.750000</td>
<td>0.036621</td>
<td>0.104531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>145</td>
<td>0.001300</td>
<td>1.551031</td>
<td>0.895742</td>
<td>180.218750</td>
<td>0.033519</td>
<td>-0.011469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>146</td>
<td>0.001600</td>
<td>1.052594</td>
<td>0.525497</td>
<td>171.718750</td>
<td>0.038834</td>
<td>0.115094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.187500</td>
<td>0.750000</td>
</tr>
<tr>
<td>147</td>
<td>0.001400</td>
<td>1.371125</td>
<td>0.031297</td>
<td>163.500000</td>
<td>0.035136</td>
<td>0.121125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>148</td>
<td>0.000900</td>
<td>0.826969</td>
<td>0.321804</td>
<td>189.218750</td>
<td>0.023262</td>
<td>0.045719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>149</td>
<td>0.001100</td>
<td>1.325250</td>
<td>0.041487</td>
<td>170.343750</td>
<td>0.026935</td>
<td>0.075250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>150</td>
<td>0.001200</td>
<td>1.240656</td>
<td>0.263828</td>
<td>174.156250</td>
<td>0.030042</td>
<td>0.068781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>151</td>
<td>0.001500</td>
<td>2.068000</td>
<td>0.624614</td>
<td>152.937500</td>
<td>0.036548</td>
<td>0.036750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>152</td>
<td>0.001300</td>
<td>1.315906</td>
<td>0.287107</td>
<td>181.437500</td>
<td>0.033494</td>
<td>-0.012219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>153</td>
<td>0.001500</td>
<td>1.382656</td>
<td>0.633689</td>
<td>167.781250</td>
<td>0.036475</td>
<td>0.038906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.281250</td>
<td>1.062500</td>
</tr>
<tr>
<td>154</td>
<td>0.002600</td>
<td>1.688094</td>
<td>0.444427</td>
<td>141.062500</td>
<td>0.064864</td>
<td>0.125594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>155</td>
<td>0.001100</td>
<td>1.755594</td>
<td>0.661673</td>
<td>170.437500</td>
<td>0.026335</td>
<td>0.036844</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.375000</td>
</tr>
<tr>
<td>156</td>
<td>0.001600</td>
<td>0.981094</td>
<td>0.994267</td>
<td>187.531250</td>
<td>0.040151</td>
<td>0.027969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.750000</td>
</tr>
<tr>
<td>157</td>
<td>0.001000</td>
<td>0.839875</td>
<td>0.858498</td>
<td>192.156250</td>
<td>0.024021</td>
<td>0.058625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>158</td>
<td>0.001200</td>
<td>0.836500</td>
<td>0.551024</td>
<td>191.625000</td>
<td>0.030358</td>
<td>0.055250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.156250</td>
<td>0.625000</td>
</tr>
<tr>
<td>159</td>
<td>0.001800</td>
<td>1.874187</td>
<td>0.039922</td>
<td>175.656250</td>
<td>0.044303</td>
<td>-0.000813</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>160</td>
<td>0.002500</td>
<td>2.270750</td>
<td>0.319948</td>
<td>140.343750</td>
<td>0.061421</td>
<td>0.020750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>1.750000</td>
</tr>
<tr>
<td>161</td>
<td>0.002800</td>
<td>1.397687</td>
<td>0.815960</td>
<td>173.500000</td>
<td>0.071023</td>
<td>-0.024188</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.125000</td>
</tr>
<tr>
<td>162</td>
<td>0.001400</td>
<td>0.803875</td>
<td>0.447184</td>
<td>181.687500</td>
<td>0.034519</td>
<td>0.069500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.500000</td>
</tr>
<tr>
<td>163</td>
<td>0.003200</td>
<td>1.813406</td>
<td>0.414541</td>
<td>150.406250</td>
<td>0.080257</td>
<td>0.047781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.375000</td>
</tr>
<tr>
<td>164</td>
<td>0.001500</td>
<td>1.086625</td>
<td>0.491458</td>
<td>184.687500</td>
<td>0.038402</td>
<td>0.071000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.203125</td>
<td>0.812500</td>
</tr>
<tr>
<td>165</td>
<td>0.002000</td>
<td>1.061656</td>
<td>0.779787</td>
<td>162.093750</td>
<td>0.049305</td>
<td>-0.016469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>0.750000</td>
</tr>
<tr>
<td>166</td>
<td>0.001800</td>
<td>1.304062</td>
<td>0.048175</td>
<td>157.437500</td>
<td>0.044627</td>
<td>0.054063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>167</td>
<td>0.001800</td>
<td>2.096750</td>
<td>0.622596</td>
<td>152.500000</td>
<td>0.045056</td>
<td>0.065500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>168</td>
<td>0.002400</td>
<td>1.239969</td>
<td>0.684414</td>
<td>178.750000</td>
<td>0.059944</td>
<td>-0.010031</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.937500</td>
</tr>
<tr>
<td>169</td>
<td>0.002600</td>
<td>2.421156</td>
<td>0.275769</td>
<td>148.000000</td>
<td>0.065235</td>
<td>-0.000719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>170</td>
<td>0.002100</td>
<td>2.032656</td>
<td>0.336890</td>
<td>161.906250</td>
<td>0.051958</td>
<td>0.001406</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>171</td>
<td>0.001400</td>
<td>1.511063</td>
<td>0.325746</td>
<td>168.468750</td>
<td>0.035243</td>
<td>0.026688</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>172</td>
<td>0.003000</td>
<td>1.951531</td>
<td>0.045671</td>
<td>144.406250</td>
<td>0.074043</td>
<td>0.076531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>173</td>
<td>0.002100</td>
<td>1.740781</td>
<td>0.342934</td>
<td>171.343750</td>
<td>0.053517</td>
<td>0.006406</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.375000</td>
</tr>
<tr>
<td>174</td>
<td>0.003400</td>
<td>2.343219</td>
<td>0.266610</td>
<td>155.906250</td>
<td>0.083916</td>
<td>-0.031781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>1.875000</td>
</tr>
<tr>
<td>175</td>
<td>0.004300</td>
<td>1.486844</td>
<td>0.470988</td>
<td>148.437500</td>
<td>0.106308</td>
<td>0.033719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>1.125000</td>
</tr>
<tr>
<td>176</td>
<td>0.003300</td>
<td>1.989469</td>
<td>0.528197</td>
<td>153.250000</td>
<td>0.082154</td>
<td>0.036344</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>177</td>
<td>0.003300</td>
<td>1.864125</td>
<td>0.239849</td>
<td>158.500000</td>
<td>0.081617</td>
<td>0.067250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>178</td>
<td>0.002200</td>
<td>1.312813</td>
<td>0.538599</td>
<td>182.281250</td>
<td>0.054456</td>
<td>0.062812</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>179</td>
<td>0.003200</td>
<td>1.249313</td>
<td>0.273266</td>
<td>174.625000</td>
<td>0.080913</td>
<td>0.014937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>0.937500</td>
</tr>
<tr>
<td>180</td>
<td>0.003500</td>
<td>1.889438</td>
<td>0.607271</td>
<td>154.156250</td>
<td>0.086246</td>
<td>0.076938</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.437500</td>
</tr>
<tr>
<td>181</td>
<td>0.003100</td>
<td>1.522625</td>
<td>0.549491</td>
<td>161.156250</td>
<td>0.077974</td>
<td>0.038250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>182</td>
<td>0.002300</td>
<td>2.018625</td>
<td>0.838069</td>
<td>162.187500</td>
<td>0.056574</td>
<td>-0.043875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.625000</td>
</tr>
<tr>
<td>183</td>
<td>0.003300</td>
<td>1.952188</td>
<td>0.067501</td>
<td>151.656250</td>
<td>0.083050</td>
<td>0.077187</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>184</td>
<td>0.003200</td>
<td>1.915406</td>
<td>0.588369</td>
<td>146.625000</td>
<td>0.080916</td>
<td>0.118531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>185</td>
<td>0.003000</td>
<td>1.611500</td>
<td>0.397920</td>
<td>163.406250</td>
<td>0.074150</td>
<td>0.049000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>186</td>
<td>0.004800</td>
<td>1.944094</td>
<td>0.120428</td>
<td>142.812500</td>
<td>0.120602</td>
<td>0.069094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>187</td>
<td>0.004200</td>
<td>1.794625</td>
<td>0.322431</td>
<td>156.375000</td>
<td>0.104972</td>
<td>-0.002250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>188</td>
<td>0.003100</td>
<td>1.390000</td>
<td>0.233579</td>
<td>163.593750</td>
<td>0.078733</td>
<td>0.061875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.062500</td>
</tr>
<tr>
<td>189</td>
<td>0.004200</td>
<td>2.439969</td>
<td>0.312798</td>
<td>139.812500</td>
<td>0.105942</td>
<td>0.096219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>190</td>
<td>0.003300</td>
<td>2.159250</td>
<td>0.763765</td>
<td>165.250000</td>
<td>0.083290</td>
<td>-0.028250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.750000</td>
</tr>
<tr>
<td>191</td>
<td>0.006300</td>
<td>2.551125</td>
<td>0.054667</td>
<td>136.125000</td>
<td>0.157735</td>
<td>0.051125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>192</td>
<td>0.003800</td>
<td>2.033563</td>
<td>0.482005</td>
<td>146.250000</td>
<td>0.095691</td>
<td>0.064813</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.562500</td>
</tr>
<tr>
<td>193</td>
<td>0.003700</td>
<td>1.528813</td>
<td>0.343776</td>
<td>160.156250</td>
<td>0.093565</td>
<td>0.060063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.125000</td>
</tr>
<tr>
<td>194</td>
<td>0.003400</td>
<td>2.250125</td>
<td>0.619515</td>
<td>149.781250</td>
<td>0.084903</td>
<td>0.062625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.750000</td>
</tr>
<tr>
<td>195</td>
<td>0.003700</td>
<td>2.154594</td>
<td>0.531447</td>
<td>153.718750</td>
<td>0.091290</td>
<td>0.045219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.687500</td>
</tr>
<tr>
<td>196</td>
<td>0.003000</td>
<td>0.998969</td>
<td>0.549875</td>
<td>174.750000</td>
<td>0.075860</td>
<td>-0.001031</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.687500</td>
</tr>
<tr>
<td>197</td>
<td>0.004500</td>
<td>2.025719</td>
<td>0.533755</td>
<td>141.531250</td>
<td>0.112904</td>
<td>0.072594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>198</td>
<td>0.003200</td>
<td>2.014812</td>
<td>0.784184</td>
<td>164.968750</td>
<td>0.080925</td>
<td>0.061687</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>199</td>
<td>0.003800</td>
<td>2.015688</td>
<td>0.753808</td>
<td>157.281250</td>
<td>0.095336</td>
<td>0.000063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.562500</td>
</tr>
<tr>
<td>200</td>
<td>0.004700</td>
<td>2.582094</td>
<td>0.081627</td>
<td>134.937500</td>
<td>0.117375</td>
<td>0.082094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>201</td>
<td>0.005500</td>
<td>2.617125</td>
<td>0.052604</td>
<td>117.593750</td>
<td>0.137294</td>
<td>0.117125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>202</td>
<td>0.002900</td>
<td>1.538594</td>
<td>0.726194</td>
<td>175.000000</td>
<td>0.072030</td>
<td>0.054219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>203</td>
<td>0.005000</td>
<td>2.341625</td>
<td>0.230639</td>
<td>125.000000</td>
<td>0.125255</td>
<td>0.107250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.812500</td>
</tr>
<tr>
<td>204</td>
<td>0.003200</td>
<td>1.479344</td>
<td>0.693954</td>
<td>166.312500</td>
<td>0.080652</td>
<td>0.057469</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.125000</td>
</tr>
<tr>
<td>205</td>
<td>0.006100</td>
<td>1.911000</td>
<td>0.508582</td>
<td>138.000000</td>
<td>0.152110</td>
<td>0.082875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.375000</td>
</tr>
<tr>
<td>206</td>
<td>0.004200</td>
<td>2.191469</td>
<td>0.566582</td>
<td>151.781250</td>
<td>0.105272</td>
<td>0.003969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.750000</td>
</tr>
<tr>
<td>207</td>
<td>0.005800</td>
<td>2.579594</td>
<td>0.054079</td>
<td>129.500000</td>
<td>0.145311</td>
<td>0.079594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>208</td>
<td>0.005500</td>
<td>1.945406</td>
<td>0.051905</td>
<td>126.000000</td>
<td>0.136839</td>
<td>0.070406</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>209</td>
<td>0.005400</td>
<td>2.622750</td>
<td>0.034297</td>
<td>118.375000</td>
<td>0.133929</td>
<td>0.122750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>210</td>
<td>0.003500</td>
<td>1.786437</td>
<td>0.333091</td>
<td>154.718750</td>
<td>0.088586</td>
<td>0.067688</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.375000</td>
</tr>
<tr>
<td>211</td>
<td>0.005700</td>
<td>1.517031</td>
<td>0.872162</td>
<td>154.875000</td>
<td>0.141901</td>
<td>0.032656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>212</td>
<td>0.005000</td>
<td>2.036375</td>
<td>0.079070</td>
<td>128.437500</td>
<td>0.124827</td>
<td>0.114500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.500000</td>
</tr>
<tr>
<td>213</td>
<td>0.005600</td>
<td>2.483187</td>
<td>0.293028</td>
<td>133.343750</td>
<td>0.140705</td>
<td>0.061313</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>214</td>
<td>0.004900</td>
<td>1.648875</td>
<td>0.830751</td>
<td>166.312500</td>
<td>0.122645</td>
<td>0.008250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>1.312500</td>
</tr>
<tr>
<td>215</td>
<td>0.005400</td>
<td>2.076156</td>
<td>0.733892</td>
<td>132.250000</td>
<td>0.135490</td>
<td>0.123031</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>216</td>
<td>0.005000</td>
<td>2.444750</td>
<td>0.460210</td>
<td>127.906250</td>
<td>0.124910</td>
<td>0.101000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>217</td>
<td>0.003100</td>
<td>1.585750</td>
<td>0.326160</td>
<td>164.843750</td>
<td>0.077131</td>
<td>0.101375</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>218</td>
<td>0.004700</td>
<td>2.034937</td>
<td>0.044813</td>
<td>132.562500</td>
<td>0.118740</td>
<td>0.144312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.500000</td>
</tr>
<tr>
<td>219</td>
<td>0.003600</td>
<td>1.324625</td>
<td>0.808525</td>
<td>153.437500</td>
<td>0.090733</td>
<td>0.074625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>220</td>
<td>0.006800</td>
<td>2.000000</td>
<td>0.083942</td>
<td>141.843750</td>
<td>0.170198</td>
<td>0.078125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.500000</td>
</tr>
<tr>
<td>221</td>
<td>0.005100</td>
<td>2.008219</td>
<td>0.046265</td>
<td>142.531250</td>
<td>0.126283</td>
<td>0.133219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>222</td>
<td>0.005500</td>
<td>1.650156</td>
<td>0.364033</td>
<td>147.906250</td>
<td>0.136476</td>
<td>0.009531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>1.312500</td>
</tr>
<tr>
<td>223</td>
<td>0.005800</td>
<td>2.438000</td>
<td>0.249815</td>
<td>148.187500</td>
<td>0.144775</td>
<td>0.016125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>224</td>
<td>0.004900</td>
<td>2.217156</td>
<td>0.524031</td>
<td>153.281250</td>
<td>0.123049</td>
<td>0.014031</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.750000</td>
</tr>
<tr>
<td>225</td>
<td>0.004000</td>
<td>1.285625</td>
<td>0.539269</td>
<td>175.250000</td>
<td>0.099108</td>
<td>0.020000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.328125</td>
<td>0.937500</td>
</tr>
<tr>
<td>226</td>
<td>0.005500</td>
<td>1.902281</td>
<td>0.062341</td>
<td>145.000000</td>
<td>0.138005</td>
<td>0.027281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>227</td>
<td>0.005500</td>
<td>1.390406</td>
<td>0.427751</td>
<td>160.781250</td>
<td>0.137022</td>
<td>0.046656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.000000</td>
</tr>
<tr>
<td>228</td>
<td>0.007800</td>
<td>2.675313</td>
<td>0.040066</td>
<td>100.968750</td>
<td>0.195479</td>
<td>0.175312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>229</td>
<td>0.008500</td>
<td>2.631281</td>
<td>0.047803</td>
<td>111.593750</td>
<td>0.213051</td>
<td>0.131281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>230</td>
<td>0.005600</td>
<td>1.051094</td>
<td>0.523414</td>
<td>158.750000</td>
<td>0.139961</td>
<td>0.051094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>0.750000</td>
</tr>
<tr>
<td>231</td>
<td>0.006200</td>
<td>1.914906</td>
<td>0.275807</td>
<td>147.687500</td>
<td>0.155035</td>
<td>0.102406</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.437500</td>
</tr>
<tr>
<td>232</td>
<td>0.005300</td>
<td>2.501469</td>
<td>0.257705</td>
<td>137.781250</td>
<td>0.133709</td>
<td>0.079594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>233</td>
<td>0.005300</td>
<td>1.980094</td>
<td>0.051462</td>
<td>136.843750</td>
<td>0.132938</td>
<td>0.105094</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>234</td>
<td>0.004600</td>
<td>1.868750</td>
<td>0.237607</td>
<td>159.562500</td>
<td>0.114713</td>
<td>0.071875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.437500</td>
</tr>
<tr>
<td>235</td>
<td>0.004400</td>
<td>1.249719</td>
<td>0.252509</td>
<td>174.031250</td>
<td>0.108821</td>
<td>0.077844</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.937500</td>
</tr>
<tr>
<td>236</td>
<td>0.006200</td>
<td>2.620219</td>
<td>0.063403</td>
<td>124.593750</td>
<td>0.155129</td>
<td>0.120219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>237</td>
<td>0.003200</td>
<td>0.717625</td>
<td>0.305497</td>
<td>186.031250</td>
<td>0.079256</td>
<td>0.045750</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.234375</td>
<td>0.437500</td>
</tr>
<tr>
<td>238</td>
<td>0.004400</td>
<td>1.477250</td>
<td>0.779215</td>
<td>172.531250</td>
<td>0.110622</td>
<td>-0.007125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.359375</td>
<td>1.125000</td>
</tr>
<tr>
<td>239</td>
<td>0.005700</td>
<td>1.629469</td>
<td>0.800205</td>
<td>161.875000</td>
<td>0.143016</td>
<td>0.066969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>240</td>
<td>0.009200</td>
<td>2.291625</td>
<td>0.582222</td>
<td>120.093750</td>
<td>0.229014</td>
<td>0.104125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.750000</td>
</tr>
<tr>
<td>241</td>
<td>0.006600</td>
<td>1.529563</td>
<td>0.206794</td>
<td>124.281250</td>
<td>0.165024</td>
<td>0.092062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.062500</td>
</tr>
<tr>
<td>242</td>
<td>0.005900</td>
<td>2.274562</td>
<td>0.581583</td>
<td>137.093750</td>
<td>0.148296</td>
<td>0.087062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.437500</td>
<td>1.750000</td>
</tr>
<tr>
<td>243</td>
<td>0.005000</td>
<td>1.453344</td>
<td>0.231155</td>
<td>145.437500</td>
<td>0.124141</td>
<td>0.093969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>0.937500</td>
</tr>
<tr>
<td>244</td>
<td>0.010400</td>
<td>2.685938</td>
<td>0.028580</td>
<td>91.187500</td>
<td>0.259130</td>
<td>0.185937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>245</td>
<td>0.012700</td>
<td>2.246594</td>
<td>0.370442</td>
<td>123.343750</td>
<td>0.317108</td>
<td>0.137219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.687500</td>
</tr>
<tr>
<td>246</td>
<td>0.008800</td>
<td>1.797625</td>
<td>0.502293</td>
<td>148.468750</td>
<td>0.219323</td>
<td>0.078875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.343750</td>
<td>1.375000</td>
</tr>
<tr>
<td>247</td>
<td>0.006700</td>
<td>1.631813</td>
<td>0.388426</td>
<td>150.937500</td>
<td>0.166549</td>
<td>0.069312</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>248</td>
<td>0.006000</td>
<td>1.315937</td>
<td>0.041215</td>
<td>155.812500</td>
<td>0.150767</td>
<td>0.065937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>249</td>
<td>0.008400</td>
<td>2.571656</td>
<td>0.135375</td>
<td>120.875000</td>
<td>0.209356</td>
<td>0.071656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>250</td>
<td>0.008300</td>
<td>2.176437</td>
<td>0.514214</td>
<td>142.312500</td>
<td>0.208007</td>
<td>0.067062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.625000</td>
</tr>
<tr>
<td>251</td>
<td>0.006300</td>
<td>2.523594</td>
<td>0.289065</td>
<td>122.156250</td>
<td>0.157367</td>
<td>0.101719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>252</td>
<td>0.011300</td>
<td>2.660062</td>
<td>0.057394</td>
<td>104.250000</td>
<td>0.281868</td>
<td>0.160062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>253</td>
<td>0.006300</td>
<td>2.090219</td>
<td>0.728129</td>
<td>158.812500</td>
<td>0.157074</td>
<td>0.043344</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.625000</td>
</tr>
<tr>
<td>254</td>
<td>0.009700</td>
<td>1.952531</td>
<td>0.273928</td>
<td>100.968750</td>
<td>0.242104</td>
<td>0.202531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.375000</td>
</tr>
<tr>
<td>255</td>
<td>0.005300</td>
<td>1.614875</td>
<td>0.507088</td>
<td>149.437500</td>
<td>0.132968</td>
<td>0.130500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.296875</td>
<td>1.187500</td>
</tr>
<tr>
<td>256</td>
<td>0.004600</td>
<td>2.410469</td>
<td>0.470044</td>
<td>143.781250</td>
<td>0.114553</td>
<td>0.066719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>257</td>
<td>0.005600</td>
<td>1.957250</td>
<td>0.512706</td>
<td>160.656250</td>
<td>0.140188</td>
<td>0.004125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>258</td>
<td>0.012400</td>
<td>2.033531</td>
<td>0.028032</td>
<td>129.031250</td>
<td>0.310667</td>
<td>0.158531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>259</td>
<td>0.006900</td>
<td>2.486875</td>
<td>0.256204</td>
<td>143.375000</td>
<td>0.173558</td>
<td>0.065000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>260</td>
<td>0.006700</td>
<td>2.631969</td>
<td>0.049591</td>
<td>120.406250</td>
<td>0.168264</td>
<td>0.131969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>261</td>
<td>0.006300</td>
<td>2.456062</td>
<td>0.285178</td>
<td>145.406250</td>
<td>0.156538</td>
<td>0.034188</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>262</td>
<td>0.007500</td>
<td>1.681875</td>
<td>0.527960</td>
<td>130.531250</td>
<td>0.186929</td>
<td>0.119375</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.312500</td>
<td>1.250000</td>
</tr>
<tr>
<td>263</td>
<td>0.006200</td>
<td>2.121219</td>
<td>0.502026</td>
<td>153.093750</td>
<td>0.155859</td>
<td>0.011844</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.625000</td>
</tr>
<tr>
<td>264</td>
<td>0.004100</td>
<td>1.382219</td>
<td>0.089439</td>
<td>160.250000</td>
<td>0.102401</td>
<td>0.116594</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.000000</td>
</tr>
<tr>
<td>265</td>
<td>0.008300</td>
<td>2.093438</td>
<td>0.411968</td>
<td>124.750000</td>
<td>0.207887</td>
<td>0.109063</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.500000</td>
</tr>
<tr>
<td>266</td>
<td>0.008900</td>
<td>2.058906</td>
<td>0.034875</td>
<td>126.562500</td>
<td>0.222189</td>
<td>0.183906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.375000</td>
<td>1.500000</td>
</tr>
<tr>
<td>267</td>
<td>0.007400</td>
<td>2.608875</td>
<td>0.073909</td>
<td>120.781250</td>
<td>0.183775</td>
<td>0.108875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>268</td>
<td>0.006300</td>
<td>2.387469</td>
<td>0.332426</td>
<td>156.406250</td>
<td>0.157265</td>
<td>0.043719</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>269</td>
<td>0.006600</td>
<td>2.007313</td>
<td>0.126664</td>
<td>140.125000</td>
<td>0.165939</td>
<td>0.022938</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.500000</td>
</tr>
<tr>
<td>270</td>
<td>0.007900</td>
<td>2.372312</td>
<td>0.598380</td>
<td>122.281250</td>
<td>0.197001</td>
<td>0.106688</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.812500</td>
</tr>
<tr>
<td>271</td>
<td>0.008800</td>
<td>2.576531</td>
<td>0.264298</td>
<td>110.406250</td>
<td>0.219564</td>
<td>0.154656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>272</td>
<td>0.008000</td>
<td>2.631000</td>
<td>0.057099</td>
<td>114.875000</td>
<td>0.200479</td>
<td>0.131000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>273</td>
<td>0.005600</td>
<td>2.336750</td>
<td>0.450358</td>
<td>156.406250</td>
<td>0.140186</td>
<td>-0.007000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>274</td>
<td>0.007000</td>
<td>2.073625</td>
<td>0.652751</td>
<td>145.812500</td>
<td>0.173851</td>
<td>0.042375</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>275</td>
<td>0.005300</td>
<td>1.513906</td>
<td>0.523808</td>
<td>155.281250</td>
<td>0.131418</td>
<td>0.107656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.281250</td>
<td>1.125000</td>
</tr>
<tr>
<td>276</td>
<td>0.008500</td>
<td>2.319188</td>
<td>0.494408</td>
<td>145.781250</td>
<td>0.212100</td>
<td>0.053562</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.812500</td>
</tr>
<tr>
<td>277</td>
<td>0.008600</td>
<td>2.180406</td>
<td>0.577412</td>
<td>136.781250</td>
<td>0.214137</td>
<td>0.133531</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.625000</td>
</tr>
<tr>
<td>278</td>
<td>0.007500</td>
<td>2.448063</td>
<td>0.300101</td>
<td>110.593750</td>
<td>0.186933</td>
<td>0.166813</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.812500</td>
</tr>
<tr>
<td>279</td>
<td>0.008200</td>
<td>1.882812</td>
<td>0.649189</td>
<td>146.125000</td>
<td>0.204620</td>
<td>0.054688</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.437500</td>
</tr>
<tr>
<td>280</td>
<td>0.006000</td>
<td>2.545938</td>
<td>0.252563</td>
<td>128.875000</td>
<td>0.149440</td>
<td>0.124062</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>281</td>
<td>0.012900</td>
<td>2.379688</td>
<td>0.484250</td>
<td>131.000000</td>
<td>0.323048</td>
<td>0.098438</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.812500</td>
</tr>
<tr>
<td>282</td>
<td>0.011500</td>
<td>2.340500</td>
<td>0.344941</td>
<td>95.031250</td>
<td>0.288345</td>
<td>0.231125</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.687500</td>
</tr>
<tr>
<td>283</td>
<td>0.005700</td>
<td>2.508844</td>
<td>0.285010</td>
<td>134.562500</td>
<td>0.142858</td>
<td>0.086969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>284</td>
<td>0.007000</td>
<td>2.177875</td>
<td>0.643062</td>
<td>133.593750</td>
<td>0.175080</td>
<td>0.146625</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>285</td>
<td>0.005000</td>
<td>2.036187</td>
<td>0.838778</td>
<td>175.062500</td>
<td>0.125452</td>
<td>0.004938</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.406250</td>
<td>1.625000</td>
</tr>
<tr>
<td>286</td>
<td>0.008400</td>
<td>2.154094</td>
<td>0.272045</td>
<td>143.531250</td>
<td>0.210086</td>
<td>0.075969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.625000</td>
</tr>
<tr>
<td>287</td>
<td>0.008500</td>
<td>2.649312</td>
<td>0.055325</td>
<td>107.875000</td>
<td>0.213008</td>
<td>0.149313</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>288</td>
<td>0.006300</td>
<td>2.443750</td>
<td>0.499017</td>
<td>128.562500</td>
<td>0.156478</td>
<td>0.100000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>289</td>
<td>0.009200</td>
<td>2.385531</td>
<td>0.404821</td>
<td>114.000000</td>
<td>0.229171</td>
<td>0.119906</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.812500</td>
</tr>
<tr>
<td>290</td>
<td>0.006300</td>
<td>1.780125</td>
<td>0.517382</td>
<td>150.781250</td>
<td>0.157621</td>
<td>0.077000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.312500</td>
</tr>
<tr>
<td>291</td>
<td>0.003900</td>
<td>2.370719</td>
<td>0.300337</td>
<td>171.031250</td>
<td>0.098286</td>
<td>0.026969</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.468750</td>
<td>1.875000</td>
</tr>
<tr>
<td>292</td>
<td>0.004600</td>
<td>1.308406</td>
<td>0.068198</td>
<td>170.750000</td>
<td>0.115032</td>
<td>0.042781</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.265625</td>
<td>1.000000</td>
</tr>
<tr>
<td>293</td>
<td>0.007400</td>
<td>2.651500</td>
<td>0.043233</td>
<td>106.656250</td>
<td>0.184049</td>
<td>0.151500</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
<tr>
<td>294</td>
<td>0.010600</td>
<td>2.305781</td>
<td>0.370243</td>
<td>126.500000</td>
<td>0.266168</td>
<td>0.102656</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.750000</td>
</tr>
<tr>
<td>295</td>
<td>0.007600</td>
<td>2.330500</td>
<td>0.366948</td>
<td>144.093750</td>
<td>0.189974</td>
<td>0.064875</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.453125</td>
<td>1.812500</td>
</tr>
<tr>
<td>296</td>
<td>0.005900</td>
<td>1.667719</td>
<td>0.383545</td>
<td>149.000000</td>
<td>0.148483</td>
<td>0.058344</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.421875</td>
<td>1.187500</td>
</tr>
<tr>
<td>297</td>
<td>0.005600</td>
<td>2.016063</td>
<td>0.289297</td>
<td>155.687500</td>
<td>0.139351</td>
<td>0.062937</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.390625</td>
<td>1.562500</td>
</tr>
<tr>
<td>298</td>
<td>0.010600</td>
<td>2.625719</td>
<td>0.219611</td>
<td>94.312500</td>
<td>0.264362</td>
<td>0.188219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>1.937500</td>
</tr>
<tr>
<td>299</td>
<td>0.010600</td>
<td>2.524094</td>
<td>0.246387</td>
<td>128.343750</td>
<td>0.264156</td>
<td>0.102219</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.484375</td>
<td>1.937500</td>
</tr>
<tr>
<td>300</td>
<td>0.010200</td>
<td>2.690281</td>
<td>0.027626</td>
<td>101.812500</td>
<td>0.254426</td>
<td>0.190281</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.500000</td>
<td>2.000000</td>
</tr>
</tbody></table>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/unsloth/" rel="tag"># unsloth</a>
              <a href="/tags/grpo/" rel="tag"># grpo</a>
              <a href="/tags/phi-4/" rel="tag"># phi-4</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/04/08/llama-cpp%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%EF%BC%88convert-hf-to-gguf-py/" rel="prev" title="llama.cpp安装与简单使用（convert-hf-to-gguf.py)">
                  <i class="fa fa-angle-left"></i> llama.cpp安装与简单使用（convert-hf-to-gguf.py)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/04/11/gradio-cam%E6%90%AD%E5%BB%BAweb%E9%A1%B5%E9%9D%A2%E4%BD%93%E9%AA%8C/" rel="next" title="gradio+cam搭建web页面体验">
                  gradio+cam搭建web页面体验 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">411k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">24:55</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
