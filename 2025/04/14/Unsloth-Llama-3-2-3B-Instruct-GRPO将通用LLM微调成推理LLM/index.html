<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言前面我们使用Unsloth将Phi-4 14B通过GRPO方法转化成了推理LLM，但示例中的两个奖励函数有些问题，官方也并未有改正的意向，只是给了另一个Llama-3.2-3B的示例供开发者参考，那么本文就一起来看一下这个新示例吧，同时本文将深入的解析其中所使用的代码。">
<meta property="og:type" content="article">
<meta property="og:title" content="Unsloth:Llama-3.2-3B-Instruct+GRPO将通用LLM微调成推理LLM">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/04/14/Unsloth-Llama-3-2-3B-Instruct-GRPO%E5%B0%86%E9%80%9A%E7%94%A8LLM%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86LLM/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言前面我们使用Unsloth将Phi-4 14B通过GRPO方法转化成了推理LLM，但示例中的两个奖励函数有些问题，官方也并未有改正的意向，只是给了另一个Llama-3.2-3B的示例供开发者参考，那么本文就一起来看一下这个新示例吧，同时本文将深入的解析其中所使用的代码。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-14T07:41:20.000Z">
<meta property="article:modified_time" content="2025-04-28T07:07:25.327Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="unsloth">
<meta property="article:tag" content="llama-3.2">
<meta property="article:tag" content="grpo">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/04/14/Unsloth-Llama-3-2-3B-Instruct-GRPO%E5%B0%86%E9%80%9A%E7%94%A8LLM%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86LLM/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/04/14/Unsloth-Llama-3-2-3B-Instruct-GRPO%E5%B0%86%E9%80%9A%E7%94%A8LLM%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86LLM/","path":"2025/04/14/Unsloth-Llama-3-2-3B-Instruct-GRPO将通用LLM微调成推理LLM/","title":"Unsloth:Llama-3.2-3B-Instruct+GRPO将通用LLM微调成推理LLM"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Unsloth:Llama-3.2-3B-Instruct+GRPO将通用LLM微调成推理LLM | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E5%9D%97%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">2.1.</span> <span class="nav-text">第一块：模型加载与初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%9D%97%EF%BC%9A%E5%8A%A0%E8%BD%BDgsm8k%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.2.</span> <span class="nav-text">第二块：加载gsm8k数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E5%9D%97%EF%BC%9A%E6%89%93%E5%8D%B0%E7%AC%AC%E4%B8%80%E6%9D%A1%E6%95%B0%E6%8D%AE"><span class="nav-number">2.3.</span> <span class="nav-text">第三块：打印第一条数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E5%9D%97%EF%BC%9A%E6%8F%90%E5%8F%96%E7%AD%94%E6%A1%88"><span class="nav-number">2.4.</span> <span class="nav-text">第四块：提取答案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E7%B3%BB%E7%BB%9F%E6%8F%90%E7%A4%BA%E8%AF%8D"><span class="nav-number">2.5.</span> <span class="nav-text">第五块：定义系统提示词</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E5%9D%97%EF%BC%9A%E5%88%B6%E4%BD%9C%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.6.</span> <span class="nav-text">第六块：制作训练数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E5%9D%97%EF%BC%9A%E5%88%9B%E5%BB%BA%E5%8C%B9%E9%85%8D%E6%A8%A1%E6%9D%BF"><span class="nav-number">2.7.</span> <span class="nav-text">第七块：创建匹配模板</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E5%9D%97%EF%BC%9A%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E5%8C%B9%E9%85%8D%E6%A8%A1%E6%9D%BF"><span class="nav-number">2.8.</span> <span class="nav-text">第八块：简单使用匹配模板</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E7%B2%BE%E7%A1%AE%E6%A0%BC%E5%BC%8F%E5%8C%B9%E9%85%8D%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.9.</span> <span class="nav-text">第九块：定义精确格式匹配奖励函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%AE%BD%E6%9D%BE%E6%A0%BC%E5%BC%8F%E5%8C%B9%E9%85%8D%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.10.</span> <span class="nav-text">第十块：定义宽松格式匹配奖励函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E7%AD%94%E6%A1%88%E6%A3%80%E6%9F%A5%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.11.</span> <span class="nav-text">第十一块：定义答案检查奖励函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E6%95%B0%E5%AD%97%E5%8C%B9%E9%85%8D%E6%A8%A1%E6%9D%BF"><span class="nav-number">2.12.</span> <span class="nav-text">第十二块：定义数字匹配模板</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E7%AD%94%E6%A1%88%E4%B8%AD%E6%95%B0%E5%AD%97%E6%A3%80%E6%9F%A5%E5%A5%96%E5%8A%B1%E5%87%BD%E6%95%B0"><span class="nav-number">2.13.</span> <span class="nav-text">第十三块：定义答案中数字检查奖励函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E5%9D%97%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%80%E5%A4%A7prompt%E9%95%BF%E5%BA%A6"><span class="nav-number">2.14.</span> <span class="nav-text">第十四块：计算最大prompt长度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%94%E5%9D%97%EF%BC%9A%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0"><span class="nav-number">2.15.</span> <span class="nav-text">第十五块：定义训练参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AD%E5%9D%97%EF%BC%9A%E5%88%9B%E5%BB%BAGRPO%E8%AE%AD%E7%BB%83%E5%99%A8"><span class="nav-number">2.16.</span> <span class="nav-text">第十六块：创建GRPO训练器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%83%E5%9D%97%EF%BC%9A%E5%8E%9F%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="nav-number">2.17.</span> <span class="nav-text">第十七块：原模型推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%85%AB%E5%9D%97%EF%BC%9Alora%E6%9D%83%E9%87%8D%E4%BF%9D%E5%AD%98%E5%92%8C%E9%AA%8C%E8%AF%81"><span class="nav-number">2.18.</span> <span class="nav-text">第十八块：lora权重保存和验证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B9%9D%E5%9D%97%EF%BC%9A%E5%BE%AE%E8%B0%83%E5%90%8E%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86"><span class="nav-number">2.19.</span> <span class="nav-text">第十九块：微调后模型推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E5%8D%81%E5%9D%97%EF%BC%9A%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%EF%BC%88vLLM%EF%BC%89"><span class="nav-number">2.20.</span> <span class="nav-text">第二十块：保存模型（vLLM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E5%85%851%EF%BC%9A%E5%8E%9F%E6%A8%A1%E5%9E%8B-%E7%B3%BB%E7%BB%9F%E6%8F%90%E7%A4%BA%E8%AF%8D%E6%8E%A8%E7%90%86"><span class="nav-number">2.21.</span> <span class="nav-text">补充1：原模型+系统提示词推理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E5%85%852%EF%BC%9A9-11%E5%92%8C9-9%E8%B0%81%E5%A4%A7"><span class="nav-number">2.22.</span> <span class="nav-text">补充2：9.11和9.9谁大</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">3.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GSM8K"><span class="nav-number">3.1.</span> <span class="nav-text">GSM8K</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%99%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">3.2.</span> <span class="nav-text">计算器机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%97%A5%E5%BF%97"><span class="nav-number">3.3.</span> <span class="nav-text">训练日志</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">71</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">91</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/04/14/Unsloth-Llama-3-2-3B-Instruct-GRPO%E5%B0%86%E9%80%9A%E7%94%A8LLM%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86LLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Unsloth:Llama-3.2-3B-Instruct+GRPO将通用LLM微调成推理LLM | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Unsloth:Llama-3.2-3B-Instruct+GRPO将通用LLM微调成推理LLM
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-04-14 15:41:20" itemprop="dateCreated datePublished" datetime="2025-04-14T15:41:20+08:00">2025-04-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-04-28 15:07:25" itemprop="dateModified" datetime="2025-04-28T15:07:25+08:00">2025-04-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/code/" itemprop="url" rel="index"><span itemprop="name">code</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:06</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><a href="https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/">前面</a>我们使用<code>Unsloth</code>将<code>Phi-4 14B</code>通过<code>GRPO</code>方法转化成了推理<code>LLM</code>，但示例中的两个奖励函数有些问题，官方也并未有改正的意向，只是给了另一个<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_1_(3B)_GRPO_LoRA.ipynb#scrollTo=vzOuSVCL_GA9">Llama-3.2-3B</a>的示例供开发者参考，那么本文就一起来看一下这个新示例吧，同时本文将深入的解析其中所使用的代码。</p>
<span id="more"></span>

<hr>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="第一块：模型加载与初始化"><a href="#第一块：模型加载与初始化" class="headerlink" title="第一块：模型加载与初始化"></a><strong>第一块：模型加载与初始化</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">max_seq_length = <span class="number">2048</span> <span class="comment"># Can increase for longer reasoning traces</span></span><br><span class="line">lora_rank = <span class="number">64</span> <span class="comment"># Larger rank = smarter, but slower</span></span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;meta-llama/Llama-3.2-3B-Instruct&quot;</span>,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    load_in_4bit = <span class="literal">False</span>, <span class="comment"># False for LoRA 16bit</span></span><br><span class="line">    fast_inference = <span class="literal">True</span>, <span class="comment"># Enable vLLM fast inference</span></span><br><span class="line">    max_lora_rank = lora_rank,</span><br><span class="line">    gpu_memory_utilization = <span class="number">0.8</span>, <span class="comment"># Reduce if out of memory</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = lora_rank, <span class="comment"># Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128</span></span><br><span class="line">    target_modules = [</span><br><span class="line">        <span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>,</span><br><span class="line">        <span class="string">&quot;gate_proj&quot;</span>, <span class="string">&quot;up_proj&quot;</span>, <span class="string">&quot;down_proj&quot;</span>,</span><br><span class="line">    ], <span class="comment"># Remove QKVO if out of memory</span></span><br><span class="line">    lora_alpha = lora_rank,</span><br><span class="line">    use_gradient_checkpointing = <span class="string">&quot;unsloth&quot;</span>, <span class="comment"># Enable long context finetuning</span></span><br><span class="line">    random_state = <span class="number">3407</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>与之前文章相似，但：</p>
<ul>
<li>使用的不是<code>4bit</code>量化版本（也许因为使用的是<code>3B</code>模型）</li>
<li><code>lora_rank</code>使用的是64，之前是16，代表微调的权重会比之前多一些</li>
</ul>
<h3 id="第二块：加载gsm8k数据集"><a href="#第二块：加载gsm8k数据集" class="headerlink" title="第二块：加载gsm8k数据集"></a><strong>第二块：加载gsm8k数据集</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&quot;openai/gsm8k&quot;</span>, <span class="string">&quot;main&quot;</span>, split = <span class="string">&quot;train&quot;</span>)</span><br><span class="line">dataset</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [&#x27;question&#x27;, &#x27;answer&#x27;],</span><br><span class="line">    num<span class="built_in">_</span>rows: 7473</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>关于<code>GSM8k</code>数据集的详细内容见附录。</p>
</blockquote>
<p>这里使用<code>openai/gsm8k</code>的<code>main</code>分支的<code>train</code>部分，其包含<code>7473</code>对问答对（QA, question-answer）。</p>
<h3 id="第三块：打印第一条数据"><a href="#第三块：打印第一条数据" class="headerlink" title="第三块：打印第一条数据"></a><strong>第三块：打印第一条数据</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset[<span class="number">0</span>][<span class="string">&quot;question&quot;</span>]</span><br><span class="line">dataset[<span class="number">0</span>][<span class="string">&quot;answer&quot;</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?</span><br><span class="line"></span><br><span class="line">Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.</span><br><span class="line">Natalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.</span><br><span class="line"><span class="params">####</span> 72</span><br></pre></td></tr></table></figure>

<p>这里分别打印数据集第一条的<code>question</code>和<code>answer</code>，值得注意的是：</p>
<ul>
<li><code>answer</code>先是思考过程，结果写在<code>####</code>后面，且是纯数字</li>
<li><code>&lt;&lt;48/2=24&gt;&gt;</code>用于指示计算器调用</li>
</ul>
<h3 id="第四块：提取答案"><a href="#第四块：提取答案" class="headerlink" title="第四块：提取答案"></a><strong>第四块：提取答案</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_hash_answer</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;####&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> text: <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> text.split(<span class="string">&quot;####&quot;</span>)[<span class="number">1</span>].strip()</span><br><span class="line">extract_hash_answer(dataset[<span class="number">0</span>][<span class="string">&quot;answer&quot;</span>])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;72&#x27;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里的疑惑点是函数名中的<code>hash</code>，其实这个<code>hash</code>指的是<code>hash symbol</code>，即<code>#</code>。而不是文件完整性校验的那个哈希。</p>
</blockquote>
<p>用于从<code>gsm8k</code>数据集的<code>answer</code>字段中提取最终答案，即<code>####</code>后方的内容。<code>.split</code>用于将文本内容按切割方式（这里是<code>####</code>切割成两部分），这里取第二部分，最后去掉首尾的空格。</p>
<h3 id="第五块：定义系统提示词"><a href="#第五块：定义系统提示词" class="headerlink" title="第五块：定义系统提示词"></a><strong>第五块：定义系统提示词</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">reasoning_start = <span class="string">&quot;&lt;start_working_out&gt;&quot;</span></span><br><span class="line">reasoning_end   = <span class="string">&quot;&lt;end_working_out&gt;&quot;</span></span><br><span class="line">solution_start = <span class="string">&quot;&lt;SOLUTION&gt;&quot;</span></span><br><span class="line">solution_end = <span class="string">&quot;&lt;/SOLUTION&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">system_prompt = \</span><br><span class="line"><span class="string">f&quot;&quot;&quot;You are given a problem.</span></span><br><span class="line"><span class="string">Think about the problem and provide your working out.</span></span><br><span class="line"><span class="string">Place it between <span class="subst">&#123;reasoning_start&#125;</span> and <span class="subst">&#123;reasoning_end&#125;</span>.</span></span><br><span class="line"><span class="string">Then, provide your solution between <span class="subst">&#123;solution_start&#125;</span><span class="subst">&#123;solution_end&#125;</span>&quot;&quot;&quot;</span></span><br><span class="line">system_prompt</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">You are given a problem.</span><br><span class="line">Think about the problem and provide your working out.</span><br><span class="line">Place it between &lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt; and &lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;.</span><br><span class="line">Then, provide your solution between &lt;SOLUTION&gt;&lt;/SOLUTION&gt;</span><br></pre></td></tr></table></figure>

<p>这里首先定义推理（reasoning）和结果（solution）的开始和结束标志，然后用在系统提示词（system_prompt）中。</p>
<h3 id="第六块：制作训练数据集"><a href="#第六块：制作训练数据集" class="headerlink" title="第六块：制作训练数据集"></a><strong>第六块：制作训练数据集</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataset = dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123;</span><br><span class="line">    <span class="string">&quot;prompt&quot;</span> : [</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_prompt&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,   <span class="string">&quot;content&quot;</span>: x[<span class="string">&quot;question&quot;</span>]&#125;,</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">&quot;answer&quot;</span>: extract_hash_answer(x[<span class="string">&quot;answer&quot;</span>]),</span><br><span class="line">&#125;)</span><br><span class="line">dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;question&#x27;: &#x27;Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?&#x27;,</span><br><span class="line"> &#x27;answer&#x27;: &#x27;72&#x27;,</span><br><span class="line"> &#x27;prompt&#x27;: [&#123;&#x27;content&#x27;: &#x27;You are given a problem.<span class="keyword">\nThink</span> about the problem and provide your working out.<span class="keyword">\nPlace</span> it between &lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt; and &lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;.<span class="keyword">\nThen</span>, provide your solution between &lt;SOLUTION&gt;&lt;/SOLUTION&gt;&#x27;,</span><br><span class="line">   &#x27;role&#x27;: &#x27;system&#x27;&#125;,</span><br><span class="line">  &#123;&#x27;content&#x27;: &#x27;Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?&#x27;,</span><br><span class="line">   &#x27;role&#x27;: &#x27;user&#x27;&#125;]&#125;</span><br></pre></td></tr></table></figure>

<p>我们一起来看一下：</p>
<ol>
<li><code>map</code>函数遍历原始 <code>dataset</code> 中的每一条数据（我们称之为 <code>x</code>）。</li>
<li>对于每一条数据 <code>x</code>，都根据 <code>lambda</code> 函数的规则创建一个<strong>新的</strong>字典。</li>
<li>这个新的字典包含两个键：<ul>
<li><code>&quot;prompt&quot;</code>：其值是一个列表，包含了固定的系统提示和一个从 x 中提取的用户问题，构造成对话格式。</li>
<li><code>&quot;answer&quot;</code>：其值是使用 <code>extract_hash_answer</code> 函数从 <code>x</code> 的原始答案中提取出来的、<code>####</code> 标记后的最终答案。</li>
</ul>
</li>
<li><code>.map()</code> 方法收集所有这些新创建的字典，组成一个新的数据集。</li>
<li>最后，<code>dataset = ...</code> 将这个全新的、转换过的数据集<strong>合并</strong>回 <code>dataset</code> 变量，如果返回的字典中有<strong>与原来数据项相同的键</strong>（比如 <code>&quot;answer&quot;</code>），那么原来数据项中该键的值会被函数返回的值<strong>覆盖（overwrite）</strong>。原来数据项中<strong>没有被返回字典的键覆盖</strong>的其他键（比如 <code>&quot;question&quot;</code>），会<strong>保持不变</strong>。</li>
</ol>
<hr>
<h3 id="第七块：创建匹配模板"><a href="#第七块：创建匹配模板" class="headerlink" title="第七块：创建匹配模板"></a><strong>第七块：创建匹配模板</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">match_format = re.<span class="built_in">compile</span>(</span><br><span class="line">    <span class="string">rf&quot;^[\s]&#123;&#123;0,&#125;&#125;&quot;</span>\</span><br><span class="line">    <span class="string">rf&quot;<span class="subst">&#123;reasoning_start&#125;</span>.+?<span class="subst">&#123;reasoning_end&#125;</span>.*?&quot;</span>\</span><br><span class="line">    <span class="string">rf&quot;<span class="subst">&#123;solution_start&#125;</span>(.+?)<span class="subst">&#123;solution_end&#125;</span>&quot;</span>\</span><br><span class="line">    <span class="string">rf&quot;[\s]&#123;&#123;0,&#125;&#125;$&quot;</span>,</span><br><span class="line">    flags = re.MULTILINE | re.DOTALL</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这段代码创建一个“<strong>搜索模板</strong>”（也就是正则表达式），用来模板匹配。</p>
<p><code>match_format = re.compile(...)</code></p>
<ul>
<li><code>re.compile(...):</code> 这是 <code>re</code> 工具箱里的一个函数，叫做 <code>compile</code>（编译）。</li>
<li><strong>作用</strong>：它接收一个定义好的“搜索模板”（就是括号里那一长串东西），然后把它**预先处理（编译）**一下，变成一个更高效的“模式对象”。</li>
</ul>
<p><strong><code>rf&quot;...&quot;</code> 多行字符串</strong></p>
<ul>
<li><code>r</code> 前缀：表示这是一个“<strong>原始字符串</strong> (raw string)”。在原始字符串里，反斜杠 <code>\</code> 就只是一个普通的反斜杠，不会被 <code>Python</code> 误解成特殊转义字符（比如 <code>\n</code> 代表换行）。这在写正则表达式时特别有用，因为正则表达式本身就大量使用 <code>\</code>。</li>
<li><code>f</code> 前缀：表示这是一个“<strong>格式化字符串</strong> (<code>f-string</code>)”。它允许你在字符串里面用大括号 <code>&#123;&#125;</code> 包裹变量名，<code>Python</code> 会自动把变量的值填进去。比如 {<code>reasoning_start</code>}。</li>
<li><code>rf&quot;...&quot;</code>: 两者结合，意味着这是一个<strong>原始的格式化字符串</strong>。它既能方便地处理反斜杠，又能方便地嵌入变量。</li>
<li>多行写法：用 <code>\</code> 连接多行，只是为了代码看起来更整洁，实际上它们组成了一个<strong>完整</strong>的字符串。</li>
</ul>
<p><strong>正则表达式模板本身（括号里的核心内容）</strong><br>这部分是最复杂的，我们把它拆开看，假设变量 <code>reasoning_start</code> 是 &lt;开始思考&gt;，<code>reasoning_end</code> 是 &lt;结束思考&gt;，<code>solution_start</code> 是 &lt;最终答案&gt;，<code>solution_end</code> 是 &lt;结束答案&gt;。</p>
<ul>
<li><code>^</code>:<ul>
<li><strong>含义</strong>：匹配<strong>字符串的开头</strong>，或者在 <code>re.MULTILINE</code> 模式下匹配<strong>行的开头</strong>。</li>
<li><strong>解释</strong>：规定要查找的模式必须从文本的开头（或者一行的开头）就开始匹配。</li>
</ul>
</li>
<li><code>[\s]&#123;0,&#125;</code>:<ul>
<li><code>\s</code>: 匹配任何<strong>空白字符</strong>（空格、制表符 <code>\t</code>、换行符 <code>\n</code> 等）。</li>
<li><code>[...]</code>: 表示匹配方括号内<strong>任意一个</strong>字符。这里 <code>[\s]</code> 其实就等同于 <code>\s</code>。</li>
<li><code>&#123;0,&#125;</code>: 表示前面的部分（<code>\s</code>）可以出现 <strong>0 次或任意多次</strong>。</li>
<li><strong>解释</strong>：允许在文本开头（或行开头）有零个或多个空白字符。这让模式更灵活，即使文本开头有缩进或空行也能匹配上。</li>
</ul>
</li>
<li><code>&#123;reasoning_start&#125;</code>:<ul>
<li><strong>含义</strong>：这里会被 <code>f-string</code> 替换成变量 <code>reasoning_start</code> 的实际值，比如 &lt;开始思考&gt;。</li>
<li><strong>解释</strong>：查找那个表示“思考过程开始”的标记。</li>
</ul>
</li>
<li><code>.+?</code>:<ul>
<li><code>.</code>: 匹配<strong>除了换行符以外的任何单个字符</strong>。但是因为后面我们用了 <code>re.DOTALL</code> 标志，这里的<code>.</code> <strong>也能匹配换行符</strong>。</li>
<li><code>+</code>: 表示前面的部分（<code>.</code>）必须出现 <strong>1 次或多次</strong>。</li>
<li><code>?</code>: 跟在 <code>+</code> 或 <code>*</code> 后面时，表示<strong>非贪婪匹配</strong>。</li>
<li><strong>解释</strong>：匹配从 &lt;开始思考&gt; 之后开始的、<strong>任意数量（至少一个）的任何字符</strong>（包括换行符），直到<strong>第一次</strong>遇到后面跟着的 &lt;结束思考&gt; 为止。“非贪婪”很重要，它确保只匹配到第一个 &lt;结束思考&gt; 就停下，而不是一直匹配到文章中最后一个 &lt;结束思考&gt;。</li>
</ul>
</li>
<li><code>&#123;reasoning_end&#125;</code>:<ul>
<li><strong>含义</strong>：被替换成变量 <code>reasoning_end</code> 的值，比如 &lt;结束思考&gt;。</li>
<li><strong>解释</strong>：找到那个表示“思考过程结束”的标记。</li>
</ul>
</li>
<li><code>.*?</code>:<ul>
<li><code>.</code>: 同样，匹配包括换行符在内的任何字符。</li>
<li><code>*</code>: 表示前面的部分（.）可以出现 <strong>0 次或多次</strong>。</li>
<li><code>?</code>: 同样，表示<strong>非贪婪匹配</strong>。</li>
<li><strong>解释</strong>：匹配 &lt;结束思考&gt; 和 &lt;最终答案&gt; 之间可能存在的<strong>任何字符（包括没有字符，或者跨越多行的字符）</strong>。非贪婪匹配确保它只匹配最短的可能部分。</li>
</ul>
</li>
<li><code>&#123;solution_start&#125;</code>:<ul>
<li><strong>含义</strong>：被替换成变量 <code>solution_start</code> 的值，比如 &lt;最终答案&gt;。</li>
<li><strong>解释</strong>：找到那个表示“最终答案开始”的标记。</li>
</ul>
</li>
<li><code>(.+?)</code>: <strong>（这是最关键的部分之一！）</strong><ul>
<li><code>(...)</code>: 圆括号表示一个“<strong>捕获组</strong> (<code>capturing group</code>)”。意思是，当整个模式匹配成功时，我特别<strong>想要提取出</strong>圆括号里面匹配到的这部分内容。</li>
<li><code>.+?</code>: 和前面一样，非贪婪地匹配至少一个任意字符（包括换行符）。</li>
<li><strong>解释</strong>：匹配 &lt;最终答案&gt; 之后，到第一个 &lt;结束答案&gt; 之前的所有内容，并且<strong>把这部分内容“捕获”起来</strong>，方便我们之后取出来。这就是我们要提取的“最终答案”本身！</li>
</ul>
</li>
<li><code>&#123;solution_end&#125;</code>:<ul>
<li><strong>含义</strong>：被替换成变量 <code>solution_end</code> 的值，比如 &lt;结束答案&gt;。</li>
<li><strong>解释</strong>：找到那个表示“最终答案结束”的标记。</li>
</ul>
</li>
<li><code>[\s]&#123;0,&#125;</code>:<ul>
<li><strong>解释</strong>：允许在 &lt;结束答案&gt; 标记后面有零个或多个空白字符。</li>
</ul>
</li>
<li><code>$</code>:<ul>
<li><strong>含义</strong>：匹配<strong>字符串的结尾</strong>，或者在 <code>re.MULTILINE</code> 模式下匹配<strong>行的结尾</strong>。</li>
<li><strong>解释</strong>：规定匹配必须一直持续到文本的末尾（或一行的末尾）。</li>
</ul>
</li>
</ul>
<p><strong><code>flags = re.MULTILINE | re.DOTALL</code></strong></p>
<ul>
<li><code>flags</code>: 这是 <code>re.compile</code> 函数的一个参数，用来改变正则表达式的行为模式。</li>
<li><code>re.MULTILINE </code>(简写 <code>re.M</code>):<ul>
<li><strong>作用</strong>：让 <code>^</code> 能匹配每一行的开头（不仅仅是整个字符串的开头），让 <code>$</code> 能匹配每一行的结尾（不仅仅是整个字符串的结尾）。</li>
<li><strong>解释</strong>：如果你的文本有多行，这个标志允许模式在每一行的开头和结尾进行匹配检查，而不是只看整个文本的最开头和最末尾。</li>
</ul>
</li>
<li><code>re.DOTALL</code> (简写 <code>re.S</code>):<ul>
<li><strong>作用</strong>：让特殊字符 . <strong>能够匹配包括换行符在内</strong>的任何字符。默认情况下，<code>.</code> 是不匹配换行符的。</li>
<li><strong>解释</strong>：因为我们的“思考过程”和“最终答案”很可能包含多行文字，我们需要 <code>.</code> 能够跨越换行符去匹配。这个标志就是干这个的。</li>
</ul>
</li>
<li><code>|</code>: 这是按位或运算符。在这里用来<strong>组合</strong>多个标志。<code>re.MULTILINE | re.DOTALL</code> 表示同时启用 <code>MULTILINE</code> 和 <code>DOTALL</code> 这两个行为模式。</li>
</ul>
<p><strong>总结一下:</strong></p>
<p>这整段代码定义并编译了一个强大的“<strong>搜索模板</strong>” (match_format)。这个模板被设计用来：</p>
<ol>
<li>在一大段可能跨越多行的文本中进行查找。</li>
<li>查找一个特定的结构：必须以可选的空白开头，然后是“思考过程”标记，接着是思考内容，然后是“思考过程结束”标记，之后可能有一些其他文字，然后是“最终答案开始”标记，接着是<strong>我们想要提取的答案内容</strong>，最后是“最终答案结束”标记，并以可选的空白结尾。</li>
<li>由于设置了 <code>MULTILINE</code> 和 <code>DOTALL</code> 标志，这个模板可以正确处理跨越多行的内容。</li>
<li>最重要的是，它使用<strong>捕获组 (.+?)</strong> 来特别标记出“最终答案”部分，以便在匹配成功后能方便地<strong>提取</strong>出来。</li>
</ol>
<p>编译好之后，你就可以用 <code>match_format.search(你的文本)</code> 或 <code>match_format.match(你的文本)</code> 来实际执行查找，并用 <code>.group(1)</code> 来获取那个被捕获的答案内容了。</p>
<p><strong>这里还有几点补充一下：</strong></p>
<ol>
<li><p>如果把正则表达式模板写在一行是什么样子的（只需要把<code>\</code>和换成去掉即可）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">match_format = re.<span class="built_in">compile</span>(<span class="string">rf&quot;^[\s]&#123;&#123;0,&#125;&#125;<span class="subst">&#123;reasoning_start&#125;</span>.+?<span class="subst">&#123;reasoning_end&#125;</span>.*?<span class="subst">&#123;solution_start&#125;</span>(.+?)<span class="subst">&#123;solution_end&#125;</span>[\s]&#123;&#123;0,&#125;&#125;$&quot;</span>, flags=re.MULTILINE | re.DOTALL)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>[\s]&#123;&#123;0,&#125;&#125;</code> 为什么 <code>0</code> 后面有个逗号且被两个花括号包着？</p>
<p>这个问题包含两个关键点：双花括号<code> &#123;&#123;...&#125;&#125;</code> 和里面的 <code>&#123;0,&#125;</code>。</p>
<ul>
<li><p><strong>双花括号 </strong>：</p>
<ul>
<li><strong>原因</strong>：因为整个字符串是 <code>rf&quot;...&quot;</code> 格式，也就是一个“<strong>原始格式化字符串 (raw f-string)</strong>”。在 f-string 中，单个花括号 {} 是有特殊含义的，用来<strong>嵌入变量</strong>（比如之前的 {reasoning_start}）。</li>
<li><strong>作用</strong>：如果你想在最终的字符串里<strong>得到一个真正的、字面意义上的花括号</strong> <code>&#123;</code> 或 <code>&#125;</code>，而不是想嵌入变量，你就必须<strong>连续写两次</strong>：<code>&#123;&#123;` 代表一个 `&#123;`, `&#125;&#125;</code> 代表一个 <code>&#125;</code>。</li>
<li><strong>小结</strong>：这里的 <code>&#123;&#123;` 和 `&#125;&#125;</code> 是 <code>f-string</code> 的语法要求，目的是在最终生成的正则表达式字符串里得到 <code>&#123;0,&#125;</code> 这几个字符本身。</li>
</ul>
</li>
<li><p><strong>里面的 {0,}</strong>：</p>
<ul>
<li><strong>含义</strong>：这是<strong>正则表达式</strong>本身的语法，叫做“<strong>量词 (quantifier)</strong>”。它用来指定它前面的那个部分（在这里是 <code>[\s]</code>，代表一个空白字符）可以出现多少次。</li>
<li><code>&#123;n,m&#125;</code>：表示匹配前面的元素至少 <code>n</code> 次，最多 <code>m</code> 次。</li>
<li><code>&#123;n&#125;</code>：表示精确匹配 <code>n</code> 次。</li>
<li><code>&#123;n,&#125;</code>：表示匹配至少 <code>n</code> 次（<code>n</code> 次或更多次）。</li>
<li><strong>{0,}</strong>：因此，这表示匹配前面的元素<strong>至少 0 次</strong>，也就是 <strong>0 次或任意多次</strong>。</li>
<li><strong>等价写法</strong>：<code>&#123;0,&#125;</code> 的作用和另一个常见的量词 <code>*</code>（星号）是<strong>完全一样</strong>的，都表示“零次或多次”。所以 <code>[\s]&#123;0,&#125;</code> 和 <code>\s*</code> 或 <code>[\s]*</code> 表达的意思是相同的。为什么原作者用 <code>&#123;0,&#125;</code> 而不是 <code>*</code> 可能是个人编码风格的选择。</li>
</ul>
</li>
<li><p><strong><code>[\s]&#123;&#123;0,&#125;&#125;</code></strong></p>
<ul>
<li><p>外层的 <code>&#123;&#123;` 和 `&#125;&#125;</code> 是为了在 <code>f-string</code> 中输出真正的 <code>&#123;</code> 和 <code>&#125;</code>。</p>
</li>
<li><p>内层的 <code>&#123;0,&#125;</code> 是正则表达式语法，表示它前面的 <code>[\s]</code>（空白字符）可以出现 0 次或任意多次。</p>
</li>
<li><p>整体含义：匹配零个或多个连续的空白字符。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>re.MULTILINE</code> 标志对 <code>^</code> 和 <code>$</code> 作用的例子对比</p>
<p><code>re.MULTILINE</code> (或 <code>re.M</code>) 改变了 <code>^</code> 和 <code>$</code> 这两个“锚点”的行为。</p>
<ul>
<li><strong>默认行为 (没有 <code>re.MULTILINE</code>)</strong>：<ul>
<li><code>^</code> 只匹配整个<strong>字符串的最开头</strong>。</li>
<li><code>$</code> 只匹配整个<strong>字符串的最末尾</strong>（或者字符串末尾的换行符之前的位置）。</li>
</ul>
</li>
<li><strong>启用 <code>re.MULTILINE</code> 后的行为</strong>：<ul>
<li><code>^</code> 既能匹配整个字符串的最开头，<strong>也能</strong>匹配字符串内<strong>每一行</strong>（由换行符 <code>\n</code> 分隔）<strong>的开头</strong>。</li>
<li><code>$</code> 既能匹配整个字符串的最末尾，<strong>也能</strong>匹配字符串内<strong>每一行</strong>（在换行符 <code>\n</code> 之前）<strong>的结尾</strong>。</li>
</ul>
</li>
</ul>
<p><strong>举例说明:</strong></p>
<p>假设我们有这样一段包含多行文字的文本：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">text</span> = <span class="string">&quot;&quot;&quot;Report Alpha End</span></span><br><span class="line"><span class="string">Report Beta End</span></span><br><span class="line"><span class="string">Final Report&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>例 1：使用 ^ 查找以 “Hello” 开头的行</strong></p>
<ul>
<li><p><strong>没有 re.MULTILINE</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">matches = re.findall(<span class="string">r&quot;^Report&quot;</span>, text)</span><br><span class="line"><span class="built_in">print</span>(matches)</span><br><span class="line"><span class="comment"># 输出: [&#x27;Report&#x27;]</span></span><br></pre></td></tr></table></figure>

<p><strong>解释</strong>：<code>^Report</code> 只匹配了整个字符串 <code>text</code> 最开头的 <code>&quot;Report&quot;</code>。它不认为第二行的 <code>&quot;Report&quot;</code> 或是在“开头”。</p>
</li>
<li><p><strong>使用 re.MULTILINE</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">matches = re.findall(<span class="string">r&quot;^Report&quot;</span>, text, flags=re.MULTILINE)</span><br><span class="line"><span class="built_in">print</span>(matches)</span><br><span class="line"><span class="comment"># 输出: [&#x27;Report&#x27;, &#x27;Report&#x27;]</span></span><br></pre></td></tr></table></figure>

<p><strong>解释</strong>：因为加了 <code>re.MULTILINE</code> 标志，<code>^Report</code> 现在不仅匹配了整个字符串的开头（第一行的 <code>&quot;Report&quot;</code>），还匹配了<strong>第二行</strong>的开头（<code>&quot;Report Beta End&quot;</code> 中的 <code>&quot;Report&quot;</code>）。</p>
</li>
</ul>
<p><strong>例 2：使用 $ 查找以 “Python” 结尾的行</strong></p>
<ul>
<li><p><strong>没有 re.MULTILINE</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">matches = re.findall(<span class="string">r&quot;End$&quot;</span>, text)</span><br><span class="line"><span class="built_in">print</span>(matches)</span><br><span class="line"><span class="comment"># 输出: []</span></span><br></pre></td></tr></table></figure>

<p><strong>解释</strong>：整个字符串以 <code>&quot;Report&quot;</code> 结尾，不是 <code>&quot;End&quot;</code>，所以 <code>$</code> 不匹配任何地方。</p>
</li>
<li><p><strong>使用 re.MULTILINE</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">matches = re.findall(<span class="string">r&quot;End$&quot;</span>, text, flags=re.MULTILINE)</span><br><span class="line"><span class="built_in">print</span>(matches)</span><br><span class="line"><span class="comment"># 输出: [&#x27;End&#x27;, &#x27;End&#x27;]</span></span><br></pre></td></tr></table></figure>

<p><strong>解释</strong>：加了 <code>re.MULTILINE</code> 后，<code>End$</code> 匹配了第一行和第二行的<code>End</code>。</p>
</li>
</ul>
<p><strong>总结 <code>re.MULTILINE</code> 的作用：</strong></p>
<p>它让 <code>^</code> 和 <code>$</code> 的作用范围从“整个字符串的边界”扩展到了“<strong>每一行的边界</strong>”，这在你需要处理按行组织的文本，并希望模式能匹配行首或行尾的特定内容时非常有用。在你给出的原始代码中，使用这个标志可能是为了确保整个模式（从思考到答案）能够作为一个完整的单元出现在某一行或连续的几行内，并且严格地从行首开始匹配到行尾结束（或者字符串的起止位置）。</p>
</li>
</ol>
<p>好好好，说了那么多，那如果不加<code>flags = re.MULTILINE</code>会怎么样？（默认大家都知道<code>re.DOTALL</code>的重要性了）</p>
<p>有这么一种几种（其实可以归类成一种情况，但是我就不🤪）：</p>
<ul>
<li><p>情况1：开头第一行全是无关文字</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">okay </span><br><span class="line">&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;Let me think!&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;&lt;SOLUTION&gt;2&lt;/SOLUTION&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>情况2：结尾最后一行全是无关文字</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;Let me think!&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;&lt;SOLUTION&gt;2&lt;/SOLUTION&gt;</span><br><span class="line">thanks</span><br></pre></td></tr></table></figure>
</li>
<li><p>情况3：第一行和最后一行都是无关文字</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">okay </span><br><span class="line">&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;Let me think!&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;&lt;SOLUTION&gt;2&lt;/SOLUTION&gt;</span><br><span class="line">thanks</span><br></pre></td></tr></table></figure></li>
</ul>
<p>那么上述三种情况都无法完成匹配（好像不匹配这些情况也行🤪，作者想匹配，咋滴吧🙄）。</p>
<hr>
<h3 id="第八块：简单使用匹配模板"><a href="#第八块：简单使用匹配模板" class="headerlink" title="第八块：简单使用匹配模板"></a><strong>第八块：简单使用匹配模板</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">match_format.search(</span><br><span class="line">    <span class="string">&quot;&lt;start_working_out&gt;Let me think!&lt;end_working_out&gt;&quot;</span>\</span><br><span class="line">    <span class="string">&quot;&lt;SOLUTION&gt;2&lt;/SOLUTION&gt;&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;re.Match object; span=(0, 71), match=&#x27;&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;Let me think!&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>这里的span&#x3D;(0, 71)表示匹配的索引（字符）范围（包括0，不包括71），其实是全部匹配了，但后方的match并没有完全（我也不知道为啥🤷‍♂️）。</p>
<h3 id="第九块：定义精确格式匹配奖励函数"><a href="#第九块：定义精确格式匹配奖励函数" class="headerlink" title="第九块：定义精确格式匹配奖励函数"></a><strong>第九块：定义精确格式匹配奖励函数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">match_format_exactly</span>(<span class="params">completions, **kwargs</span>):</span><br><span class="line">    scores = []</span><br><span class="line">    <span class="keyword">for</span> completion <span class="keyword">in</span> completions:</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        response = completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        <span class="comment"># Match if format is seen exactly!</span></span><br><span class="line">        <span class="keyword">if</span> match_format.search(response) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: score += <span class="number">3.0</span></span><br><span class="line">        scores.append(score)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>

<p>这个 match_format_exactly 函数的作用是：</p>
<ol>
<li>接收一个包含多个“完成项”的列表 completions。</li>
<li>假设每个“完成项”的结构大致是 [{“content”: “实际文本”}, …]。</li>
<li>它会遍历每一个“完成项”，提取出其中的 “content” 文本。</li>
<li>使用一个预先编译好的正则表达式 match_format，通过 search 方法检查提取出的文本中<strong>是否包含</strong>符合该正则表达式格式的子串。</li>
<li>如果文本中<strong>包含</strong>了该格式，就给这个“完成项”打 3.0 分；如果不包含，就打 0 分。</li>
<li>最后，函数返回一个列表，该列表按顺序包含了每个输入“完成项”得到的分数（0 或 3.0）。</li>
</ol>
<p><strong>需要注意的点:</strong></p>
<ul>
<li>函数名中的 “exactly” 可能有点误导。因为使用的是 search 方法，它只需要在响应文本中<strong>找到</strong>模式即可，并不要求整个响应文本<strong>完全</strong>等于该模式。如果需要完全匹配整个响应，应该使用 <code>match_format.fullmatch(response)</code>。</li>
</ul>
<h3 id="第十块：定义宽松格式匹配奖励函数"><a href="#第十块：定义宽松格式匹配奖励函数" class="headerlink" title="第十块：定义宽松格式匹配奖励函数"></a><strong>第十块：定义宽松格式匹配奖励函数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">match_format_approximately</span>(<span class="params">completions, **kwargs</span>):</span><br><span class="line">    scores = []</span><br><span class="line">    <span class="keyword">for</span> completion <span class="keyword">in</span> completions:</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        response = completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">        <span class="comment"># Count how many keywords are seen - we penalize if too many!</span></span><br><span class="line">        <span class="comment"># If we see 1, then plus some points!</span></span><br><span class="line">        score += <span class="number">0.5</span> <span class="keyword">if</span> response.count(reasoning_start) == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1.0</span></span><br><span class="line">        score += <span class="number">0.5</span> <span class="keyword">if</span> response.count(reasoning_end)   == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1.0</span></span><br><span class="line">        score += <span class="number">0.5</span> <span class="keyword">if</span> response.count(solution_start)  == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1.0</span></span><br><span class="line">        score += <span class="number">0.5</span> <span class="keyword">if</span> response.count(solution_end)    == <span class="number">1</span> <span class="keyword">else</span> -<span class="number">1.0</span></span><br><span class="line">        scores.append(score)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>

<p>match_format_approximately 函数的作用是：</p>
<ol>
<li>它不使用正则表达式来检查复杂的结构。</li>
<li>而是<strong>独立地检查</strong>四个预定义的标记字符串（reasoning_start 等）在每个响应文本中<strong>出现的次数</strong>。</li>
<li>它<strong>奖励</strong>每个标记<strong>正好出现一次</strong>的情况（每次加 0.5 分）。</li>
<li>它<strong>惩罚</strong>每个标记<strong>不出现</strong>或<strong>出现多次</strong>（超过一次）的情况（每次减 1.0 分）。</li>
<li>一个完成项的总分是这四个独立检查得分的总和。</li>
</ol>
<p><strong>与 match_format_exactly 的关键区别:</strong></p>
<ul>
<li><strong>exactly</strong> 使用正则表达式检查标记<strong>是否存在</strong>并且是否符合<strong>特定的顺序和结构</strong>（由正则表达式定义）。只要找到一个符合结构的匹配就算成功（得 3 分）。</li>
<li><strong>approximately</strong> 完全不关心标记的顺序或它们之间的内容。它只关心<strong>每个标记自身出现的次数</strong>。它更像是检查“<strong>配料表</strong>”（是否包含了所有必需的标记，且不多不少），而不是检查“<strong>烹饪步骤</strong>”（标记是否按正确顺序和结构组织）。</li>
</ul>
<p>这种“近似”匹配方法可能用于快速筛选那些看起来“可能”包含了所需格式元素的响应，即使它们的结构不完全正确。高分（接近 2.0）表示所有必需的标记都出现了，并且都只出现了一次，这通常是一个好的迹象。低分则表示缺少标记或标记重复，格式可能存在问题。</p>
<hr>
<h3 id="第十一块：定义答案检查奖励函数"><a href="#第十一块：定义答案检查奖励函数" class="headerlink" title="第十一块：定义答案检查奖励函数"></a><strong>第十一块：定义答案检查奖励函数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_answer</span>(<span class="params">prompts, completions, answer, **kwargs</span>):</span><br><span class="line">    question = prompts[<span class="number">0</span>][-<span class="number">1</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line"></span><br><span class="line">    extracted_responses = [</span><br><span class="line">        guess.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (guess := match_format.search(r)) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span> \</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> responses</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    scores = []</span><br><span class="line">    <span class="keyword">for</span> guess, true_answer <span class="keyword">in</span> <span class="built_in">zip</span>(extracted_responses, answer):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> guess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            scores.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Correct answer gets 3 points!</span></span><br><span class="line">        <span class="keyword">if</span> guess == true_answer:</span><br><span class="line">            score += <span class="number">3.0</span></span><br><span class="line">        <span class="comment"># Match if spaces are seen, but less reward</span></span><br><span class="line">        <span class="keyword">elif</span> guess.strip() == true_answer.strip():</span><br><span class="line">            score += <span class="number">1.5</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># We also reward it if the answer is close via ratios!</span></span><br><span class="line">            <span class="comment"># Ie if the answer is within some range, reward it!</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                ratio = <span class="built_in">float</span>(guess) / <span class="built_in">float</span>(true_answer)</span><br><span class="line">                <span class="keyword">if</span>   ratio &gt;= <span class="number">0.9</span> <span class="keyword">and</span> ratio &lt;= <span class="number">1.1</span>: score += <span class="number">1.0</span></span><br><span class="line">                <span class="keyword">elif</span> ratio &gt;= <span class="number">0.8</span> <span class="keyword">and</span> ratio &lt;= <span class="number">1.2</span>: score += <span class="number">0.5</span></span><br><span class="line">                <span class="keyword">else</span>: score -= <span class="number">1.5</span> <span class="comment"># Penalize wrong answers</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                score -= <span class="number">1.5</span> <span class="comment"># Penalize</span></span><br><span class="line">        scores.append(score)</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>

<p>这个函数的主要目的是<strong>评估模型生成的一系列“完成”（completions）中提取出的答案与提供的标准答案的匹配程度，并给出一系列相应的分数</strong>。它采用了一种分层次的评分策略，从精确匹配到近似数值匹配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">extracted_responses = [</span><br><span class="line">        guess.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (guess := match_format.search(r)) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span> \</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> responses</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相当于</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 先创建一个空的列表，用来存放最终提取出来的答案</span></span><br><span class="line">extracted_responses = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 使用 for 循环遍历 `responses` 列表中的每一个原始响应文本 `r`</span></span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> responses:</span><br><span class="line">    <span class="comment"># 3. 对当前的响应文本 `r`，调用 match_format.search() 进行查找</span></span><br><span class="line">    guess = match_format.search(r)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 检查 search() 的结果</span></span><br><span class="line">    <span class="keyword">if</span> guess <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 5. 如果 guess 不是 None，意味着找到了匹配</span></span><br><span class="line">        <span class="comment">#    从匹配对象 guess 中提取第一个捕获组的内容 (就是 &lt;SOLUTION&gt;...&lt;/SOLUTION&gt; 之间的部分)</span></span><br><span class="line">        extracted_answer = guess.group(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 6. 将提取到的答案添加到 extracted_responses 列表中</span></span><br><span class="line">        extracted_responses.append(extracted_answer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 7. 如果 guess 是 None，意味着没有找到匹配</span></span><br><span class="line">        <span class="comment">#    在这种情况下，将 None 添加到 extracted_responses 列表中</span></span><br><span class="line">        extracted_responses.append(<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环结束后，extracted_responses 列表就包含了所有处理结果</span></span><br><span class="line"><span class="comment"># （要么是提取出的答案字符串，要么是 None）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这小段代码作用：</p>
<ol>
<li>遍历 <code>responses</code> 列表里的每一个字符串 <code>r</code>。</li>
<li>用 <code>match_format.search(r)</code> 尝试在这个字符串里找到符合模式的部分。</li>
<li>如果找到了 (结果不是 <code>None</code>)，就从找到的匹配对象里提取出第一个括号 (…) 捕获的内容 (<code>.group(1)</code>)，并把它存起来。</li>
<li>如果没找到 (结果是 <code>None</code>)，就存一个 <code>None</code>。</li>
<li>最后，把所有存起来的结果（提取到的字符串或 <code>None</code>）组成一个新的列表 <code>extracted_responses</code>。</li>
</ol>
<p>接下来的代码解释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">scores = []</span><br><span class="line">    <span class="comment"># 1. 初始化最终分数列表:</span></span><br><span class="line">    <span class="comment">#    - 创建一个空列表 `scores` 用于存储每个 completion 的最终得分。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> guess, true_answer <span class="keyword">in</span> <span class="built_in">zip</span>(extracted_responses, answer):</span><br><span class="line">        <span class="comment"># 2. 遍历提取出的答案和标准答案:</span></span><br><span class="line">        <span class="comment">#    - `zip(extracted_responses, answer)`: 将模型提取出的答案列表和标准答案列表配对，</span></span><br><span class="line">        <span class="comment">#      每次循环同时取出对应的 `guess` (模型答案) 和 `true_answer` (标准答案)。</span></span><br><span class="line"></span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 3. 初始化当前对的分数:</span></span><br><span class="line">        <span class="comment">#    - 对每一对答案，将分数 `score` 重置为 0。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> guess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 4. 处理提取失败的情况:</span></span><br><span class="line">            <span class="comment">#    - 如果 `guess` 是 `None`，意味着之前的正则表达式未能从模型响应中成功提取出答案。</span></span><br><span class="line">            scores.append(<span class="number">0</span>) <span class="comment"># 这种情况下得分记为 0。</span></span><br><span class="line">            <span class="keyword">continue</span> <span class="comment"># 跳过后续的比较，直接处理下一对答案。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># --- 如果 guess 不是 None (即成功提取出模型答案) ---</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Correct answer gets 3 points!</span></span><br><span class="line">        <span class="keyword">if</span> guess == true_answer:</span><br><span class="line">            <span class="comment"># 5. 精确匹配检查:</span></span><br><span class="line">            <span class="comment">#    - 判断提取出的 `guess` 是否与 `true_answer` **完全相同** (字符串完全相等)。</span></span><br><span class="line">            score += <span class="number">3.0</span> <span class="comment"># 如果完全相同，加 3.0 分。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Match if spaces are seen, but less reward</span></span><br><span class="line">        <span class="keyword">elif</span> guess.strip() == true_answer.strip():</span><br><span class="line">            <span class="comment"># 6. 忽略首尾空格的匹配检查:</span></span><br><span class="line">            <span class="comment">#    - `.strip()`: 去除字符串开头和结尾的空白字符（空格、换行、制表符等）。</span></span><br><span class="line">            <span class="comment">#    - 如果去除首尾空格后的 `guess` 和 `true_answer` 相等。</span></span><br><span class="line">            score += <span class="number">1.5</span> <span class="comment"># 加 1.5 分（比精确匹配少）。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 7. 如果字符串不匹配，尝试数值近似匹配:</span></span><br><span class="line">            <span class="comment"># We also reward it if the answer is close via ratios!</span></span><br><span class="line">            <span class="comment"># Ie if the answer is within some range, reward it!</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># 8. 尝试转换为浮点数并计算比率:</span></span><br><span class="line">                <span class="comment">#    - `float(guess)`: 尝试将模型答案转换为浮点数。</span></span><br><span class="line">                <span class="comment">#    - `float(true_answer)`: 尝试将标准答案转换为浮点数。</span></span><br><span class="line">                <span class="comment">#    - `/`: 计算两者的比率。</span></span><br><span class="line">                <span class="comment">#    - `try...except`: 使用 try-except 块来捕获转换失败可能引发的异常 </span></span><br><span class="line">                <span class="comment">#      (比如答案是 &quot;two&quot; 而不是 &quot;2&quot;，或者答案为空字符串等，无法转为 float)。</span></span><br><span class="line">                ratio = <span class="built_in">float</span>(guess) / <span class="built_in">float</span>(true_answer)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> ratio &gt;= <span class="number">0.9</span> <span class="keyword">and</span> ratio &lt;= <span class="number">1.1</span>:</span><br><span class="line">                    <span class="comment"># 9. 检查比率是否在 [0.9, 1.1] 区间 (即 ±10%):</span></span><br><span class="line">                    score += <span class="number">1.0</span> <span class="comment"># 如果非常接近，加 1.0 分。</span></span><br><span class="line">                <span class="keyword">elif</span> ratio &gt;= <span class="number">0.8</span> <span class="keyword">and</span> ratio &lt;= <span class="number">1.2</span>:</span><br><span class="line">                    <span class="comment"># 10. 检查比率是否在 [0.8, 1.2] 区间 (即 ±20%):</span></span><br><span class="line">                    score += <span class="number">0.5</span> <span class="comment"># 如果比较接近，加 0.5 分。</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 11. 如果比率超出 ±20% 范围:</span></span><br><span class="line">                    score -= <span class="number">1.5</span> <span class="comment"># 答案偏差太大，扣 1.5 分 (惩罚)。</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="comment"># 12. 处理转换失败或除零错误:</span></span><br><span class="line">                <span class="comment">#    - 如果 `try` 块中的代码（主要是 `float()` 转换或除法）出错。</span></span><br><span class="line">                score -= <span class="number">1.5</span> <span class="comment"># 同样扣 1.5 分 (惩罚无法进行数值比较的情况)。</span></span><br><span class="line">        </span><br><span class="line">        scores.append(score)</span><br><span class="line">        <span class="comment"># 13. 将计算出的分数添加到列表:</span></span><br><span class="line">        <span class="comment">#     - 将当前这对答案最终计算得到的 `score` 添加到 `scores` 列表中。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line">    <span class="comment"># 14. 返回分数列表:</span></span><br><span class="line">    <span class="comment">#     - 当所有答案对都处理完毕后，返回包含所有分数的 `scores` 列表。</span></span><br></pre></td></tr></table></figure>

<p><code>check_answer</code> 函数通过以下步骤来评估模型完成项的答案质量：</p>
<ol>
<li>从原始完成项中提取出纯文本响应。</li>
<li>使用预定义的正则表达式 <code>match_format</code> 尝试从每个响应中提取出答案部分（捕获组 1）。如果提取失败，该响应的得分为 0。</li>
<li>如果提取成功，将提取出的答案与对应的标准答案进行比较：<ul>
<li><strong>精确匹配</strong>：得分 +3.0。</li>
<li><strong>忽略首尾空格后匹配</strong>：得分 +1.5。</li>
<li><strong>否则，尝试数值比较</strong>：<ul>
<li>转换为浮点数计算比率。</li>
<li>比率在 <strong>±10%</strong> 内：得分 +1.0。</li>
<li>比率在 <strong>±20%</strong> 内：得分 +0.5。</li>
<li>比率<strong>超出 ±20%</strong> 或<strong>无法进行数值转换&#x2F;比较</strong>：得分 -1.5 (惩罚)。</li>
</ul>
</li>
</ul>
</li>
<li>返回一个包含每个完成项得分的列表。</li>
</ol>
<p>这个函数旨在对预期答案主要是数值类型的问题进行比较鲁棒的评分，同时也能处理一些格式上的小差异（如多余空格）。</p>
<hr>
<h3 id="第十二块：定义数字匹配模板"><a href="#第十二块：定义数字匹配模板" class="headerlink" title="第十二块：定义数字匹配模板"></a><strong>第十二块：定义数字匹配模板</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">match_numbers = re.<span class="built_in">compile</span>(</span><br><span class="line">    solution_start + <span class="string">r&quot;.*?([\d\.\,]&#123;1,&#125;)&quot;</span>,</span><br><span class="line">    flags = re.MULTILINE | re.DOTALL</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(match_numbers.findall(<span class="string">&quot;&lt;SOLUTION&gt;  0.34  &lt;/SOLUTION&gt;&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(match_numbers.findall(<span class="string">&quot;&lt;SOLUTION&gt;  123,456  &lt;/SOLUTION&gt;&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;0.34&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;123,456&#x27;</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li><code>.*?</code>:<ul>
<li><code>.</code>: 匹配任何单个字符（因为有 <code>re.DOTALL</code> 标志，所以<strong>包括换行符</strong>）。</li>
<li><code>*</code>: 匹配前面的 <code>.</code> 零次或多次。</li>
<li><code>?</code>: 使 <code>*</code> 变成<strong>非贪婪</strong>模式。这意味着它会匹配<strong>尽可能少</strong>的字符，直到后面的模式能够匹配为止。</li>
<li><strong>作用</strong>: 从 <code>solution_start</code> 标记之后开始，匹配任意数量的最少字符。</li>
</ul>
</li>
<li><code>([\d\.\,]&#123;1,&#125;)</code>: 这是模式的关键部分，也是一个<strong>捕获组</strong>。<ul>
<li><code>(...)</code>: 定义了一个<strong>捕获组</strong>。<code>findall</code> 方法特别关注捕获组——如果模式中有捕获组，<code>findall</code> 会返回<strong>所有匹配中捕获组的内容</strong>。</li>
<li><code>[ ... ]</code>: 定义一个字符集，匹配方括号内<strong>任意一个</strong>字符。</li>
<li><code>\d</code>: 匹配任何<strong>数字</strong> (<code>0-9</code>)。</li>
<li><code>\.</code>: 匹配一个<strong>字面量的点</strong> (<code>.</code>)。需要用反斜杠 <code>\</code> 转义，因为 <code>.</code> 在正则表达式中通常有特殊含义（匹配任意字符）。</li>
<li><code>\,</code>: 匹配一个<strong>字面量的逗号</strong> (<code>,</code>)。在字符集 <code>[]</code> 内部，逗号通常不需要转义，但转义了也没错，有时能增加清晰度。</li>
<li><code>[ \d\.\,]:</code>这个字符集整体表示：匹配一个字符，这个字符必须是<strong>数字、点或逗号</strong>中的任意一个。</li>
<li><code>&#123;1,&#125;</code>: 这是量词，表示前面的元素（即 <code>[\d\.\,]</code> 字符集）必须出现<strong>至少 1 次</strong> (<code>1</code>) 或<strong>任意多次</strong> (<code>,</code>)。</li>
<li><strong>捕获组 <code>([\d\.\,]&#123;1,&#125;)</code> 的整体含义</strong>: 匹配并<strong>捕获</strong>一个由<strong>至少一个</strong>数字、点或逗号组成的<strong>连续</strong>序列。</li>
</ul>
</li>
<li><code>flags = re.MULTILINE | re.DOTALL</code>:<ul>
<li><code>re.DOTALL</code>: 让 . 可以匹配包括换行符在内的所有字符。这影响了前面的 <code>.*?</code>。</li>
<li><code>re.MULTILINE</code>: 让 <code>^</code> 和 <code>$</code> 可以匹配行的开头和结尾。<strong>但这个特定的正则表达式模式并没有使用 <code>^</code> 或 <code>$</code>，所以 <code>re.MULTILINE</code> 在这里实际上没有产生任何效果。</strong></li>
</ul>
</li>
</ul>
<h3 id="第十三块：定义答案中数字检查奖励函数"><a href="#第十三块：定义答案中数字检查奖励函数" class="headerlink" title="第十三块：定义答案中数字检查奖励函数"></a><strong>第十三块：定义答案中数字检查奖励函数</strong></h3><p>原始：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> PRINTED_TIMES</span><br><span class="line">PRINTED_TIMES = <span class="number">0</span></span><br><span class="line"><span class="keyword">global</span> PRINT_EVERY_STEPS</span><br><span class="line">PRINT_EVERY_STEPS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_numbers</span>(<span class="params">prompts, completions, answer, **kwargs</span>):</span><br><span class="line">    question = prompts[<span class="number">0</span>][-<span class="number">1</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line"></span><br><span class="line">    extracted_responses = [</span><br><span class="line">        guess.group(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> (guess := match_numbers.search(r)) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span> \</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> responses</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    scores = []</span><br><span class="line">    <span class="comment"># Print only every few steps</span></span><br><span class="line">    <span class="keyword">global</span> PRINTED_TIMES</span><br><span class="line">    <span class="keyword">global</span> PRINT_EVERY_STEPS</span><br><span class="line">    <span class="keyword">if</span> PRINTED_TIMES % PRINT_EVERY_STEPS == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">20</span>, <span class="string">f&quot;Question:\n<span class="subst">&#123;question&#125;</span>&quot;</span>, <span class="string">f&quot;\nAnswer:\n<span class="subst">&#123;answer[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nResponse:\n<span class="subst">&#123;responses[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nExtracted:\n<span class="subst">&#123;extracted_responses[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    PRINTED_TIMES += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> guess, true_answer <span class="keyword">in</span> <span class="built_in">zip</span>(extracted_responses, answer):</span><br><span class="line">        <span class="keyword">if</span> guess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            scores.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># Convert to numbers</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            true_answer = <span class="built_in">float</span>(true_answer.strip())</span><br><span class="line">            <span class="comment"># Remove commas like in 123,456</span></span><br><span class="line">            guess       = <span class="built_in">float</span>(guess.strip().replace(<span class="string">&quot;,&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">            scores.append(<span class="number">1.5</span> <span class="keyword">if</span> guess == true_answer <span class="keyword">else</span> -<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            scores.append(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>

<p>带注释版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义全局变量用于控制打印频率</span></span><br><span class="line"><span class="keyword">global</span> PRINTED_TIMES</span><br><span class="line">PRINTED_TIMES = <span class="number">0</span>  <span class="comment"># 初始化一个计数器，记录函数被调用的次数</span></span><br><span class="line"><span class="keyword">global</span> PRINT_EVERY_STEPS</span><br><span class="line">PRINT_EVERY_STEPS = <span class="number">5</span> <span class="comment"># 设置每调用 5 次函数打印一次信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_numbers</span>(<span class="params">prompts, completions, answer, **kwargs</span>):</span><br><span class="line">    <span class="comment"># 1. 定义函数:</span></span><br><span class="line">    <span class="comment">#    - `prompts`: 就是第6块的prompts。</span></span><br><span class="line">    <span class="comment">#    - `completions`: 包含模型生成的多个完成项的列表。</span></span><br><span class="line">    <span class="comment">#    - `answer`: 包含与每个 completion 对应的标准答案的列表 (预期是数字字符串)。</span></span><br><span class="line">    <span class="comment">#    - `**kwargs`: 可选关键字参数 (未使用)。</span></span><br><span class="line"></span><br><span class="line">    question = prompts[<span class="number">0</span>][-<span class="number">1</span>][<span class="string">&quot;content&quot;</span>]</span><br><span class="line">    <span class="comment"># 2. 提取问题文本 (主要用于打印调试信息)。</span></span><br><span class="line">    responses = [completion[<span class="number">0</span>][<span class="string">&quot;content&quot;</span>] <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line">    <span class="comment"># 3. 提取原始响应文本列表。</span></span><br><span class="line"></span><br><span class="line">    extracted_responses = [</span><br><span class="line">        <span class="comment"># 使用列表推导式和海象运算符，配合 match_numbers 正则表达式提取数字字符串</span></span><br><span class="line">        guess.group(<span class="number">1</span>) <span class="comment"># 如果匹配成功，取第一个捕获组 (应为数字、点、逗号组成的字符串)</span></span><br><span class="line">        <span class="keyword">if</span> (guess := match_numbers.search(r)) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">None</span> <span class="comment"># 尝试搜索，找到则赋值并判断非 None</span></span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> responses <span class="comment"># 遍历每个原始响应 r</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># 4. 提取模型响应中的数字字符串:</span></span><br><span class="line">    <span class="comment">#    - 这与 `check_answer` 中的提取逻辑类似，但使用的是 `match_numbers` 正则表达式。</span></span><br><span class="line">    <span class="comment">#    - `match_numbers` 被设计用来查找 solution_start 标记后的第一个数字/点/逗号序列。</span></span><br><span class="line">    <span class="comment">#    - `extracted_responses` 列表包含了从每个响应中提取出的数字字符串（或 None）。</span></span><br><span class="line"></span><br><span class="line">    scores = [] <span class="comment"># 5. 初始化最终分数列表。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- 打印调试信息部分 ---</span></span><br><span class="line">    <span class="comment"># Print only every few steps</span></span><br><span class="line">    <span class="keyword">global</span> PRINTED_TIMES <span class="comment"># 声明要修改全局变量 PRINTED_TIMES</span></span><br><span class="line">    <span class="keyword">global</span> PRINT_EVERY_STEPS <span class="comment"># 声明要使用全局变量 PRINT_EVERY_STEPS</span></span><br><span class="line">    <span class="keyword">if</span> PRINTED_TIMES % PRINT_EVERY_STEPS == <span class="number">0</span>: <span class="comment"># 检查调用次数是否是打印步长的整数倍</span></span><br><span class="line">        <span class="comment"># 6. 定期打印调试信息:</span></span><br><span class="line">        <span class="comment">#    - `PRINTED_TIMES % PRINT_EVERY_STEPS == 0`: 使用模运算 (%) 判断调用次数 `PRINTED_TIMES` </span></span><br><span class="line">        <span class="comment">#      除以 `PRINT_EVERY_STEPS` (5) 的余数是否为 0。如果是，则执行打印。</span></span><br><span class="line">        <span class="comment">#    - `print(...)`: 打印分隔符、问题、第一个标准答案、第一个原始响应和第一个提取出的数字字符串。</span></span><br><span class="line">        <span class="comment">#      这有助于在程序运行时观察输入、输出和提取结果是否符合预期。</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">20</span>, <span class="string">f&quot;Question:\n<span class="subst">&#123;question&#125;</span>&quot;</span>, <span class="string">f&quot;\nAnswer:\n<span class="subst">&#123;answer[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nResponse:\n<span class="subst">&#123;responses[<span class="number">0</span>]&#125;</span>&quot;</span>, <span class="string">f&quot;\nExtracted:\n<span class="subst">&#123;extracted_responses[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    PRINTED_TIMES += <span class="number">1</span> <span class="comment"># 7. 增加调用次数计数器: 每次函数执行完毕前，将计数器加 1。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># --- 核心评分逻辑 ---</span></span><br><span class="line">    <span class="keyword">for</span> guess, true_answer <span class="keyword">in</span> <span class="built_in">zip</span>(extracted_responses, answer):</span><br><span class="line">        <span class="comment"># 8. 遍历提取出的数字字符串和标准答案。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> guess <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 9. 处理提取失败: 如果未能从模型响应中提取出数字字符串 (`guess` is None)。</span></span><br><span class="line">            scores.append(<span class="number">0</span>) <span class="comment"># 得分为 0。</span></span><br><span class="line">            <span class="keyword">continue</span> <span class="comment"># 跳过后续比较。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to numbers</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 10. 尝试将标准答案和提取出的猜测转换为数字进行比较:</span></span><br><span class="line">            true_answer = <span class="built_in">float</span>(true_answer.strip())</span><br><span class="line">            <span class="comment">#    - `true_answer.strip()`: 去除标准答案字符串首尾的空格。</span></span><br><span class="line">            <span class="comment">#    - `float(...)`: 将处理过的标准答案字符串转换为浮点数。</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Remove commas like in 123,456</span></span><br><span class="line">            guess = <span class="built_in">float</span>(guess.strip().replace(<span class="string">&quot;,&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">            <span class="comment">#    - `guess.strip()`: 去除提取出的数字字符串首尾的空格。</span></span><br><span class="line">            <span class="comment">#    - `.replace(&quot;,&quot;, &quot;&quot;)`: 移除字符串中所有的逗号 (处理 &quot;123,456&quot; 这样的情况)。</span></span><br><span class="line">            <span class="comment">#    - `float(...)`: 将处理后的、不含逗号的字符串转换为浮点数。</span></span><br><span class="line">            </span><br><span class="line">            scores.append(<span class="number">1.5</span> <span class="keyword">if</span> guess == true_answer <span class="keyword">else</span> -<span class="number">0.5</span>)</span><br><span class="line">            <span class="comment"># 11. 比较转换后的数字并评分:</span></span><br><span class="line">            <span class="comment">#    - `guess == true_answer`: 判断两个浮点数是否**完全相等**。</span></span><br><span class="line">            <span class="comment">#    - `1.5 if ... else -0.5`: </span></span><br><span class="line">            <span class="comment">#        - 如果数字完全相等，得 1.5 分。</span></span><br><span class="line">            <span class="comment">#        - 如果数字不相等，得 -0.5 分 (作为惩罚)。</span></span><br><span class="line">            <span class="comment">#    - 将得分添加到 `scores` 列表。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="comment"># 12. 处理转换失败:</span></span><br><span class="line">            <span class="comment">#    - 如果 `try` 块中的任何操作（`strip`, `replace`, `float`）失败，</span></span><br><span class="line">            <span class="comment">#      比如答案或提取结果不是有效的数字格式。</span></span><br><span class="line">            scores.append(<span class="number">0</span>) <span class="comment"># 转换失败或比较出错的情况下，得分记为 0。</span></span><br><span class="line">            <span class="keyword">continue</span> <span class="comment"># 继续处理下一对。</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> scores <span class="comment"># 13. 返回包含所有分数的列表。</span></span><br></pre></td></tr></table></figure>

<p><code>check_numbers</code> 函数专门用于评估模型生成的答案中<strong>数值</strong>的准确性：</p>
<ol>
<li>它使用 <code>match_numbers</code> 正则表达式从模型响应中<strong>提取</strong>看起来像数字（可能包含点和逗号）的字符串。</li>
<li>它有一个<strong>调试打印机制</strong>，每隔 PRINT_EVERY_STEPS 次调用就会打印一次处理信息，方便开发者跟踪情况。</li>
<li>如果成功提取出数字字符串，它会尝试将<strong>提取出的字符串</strong>（去除空格和逗号后）和<strong>标准答案字符串</strong>（去除空格后）都<strong>转换成浮点数</strong>。</li>
<li><strong>比较这两个浮点数是否完全相等</strong>：<ul>
<li>相等：得 <code>1.5</code> 分。</li>
<li>不相等：得 <code>-0.5</code> 分（惩罚）。</li>
</ul>
</li>
<li>如果在提取或转换数字的过程中出现任何<strong>错误</strong>（比如提取失败、无法转换为浮点数），则该项得 <code>0</code> 分。</li>
<li>最终返回一个包含每个完成项得分的列表。</li>
</ol>
<p><strong>与 <code>check_answer</code> 的主要区别:</strong></p>
<ul>
<li><strong>侧重点</strong>: <code>check_numbers</code> <strong>只关心数值的精确匹配</strong>（转换成 <code>float</code> 后比较），而 <code>check_answer</code> 的评分逻辑更复杂，包含字符串精确匹配、忽略空格匹配和数值的<strong>近似比率</strong>匹配。</li>
<li><strong>错误处理</strong>: <code>check_numbers</code> 在无法转换为数字比较时给 <code>0</code> 分，而 <code>check_answer</code> 在无法进行数值比较时会给 <code>-1.5</code> 的惩罚分。</li>
<li><strong>评分</strong>: <code>check_numbers</code> 的分数范围是明确的 <code>&#123;1.5, -0.5, 0&#125;</code>，而 <code>check_answer</code> 的分数可能性更多 <code>&#123;3.0, 1.5, 1.0, 0.5, 0, -1.5&#125;</code>。</li>
<li><strong>调试信息</strong>: <code>check_numbers</code> 包含了一个定期打印调试信息的机制。</li>
</ul>
<hr>
<h3 id="第十四块：计算最大prompt长度"><a href="#第十四块：计算最大prompt长度" class="headerlink" title="第十四块：计算最大prompt长度"></a><strong>第十四块：计算最大prompt长度</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">max</span>(dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;tokens&quot;</span> : tokenizer.apply_chat_template(x[<span class="string">&quot;prompt&quot;</span>], add_generation_prompt = <span class="literal">True</span>, tokenize = <span class="literal">True</span>)&#125;,</span><br><span class="line">    batched = <span class="literal">True</span>,</span><br><span class="line">).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123;<span class="string">&quot;length&quot;</span> : <span class="built_in">len</span>(x[<span class="string">&quot;tokens&quot;</span>])&#125;)[<span class="string">&quot;length&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">287</span><br></pre></td></tr></table></figure>

<p>这行代码的最终目的是计算出数据集中所有 <code>prompt</code>（经过特定格式化和分词后）的<strong>最大长度</strong>（以<code> token</code> 数量计）。这在设置模型训练或推理时的最大序列长度时很有用。</p>
<p><strong>分解步骤:</strong></p>
<ol>
<li><p><strong><code>dataset.map(...)</code> (第一个 map):</strong></p>
<ul>
<li><strong>作用</strong>: 对原始 <code>dataset</code> 中的每一条数据（或每一批数据）应用一个转换函数。</li>
<li><strong><code>lambda x: ...</code></strong>: 定义了这个转换函数。<ul>
<li><code>x</code>: 代表数据集中的一条数据（或一批数据，因为 <code>batched=True</code>）。我们知道 <code>x</code> 至少包含一个键 <code>&quot;prompt&quot;</code>。</li>
<li><code>x[&quot;prompt&quot;]</code>: 获取这条数据（或这批数据）的 “prompt” 部分。根据之前的例子，这可能是一个列表，包含了对话的角色和内容，例如 <code>[&#123;&#39;role&#39;: &#39;system&#39;, &#39;content&#39;: &#39;...&#39;&#125;, &#123;&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;...&#39;&#125;]</code>。</li>
<li><strong><code>tokenizer.apply_chat_template(x[&quot;prompt&quot;], add_generation_prompt = True, tokenize = True)</code></strong>: 这是核心操作。<ul>
<li><code>tokenizer</code>: 指向一个预先加载好的分词器对象 (Tokenizer)。</li>
<li><code>.apply_chat_template()</code>: 这个方法会根据分词器内部定义的“聊天模板”，将输入的对话结构 (<code>x[&quot;prompt&quot;]</code>) 转换成一个适合模型输入的<strong>单一字符串或 token ID 列表</strong>。它会添加特殊标记来区分不同的角色（如 system, user, assistant）。</li>
<li><code>add_generation_prompt = True</code>: 这个参数通常意味着在模板末尾加上提示模型开始生成回应的标记（比如 <code>&lt;|assistant|&gt;\n</code> 或类似标记对应的 token ID）。</li>
<li><code>tokenize = True</code>: 这个参数指示 <code>.apply_chat_template</code> 不仅仅是生成格式化后的字符串，还要<strong>直接将其分词（tokenize）</strong>，输出一个<strong>整数列表（token IDs）</strong>。</li>
</ul>
</li>
<li><code>&#123;&quot;tokens&quot; : ...&#125;</code>: lambda 函数返回一个新的字典，其中键是 <code>&quot;tokens&quot;</code>，值是上面 <code>apply_chat_template</code> 产生的 token ID 列表。</li>
</ul>
</li>
<li><strong><code>batched = True</code></strong>: 这是一个重要的性能优化参数。它告诉 <code>.map</code> 函数应该将数据<strong>分批 (batch)</strong> 传递给 lambda 函数，而不是一条一条地处理。lambda 函数中的 <code>x[&quot;prompt&quot;]</code> 此时会是一批 prompts，<code>apply_chat_template</code> 通常也能高效地处理批量输入。</li>
<li><strong>第一个 map 的输出</strong>: 它产生一个新的数据集（我们叫它 <code>dataset_with_tokens</code>）。这个新数据集中，每条数据都包含了原始数据的所有字段，<strong>外加一个新的字段 <code>&quot;tokens&quot;</code></strong>，里面存着对应 <code>&quot;prompt&quot;</code> 被格式化和分词后的 <code>token ID</code> 列表。</li>
</ul>
</li>
<li><p><strong><code>.map(...)</code> (第二个 map):</strong></p>
<ul>
<li><strong>作用</strong>: 对<strong>上一步产生</strong>的 <code>dataset_with_tokens</code> 数据集再次应用转换。</li>
<li><strong><code>lambda x: ...</code></strong>: 定义了第二个转换函数。<ul>
<li><code>x</code>: 代表 <code>dataset_with_tokens</code> 中的一条数据（这里没有 <code>batched=True</code>，所以通常是一条条处理）。这条数据现在肯定包含 <code>&quot;tokens&quot;</code> 字段。</li>
<li><code>x[&quot;tokens&quot;]</code>: 获取这条数据的 token ID 列表。</li>
<li><code>len(x[&quot;tokens&quot;])</code>: 计算这个 token ID 列表的<strong>长度</strong>，也就是 token 的数量。</li>
<li><code>&#123;&quot;length&quot; : ...&#125;</code>: lambda 函数返回一个字典，键是 <code>&quot;length&quot;</code>，值是计算出的 token 数量。</li>
</ul>
</li>
<li><strong>第二个 map 的输出</strong>: 它又产生一个新的数据集（我们叫它 <code>dataset_with_tokens_and_length</code>）。这个数据集中，每条数据包含了之前的所有字段，<strong>又增加了一个新字段 <code>&quot;length&quot;</code></strong>，里面存着对应 “prompt” 的 token 数量。</li>
</ul>
</li>
<li><p><strong><code>[&quot;length&quot;]</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 在很多数据处理库（如 Hugging Face <code>datasets</code>）中，对数据集对象使用方括号索引（如 <code>[&quot;length&quot;]</code>) 会<strong>提取出所有数据条目中名为 “length” 的那个字段的值</strong>。</li>
<li><strong>输出</strong>: 它返回一个包含所有 “prompt” 长度的列表（或类似的数据结构），例如 <code>[150, 123, 210, 88, ...]</code>。</li>
</ul>
</li>
<li><p><strong><code>max(...)</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 这是 Python 内置的标准函数，用于找出列表或其他可迭代对象中的<strong>最大值</strong>。</li>
<li><strong>输入</strong>: 上一步产生的包含所有 token 长度的列表。</li>
<li><strong>输出</strong>: 一个<strong>单一的数字</strong>，代表了数据集中所有经过处理的 “prompt” 中最长的那一个的 token 数量。</li>
</ul>
</li>
</ol>
<p><strong>总结:</strong></p>
<p>这行代码执行了一个<strong>数据处理流水线</strong>：</p>
<ol>
<li>取原始 <code>dataset</code>。</li>
<li><strong>第一步映射 (<code>map</code>)</strong>: 将每个 <code>prompt</code> 使用 <code>tokenizer</code> 的聊天模板格式化并分词，得到 <code>token ID</code> 列表，并将结果存入新的 <code>&quot;tokens&quot;</code> 字段（批量处理以提高效率）。</li>
<li><strong>第二步映射 (<code>map</code>)</strong>: 计算每个 <code>&quot;tokens&quot;</code> 列表的长度，并将结果存入新的 <code>&quot;length&quot;</code> 字段。</li>
<li><strong>提取长度</strong>: 从最终的数据集中单独拿出所有 <code>&quot;length&quot;</code> 的值，形成一个长度列表。</li>
<li><strong>计算最大值</strong>: 在这个长度列表中找到最大的那个数字。</li>
</ol>
<p>最终结果就是整个数据集中，经过聊天模板格式化、添加生成提示并分词后的最长 <code>prompt</code> 的 <code>token</code> 数量。</p>
<hr>
<h3 id="第十五块：定义训练参数"><a href="#第十五块：定义训练参数" class="headerlink" title="第十五块：定义训练参数"></a><strong>第十五块：定义训练参数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">max_prompt_length = <span class="number">287</span> + <span class="number">1</span> <span class="comment"># + 1 just in case!</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> GRPOConfig, GRPOTrainer</span><br><span class="line">training_args = GRPOConfig(</span><br><span class="line">    learning_rate = <span class="number">5e-6</span>,</span><br><span class="line">    weight_decay = <span class="number">0.1</span>,</span><br><span class="line">    warmup_ratio = <span class="number">0.1</span>,</span><br><span class="line">    lr_scheduler_type = <span class="string">&quot;cosine&quot;</span>,</span><br><span class="line">    optim = <span class="string">&quot;adamw_torch_fused&quot;</span>,</span><br><span class="line">    logging_steps = <span class="number">1</span>,</span><br><span class="line">    per_device_train_batch_size = <span class="number">1</span>,</span><br><span class="line">    gradient_accumulation_steps = <span class="number">4</span>, <span class="comment"># Increase to 4 for smoother training</span></span><br><span class="line">    num_generations = <span class="number">8</span>, <span class="comment"># Decrease if out of memory</span></span><br><span class="line">    max_prompt_length = max_prompt_length,</span><br><span class="line">    max_completion_length = max_seq_length - max_prompt_length,</span><br><span class="line">    <span class="comment"># num_train_epochs = 1, # Set to 1 for a full training run</span></span><br><span class="line">    max_steps = <span class="number">1000</span>,</span><br><span class="line">    save_steps = <span class="number">250</span>,</span><br><span class="line">    max_grad_norm = <span class="number">0.1</span>,</span><br><span class="line">    report_to = <span class="string">&quot;none&quot;</span>, <span class="comment"># Can use Weights &amp; Biases</span></span><br><span class="line">    output_dir = <span class="string">&quot;outputs&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.</span><br><span class="line">We will change the batch size of <span class="number">1</span> to the `num_generations` of <span class="number">8</span></span><br><span class="line"></span><br><span class="line">（翻译）</span><br><span class="line">Unsloth：我们现在要求 `per_device_train_batch_size` 必须是 `num_generations` 的倍数。  </span><br><span class="line">我们会把原本的 batch size 从 <span class="number">1</span> 自动调整为 `num_generations` 的值 <span class="number">8</span>。</span><br></pre></td></tr></table></figure>

<p>这段代码主要是用来设置训练过程中的各种参数，使用的是 <code>trl</code> (Transformer Reinforcement Learning) 库中的 <code>GRPOConfig</code> 类。</p>
<p><strong>1. <code>max_prompt_length = 287 + 1 # + 1 just in case!</code></strong></p>
<ul>
<li><strong>作用</strong>: 计算并设置一个名为 <code>max_prompt_length</code> 的变量，值为 <code>288</code>。</li>
<li><strong>含义</strong>: 这个变量很可能代表在训练或生成过程中，允许输入给模型的“提示”（<code>prompt</code>）的最大长度（通常以 <code>token</code> 数量计算）。</li>
<li><strong>+ 1 just in case!</strong>: 这个注释表明加 <code>1</code> 是为了留出一点余量或缓冲区。这在处理序列长度时很常见，可能是为了容纳一个额外的特殊标记（如句子结束符 <code>EOS</code>），或者只是为了避免因精确计算差一导致的问题。</li>
</ul>
<p><strong>2. <code>from trl import GRPOConfig, GRPOTrainer</code></strong></p>
<ul>
<li><strong>作用</strong>: 从 <code>trl</code> 库导入两个重要的类：<code>GRPOConfig</code> 和 <code>GRPOTrainer</code>。</li>
<li><strong><code>trl</code></strong>: 这是一个流行的 <code>Hugging Face</code> 库，专门用于使用强化学习等方法（如 <code>RLHF - Reinforcement Learning from Human Feedback</code>, <code>DPO - Direct Preference Optimization</code>, <code>GRPO - Generalized Preference Optimization</code>）来微调 <code>Transformer</code> 模型（如 <code>GPT</code> 系列、<code>LLaMA</code> 等）。</li>
<li><strong><code>GRPOConfig</code></strong>: 这是一个配置类（通常是 <code>dataclass</code>），专门用来<strong>收集和存储</strong>所有与 <code>GRPO</code> 训练过程相关的超参数和设置。把它想象成一个包含所有训练选项的设置菜单。</li>
<li><strong><code>GRPOTrainer</code></strong>: 这是实际执行 <code>GRPO</code> 训练过程的<strong>训练器</strong>类。你需要把后面创建的 <code>GRPOConfig</code> 对象传递给它，它才知道该如何进行训练。</li>
</ul>
<p><strong>3. <code>training_args = GRPOConfig(...)</code></strong></p>
<ul>
<li><strong>作用</strong>: 创建 <code>GRPOConfig</code> 类的一个实例，并将所有指定的训练参数传递给它。这个创建好的配置对象被存储在变量 <code>training_args</code> 中。</li>
<li><strong>含义</strong>: <code>training_args</code> 现在包含了运行 <code>GRPOTrainer</code> 所需的所有配置信息。之后创建 <code>GRPOTrainer</code> 时，通常会这样写：<code>trainer = GRPOTrainer(..., args=training_args, ...)</code>。</li>
</ul>
<p><strong>4. <code>GRPOConfig(...)</code> 中的参数详解:</strong></p>
<p>下面是传递给 <code>GRPOConfig</code> 的每个参数的解释：</p>
<ul>
<li><strong><code>learning_rate = 5e-6</code></strong>: 学习率（Learning Rate）。控制模型在每次更新时调整其内部参数（权重）的幅度。<code>5e-6</code> (即 0.000005) 是一个在微调大型语言模型时常用的较小值，有助于稳定训练。</li>
<li><strong><code>weight_decay = 0.1</code></strong>: 权重衰减（Weight Decay）。一种正则化技术，通过给模型的损失函数添加一个惩罚项来限制模型权重的大小，有助于防止模型过拟合训练数据。</li>
<li><strong><code>warmup_ratio = 0.1</code></strong>: 学习率预热（Warmup）比例。在训练初期，学习率不是直接设为 <code>5e-6</code>，而是从一个更小的值开始，在总训练步数的 <code>10%</code> (0.1) 期间内逐渐增加到 <code>5e-6</code>。这有助于在训练早期稳定模型的学习过程。</li>
<li><strong><code>lr_scheduler_type = &quot;cosine&quot;</code></strong>: 学习率调度器类型（Learning Rate Scheduler Type）。定义了在预热阶段之后，学习率如何随着训练的进行而变化。”cosine” 表示学习率将按照余弦曲线的形状逐渐下降，通常会降到接近 0。</li>
<li><strong><code>optim = &quot;adamw_torch_fused&quot;</code></strong>: 优化器（Optimizer）。用于根据计算出的梯度来更新模型权重的算法。”adamw” (Adam with weight decay) 是目前非常流行的优化器。”torch_fused” 可能指使用了 PyTorch 提供的更高效的、融合了多个操作的版本。</li>
<li><strong><code>logging_steps = 1</code></strong>: 日志记录步数。每隔多少个训练步（更新步）记录一次训练信息（如损失值、学习率等）。<code>1</code> 表示每一步都记录。</li>
<li><strong><code>per_device_train_batch_size = 1</code></strong>: 每个设备（GPU&#x2F;CPU）的训练批次大小。表示在单个设备上一次处理多少个训练样本。<code>1</code> 是非常小的批次大小，通常在模型很大或显存有限时使用。</li>
<li><strong><code>gradient_accumulation_steps = 4</code></strong>: 梯度累积步数。为了在不增加显存占用的情况下模拟更大的批次大小。计算 <code>per_device_train_batch_size</code> (1) 个样本的梯度，但不立即更新模型，而是累积 <code>gradient_accumulation_steps</code> (4) 次的梯度后，才执行一次模型更新。<strong>有效批次大小</strong> &#x3D; <code>per_device_train_batch_size * &lt;设备数量&gt; * gradient_accumulation_steps</code>。这里的设置为 <code>1 * &lt;设备数量&gt; * 4</code>。注释“Increase to 4 for smoother training”表明目标是达到等效于批次大小为 4 的训练效果。</li>
<li><strong><code>num_generations = 8</code></strong>: 生成数量。在 GRPO 或类似 RLHF 的训练中，模型需要为每个输入 prompt 生成多个可能的完成（completions）。这个参数指定了为每个 prompt 生成多少个不同的完成。注释“Decrease if out of memory”说明生成更多完成会消耗更多显存。</li>
<li><strong><code>max_prompt_length = max_prompt_length</code></strong>: 设置输入提示的最大 token 长度，使用了我们之前计算的变量 <code>max_prompt_length</code> (值为 288)。超过这个长度的提示可能会被截断。</li>
<li><strong><code>max_completion_length = max_seq_length - max_prompt_length</code></strong>: 设置模型生成的完成部分的最大 token 长度。它的值是根据一个（必须在别处定义的）<code>max_seq_length</code> 变量（代表模型能处理的总序列长度）减去提示的最大长度计算得到的。这确保了 <code>提示 + 完成</code> 的总长度不会超过模型的处理能力。</li>
<li><strong><code># num_train_epochs = 1</code></strong>: <strong>被注释掉了</strong>。如果启用，这个参数会指定模型需要完整地遍历整个训练数据集多少次（称为一个 epoch）。</li>
<li><strong><code>max_steps = 1000</code></strong>: 最大训练步数。指定了训练过程总共要执行多少次模型<strong>更新</strong>（注意是更新步，不是样本处理步）。如果设置了 <code>max_steps</code>，它通常会覆盖 <code>num_train_epochs</code>。这里表示训练将在 1000 次更新后停止。</li>
<li><strong><code>save_steps = 250</code></strong>: 保存步数。每隔多少个训练步（更新步）保存一次模型的检查点（checkpoint）。检查点是模型状态的快照，可以用来稍后恢复训练或用于推理。这里表示每 250 步保存一次。</li>
<li><strong><code>max_grad_norm = 0.1</code></strong>: 最大梯度范数（Gradient Clipping）。一种防止梯度爆炸（梯度值变得非常大导致训练不稳定）的技术。如果计算出的梯度的范数（可以理解为梯度向量的“长度”）超过了这个阈值（0.1），就会将其缩放到这个阈值。0.1 是一个相对较严格的裁剪值。</li>
<li><strong><code>report_to = &quot;none&quot;</code></strong>: 报告目标。指定将训练过程中的日志和指标发送到哪里。”none” 表示不发送到任何外部服务。常用的选项还包括 <code>&quot;wandb&quot;</code> (Weights &amp; Biases) 或 <code>&quot;tensorboard&quot;</code>，这些是用于可视化和跟踪实验的工具。</li>
<li><strong><code>output_dir = &quot;outputs&quot;</code></strong>: 输出目录。指定训练过程中产生的输出文件（如模型检查点、日志文件等）保存到哪个文件夹。这里是名为 “outputs” 的文件夹。</li>
</ul>
<p><strong>总结:</strong></p>
<p>这段代码精心配置了用于 <code>GRPO</code> 训练过程的大量参数，涵盖了学习率、优化器、批次大小、序列长度、训练时长、保存策略、日志记录等多个方面。这些设置将被打包到 <code>training_args</code> 对象中，然后传递给 <code>GRPOTrainer</code>，指导其如何高效、稳定地进行模型微调。</p>
<hr>
<h3 id="第十六块：创建GRPO训练器"><a href="#第十六块：创建GRPO训练器" class="headerlink" title="第十六块：创建GRPO训练器"></a><strong>第十六块：创建GRPO训练器</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">trainer = GRPOTrainer(</span><br><span class="line">    model = model,</span><br><span class="line">    processing_class = tokenizer,</span><br><span class="line">    reward_funcs = [</span><br><span class="line">        match_format_exactly,</span><br><span class="line">        match_format_approximately,</span><br><span class="line">        check_answer,</span><br><span class="line">        check_numbers,</span><br><span class="line">    ],</span><br><span class="line">    args = training_args,</span><br><span class="line">    train_dataset = dataset,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<p>这段代码的核心作用是<strong>创建并启动一个 <code>GRPO</code> 训练器</strong>，使用之前定义的模型、分词器、奖励函数和训练参数来对数据集进行训练。</p>
<p>带注释：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">trainer = GRPOTrainer(</span><br><span class="line">    model = model,</span><br><span class="line">    <span class="comment"># 1. model:</span></span><br><span class="line">    <span class="comment">#    - 传入你想要训练的模型对象。这通常是一个预训练好的、支持生成任务的 Transformer 模型</span></span><br><span class="line">    <span class="comment">#      （比如从 Hugging Face Hub 加载的 GPT、LLaMA 或其他类似模型），可能已经进行了一些初步的微调。</span></span><br><span class="line">    <span class="comment">#    - GRPOTrainer 将会调整这个模型的参数。</span></span><br><span class="line"></span><br><span class="line">    processing_class = tokenizer,</span><br><span class="line">    <span class="comment"># 2. processing_class:</span></span><br><span class="line">    <span class="comment">#    - 传入用于文本处理的对象，通常就是分词器 (tokenizer)对象。</span></span><br><span class="line">    <span class="comment">#    - 训练器内部需要用它来将文本数据转换成模型可以理解的 token IDs，</span></span><br><span class="line">    <span class="comment">#      以及可能将模型的输出 token IDs 解码回文本。</span></span><br><span class="line">    <span class="comment">#    - 命名为 processing_class 可能暗示它也可以是更复杂的处理类，但常见用法是传入 tokenizer。</span></span><br><span class="line"></span><br><span class="line">    reward_funcs = [</span><br><span class="line">        match_format_exactly,</span><br><span class="line">        match_format_approximately,</span><br><span class="line">        check_answer,</span><br><span class="line">        check_numbers,</span><br><span class="line">    ],</span><br><span class="line">    <span class="comment"># 3. reward_funcs:</span></span><br><span class="line">    <span class="comment">#    - 传入一个包含多个奖励函数 (reward function) 的列表。</span></span><br><span class="line">    <span class="comment">#    - 在 GRPO（以及类似的基于强化学习或偏好的训练方法）中，模型会为每个 prompt 生成多个响应。</span></span><br><span class="line">    <span class="comment">#      这些奖励函数的作用是评估每个生成的响应的质量，并给出一个或多个数值分数（奖励）。</span></span><br><span class="line">    <span class="comment">#    - 训练器会利用这些奖励分数来调整模型，使其更倾向于生成获得高奖励的响应。</span></span><br><span class="line">    <span class="comment">#    - 这里列出的四个函数 (match_format_exactly, match_format_approximately, check_answer, check_numbers) </span></span><br><span class="line">    <span class="comment">#      就是我们之前讨论过的，它们分别从不同角度（格式精确性、格式近似性、答案准确性、数值准确性）</span></span><br><span class="line">    <span class="comment">#      来评价生成的响应。训练器可能会组合使用这些函数的输出来计算最终的奖励信号。</span></span><br><span class="line"></span><br><span class="line">    args = training_args,</span><br><span class="line">    <span class="comment"># 4. args:</span></span><br><span class="line">    <span class="comment">#    - 传入我们之前创建的 `GRPOConfig` 对象(`training_args`)。</span></span><br><span class="line">    <span class="comment">#    - 这个对象包含了所有关于训练过程的配置参数（学习率、批次大小、训练步数、保存间隔等等）。</span></span><br><span class="line">    <span class="comment">#    - 训练器会读取这个配置对象来确定如何执行训练。</span></span><br><span class="line"></span><br><span class="line">    train_dataset = dataset,</span><br><span class="line">    <span class="comment"># 5. train_dataset:</span></span><br><span class="line">    <span class="comment">#    - 传入用于训练的数据集对象。</span></span><br><span class="line">    <span class="comment">#    - 这个数据集应该包含了模型需要学习的 prompt（可能还有对应的期望答案，供奖励函数使用）。</span></span><br><span class="line">    <span class="comment">#    - 训练器会从这个数据集中抽取样本进行训练。</span></span><br><span class="line"></span><br><span class="line">) <span class="comment"># GRPOTrainer 初始化结束</span></span><br><span class="line"></span><br><span class="line">trainer.train()</span><br><span class="line"><span class="comment"># 6. trainer.train():</span></span><br><span class="line"><span class="comment">#    - 调用创建好的 `trainer` 对象的 `train()` 方法。</span></span><br><span class="line"><span class="comment">#    - 作用: 启动并执行整个 GRPO 训练过程。</span></span><br><span class="line"><span class="comment">#    - 内部流程 (大致):</span></span><br><span class="line"><span class="comment">#        1. 训练器会根据 `training_args` 中的设置（如批次大小、梯度累积步数）从 `train_dataset` 中加载数据。</span></span><br><span class="line"><span class="comment">#        2. 对于每个 prompt，使用当前的model生成num_generations个不同的响应（completions）。</span></span><br><span class="line"><span class="comment">#        3. 将生成的响应和对应的 prompt（以及可能的标准答案）传递给reward_funcs列表中的每一个奖励函数，得到每个响应的多个奖励分数。</span></span><br><span class="line"><span class="comment">#        4. GRPOTrainer 使用 GRPO 算法，根据这些奖励分数（比较不同生成结果的好坏）来计算损失，并计算相对于模型参数的梯度。</span></span><br><span class="line"><span class="comment">#        5. 使用指定的优化器 (optim) 和学习率调度策略 (lr_scheduler_type)，根据累积的梯度来更新 model的参数。</span></span><br><span class="line"><span class="comment">#        6. 重复步骤 1-5，直到达到max_steps或其他停止条件。</span></span><br><span class="line"><span class="comment">#        7. 在训练过程中，按照logging_steps记录日志，并按照save_steps保存模型检查点到output_dir。</span></span><br></pre></td></tr></table></figure>

<p>这段代码做了两件主要事情：</p>
<ol>
<li><strong>配置和初始化 <code>GRPOTrainer</code></strong>: 它将训练所需的所有组件（要训练的模型、处理文本的分词器、评估生成质量的奖励函数、详细的训练设置、以及训练数据）组装在一起，创建了一个功能完备的训练器对象 <code>trainer</code>。</li>
<li><strong>启动训练</strong>: 调用 <code>trainer.train()</code> 方法，让训练器按照预设的配置和 <code>GRPO</code> 算法，开始迭代地从数据中学习，通过生成响应、评估奖励、计算损失和更新模型参数的过程，来优化模型，使其能够生成更符合奖励函数期望（即更高质量）的输出。</li>
</ol>
<hr>
<h3 id="第十七块：原模型推理"><a href="#第十七块：原模型推理" class="headerlink" title="第十七块：原模型推理"></a><strong>第十七块：原模型推理</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">text = tokenizer.apply_chat_template([</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the sqrt of 101?&quot;</span>&#125;,</span><br><span class="line">], tokenize = <span class="literal">False</span>, add_generation_prompt = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> SamplingParams</span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature = <span class="number">0.8</span>,</span><br><span class="line">    top_p = <span class="number">0.95</span>,</span><br><span class="line">    max_tokens = <span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line">output = model.fast_generate(</span><br><span class="line">    [text],</span><br><span class="line">    sampling_params = sampling_params,</span><br><span class="line">    lora_request = <span class="literal">None</span>,</span><br><span class="line">)[<span class="number">0</span>].outputs[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The square root of 101 is approximately 10.05.</span><br></pre></td></tr></table></figure>

<p>这段代码的目的是使用一个预先加载好的分词器 (<code>tokenizer</code>) 和一个专门为 <code>vLLM</code> 优化的模型 (model)，根据给定的用户提示，生成一段文本回复。</p>
<ul>
<li><strong><code>tokenizer.apply_chat_template(...)</code></strong>: 这个方法的作用是将对话历史（这里只有一个用户回合）按照特定模型预设的“聊天模板”格式化。不同的模型有不同的格式要求，例如可能包含特殊的标记来区分用户 (user) 和助手 (assistant) 的发言。</li>
<li><strong>[{“role”: “user”, “content”: “What is the sqrt of 101?”}]</strong>: 这是输入的对话内容，一个包含单条用户消息的列表。</li>
<li><strong><code>tokenize = False</code></strong>: 这个参数非常重要。它指示 apply_chat_template <strong>不要</strong>将格式化后的文本转换成 token ID 列表，而是<strong>返回一个格式化好的字符串</strong>。这个字符串会包含模板定义的特殊标记和文本内容。例如，结果可能类似于 “&lt;s&gt;[INST] What is the sqrt of 101? [&#x2F;INST]” 或其他根据模型模板定义的格式。</li>
<li><strong>add_generation_prompt &#x3D; True</strong>: 这个参数通常会在格式化后的字符串末尾添加一个提示，告诉模型现在轮到它（助手&#x2F;assistant）开始生成回应了。例如，在上面的例子后面可能会加上 “ （如果这是 Llama 2 的模板）。</li>
<li><strong><code>text = ...</code></strong>: 将最终格式化好的、准备输入给模型的<strong>字符串</strong>存储在变量 text 中。</li>
<li><strong>from vllm import SamplingParams</strong>: 从<code> vllm</code> 库导入 <code>SamplingParams</code> 类。<code>vllm</code> 是一个用于快速、高效地进行 <code>LLM</code>（大型语言模型）推理的库。<code>SamplingParams</code> 这个类专门用来控制模型在生成文本时如何选择下一个词（<code>token</code>）</li>
<li><strong><code>sampling_params = SamplingParams(...)</code></strong>: 创建 <code>SamplingParams</code> 类的一个实例，用来定义文本生成的策略。</li>
<li><strong><code>temperature = 0.8</code></strong>: 控制生成文本的随机性。<ul>
<li>值越高（比如 <code>0.8</code>），生成的文本越随机、越有创意，但也可能越不连贯。</li>
<li>值越低（比如 <code>0.2</code>），生成的文本越确定性、越集中于模型认为最可能的词，但也可能越重复、越缺乏多样性。</li>
<li><code>0.8</code> 是一个相对较高的温度，倾向于生成更多样化的内容。</li>
</ul>
</li>
<li><strong>top_p &#x3D; 0.95</strong>: 使用 <code>Nucleus Sampling</code> (核采样) 策略。在每一步选择下一个词时，模型会按概率从高到低排序所有可能的词，然后只考虑累积概率达到 <code>0.95 (95%)</code> 的那些最可能的词进行采样。这有助于排除那些概率极低、可能导致奇怪结果的词，同时仍然保持一定的多样性（比 <code>top_k</code> 更灵活）。</li>
<li><strong>max_tokens &#x3D; 1024</strong>: 设置模型生成的<strong>最大 token 数量</strong>。这是指模型在原始输入 text 之后<strong>新生成</strong>的 token 数量上限。防止生成无限长的文本。</li>
<li><strong>model.fast_generate(…)</strong>: 调用 model 对象（假设这是一个用 vllm 加载或包装的模型）的 fast_generate 方法来执行文本生成。这个方法利用了 vLLM 的优化技术（如 PagedAttention）来加速推理过程。</li>
<li><strong>[text]</strong>: 输入的 prompt。注意，即使只有一个 prompt，通常也需要将其放入一个<strong>列表</strong>中传递给生成方法。</li>
<li><strong>sampling_params &#x3D; sampling_params</strong>: 将我们之前配置好的 sampling_params 对象传递进去，告诉生成器使用这些参数来控制生成过程。</li>
<li><strong>lora_request &#x3D; None</strong>: vLLM 支持动态加载 LoRA 适配器。这里设置为 None 表示不使用任何特定的 LoRA 适配器进行此次生成。</li>
<li><strong>[…] (方法调用结束后的部分)</strong>: 这部分用来从生成结果中提取出我们想要的文本。<ul>
<li>[0]: fast_generate (或 vLLM 的 generate) 方法返回一个列表，列表中的每个元素对应输入列表中的一个 prompt 的结果。因为我们只输入了一个 prompt ([text])，所以我们取结果列表的第一个元素 [0]。这个元素通常是一个 RequestOutput 对象。</li>
<li>.outputs: 每个 RequestOutput 对象包含一个名为 outputs 的列表，里面是为该 prompt 生成的一个或多个完成序列（可以通过 SamplingParams 中的 n 参数控制生成多个）。</li>
<li>[0]: 我们取 outputs 列表中的第一个完成序列 [0]（假设 n&#x3D;1，这是默认情况）。这个元素通常是一个 CompletionOutput 对象。</li>
<li>.text: CompletionOutput 对象有一个 text 属性，它包含了最终生成的<strong>文本字符串</strong>。</li>
</ul>
</li>
<li><strong>output &#x3D; …</strong>: 将提取出的生成文本字符串存储在变量 output 中。</li>
</ul>
<p>这段代码完整地演示了使用 vllm 进行文本生成的基本流程：</p>
<ol>
<li>使用与模型匹配的 tokenizer 将用户输入格式化为模型期望的<strong>字符串</strong>格式（包括聊天模板和生成提示）。</li>
<li>定义 SamplingParams 来控制生成的多样性、长度等。</li>
<li>调用 vllm 优化后的模型生成方法 (fast_generate 或类似方法），传入格式化后的 prompt 字符串列表和采样参数。</li>
<li>从复杂的返回结构中解析并提取出最终生成的文本字符串。</li>
</ol>
<hr>
<h3 id="第十八块：lora权重保存和验证"><a href="#第十八块：lora权重保存和验证" class="headerlink" title="第十八块：lora权重保存和验证"></a><strong>第十八块：lora权重保存和验证</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model.save_lora(<span class="string">&quot;grpo_saved_lora&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> safetensors <span class="keyword">import</span> safe_open</span><br><span class="line"></span><br><span class="line">tensors = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> safe_open(<span class="string">&quot;grpo_saved_lora/adapter_model.safetensors&quot;</span>, framework = <span class="string">&quot;pt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="comment"># Verify both A and B are non zero</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> f.keys():</span><br><span class="line">tensor = f.get_tensor(key)</span><br><span class="line">n_zeros = (tensor == <span class="number">0</span>).<span class="built_in">sum</span>() / tensor.numel()</span><br><span class="line"><span class="keyword">assert</span>(n_zeros.item() != tensor.numel())</span><br></pre></td></tr></table></figure>

<p>这段代码首先将训练得到的 <code>LoRA</code> 权重保存到 <code>grpo_saved_lora</code> 目录下的 <code>adapter_model.safetensors</code> 文件中。然后，它使用 <code>safetensors</code> 库重新打开这个文件，逐一加载其中的每个权重张量，并执行一个断言检查。这个断言的目的是进行某种形式的验证，<strong>很可能是想确保加载的张量不是完全由零组成的</strong>，但实际的检查逻辑 <code>(proportion != total_count)</code> 写得有些奇怪，可能无法完全达到预期的验证效果。</p>
<h3 id="第十九块：微调后模型推理"><a href="#第十九块：微调后模型推理" class="headerlink" title="第十九块：微调后模型推理"></a><strong>第十九块：微调后模型推理</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_prompt&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,   <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the sqrt of 101?&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>, <span class="comment"># Must add for generation</span></span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> SamplingParams</span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature = <span class="number">0.8</span>,</span><br><span class="line">    top_p = <span class="number">0.95</span>,</span><br><span class="line">    max_tokens = <span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line">output = model.fast_generate(</span><br><span class="line">    text,</span><br><span class="line">    sampling_params = sampling_params,</span><br><span class="line">    lora_request = model.load_lora(<span class="string">&quot;grpo_saved_lora&quot;</span>),</span><br><span class="line">)[<span class="number">0</span>].outputs[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;<span class="keyword">\nTo</span> find the square root of 101, I can use a calculator or a mathematical algorithm. Since 101 is not a perfect square, I will use a numerical method to approximate the square root.<span class="keyword">\n</span><span class="keyword">\nUsing</span> a calculator, I can find that the square root of 101 is approximately 10.05.<span class="keyword">\nAlternatively</span>, I can also use the Babylonian method, which is an iterative method for computing the square root of a number. However, this method requires multiple iterations and may not be necessary for a simple calculation like this.<span class="keyword">\n</span><span class="keyword">\nIn</span> this case, the approximate value of the square root of 101 is 10.0498756791108427 (using a more precise calculation). However, for most practical purposes, the value 10.05 is sufficient.<span class="keyword">\n</span><span class="keyword">\nSo</span>, the square root of 101 is approximately 10.05.<span class="keyword">\n</span>&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;<span class="keyword">\n</span><span class="keyword">\n</span>&lt;SOLUTION&gt;10.05&lt;/SOLUTION&gt;&#x27;</span><br><span class="line"></span><br><span class="line"><span class="params">#</span> 格式化后</span><br><span class="line">&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;</span><br><span class="line">To find the square root of 101, I can use a calculator or a mathematical algorithm. Since 101 is not a perfect square, I will use a numerical method to approximate the square root.</span><br><span class="line"></span><br><span class="line">Using a calculator, I can find that the square root of 101 is approximately 10.05.</span><br><span class="line">Alternatively, I can also use the Babylonian method, which is an iterative method for computing the square root of a number. However, this method requires multiple iterations and may not be necessary for a simple calculation like this.</span><br><span class="line"></span><br><span class="line">In this case, the approximate value of the square root of 101 is 10.0498756791108427 (using a more precise calculation). However, for most practical purposes, the value 10.05 is sufficient.</span><br><span class="line"></span><br><span class="line">So, the square root of 101 is approximately 10.05.</span><br><span class="line">&lt;end<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;</span><br><span class="line"></span><br><span class="line">&lt;SOLUTION&gt;10.05&lt;/SOLUTION&gt;</span><br></pre></td></tr></table></figure>

<p>代码与第十六块类似，只是：</p>
<ul>
<li>加了系统提示词（<code>system_prompt</code>）</li>
<li>动态加载了刚才保存的<code>lora</code>权重。</li>
</ul>
<h3 id="第二十块：保存模型（vLLM）"><a href="#第二十块：保存模型（vLLM）" class="headerlink" title="第二十块：保存模型（vLLM）"></a><strong>第二十块：保存模型（vLLM）</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save_pretrained_merged(<span class="string">&quot;model&quot;</span>, tokenizer, save_method = <span class="string">&quot;merged_16bit&quot;</span>,)</span><br></pre></td></tr></table></figure>

<p>以<code>float16</code>精度保存微调后的模型到<code>model</code>文件夹。</p>
<p><strong>然后就可以通过下面命令启动模型服务了：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m vllm.entrypoints.openai.api_server --model model_llama3_2_3B/  --served-model-name llama3<span class="number">.2</span> --<span class="built_in">max</span>-model-<span class="built_in">len</span>=<span class="number">2048</span></span><br></pre></td></tr></table></figure>

<h3 id="补充1：原模型-系统提示词推理"><a href="#补充1：原模型-系统提示词推理" class="headerlink" title="补充1：原模型+系统提示词推理"></a><strong>补充1：原模型+系统提示词推理</strong></h3><p>我觉得有必要补充一个输出与第十九条输出对比：</p>
<ul>
<li>添加系统提示词</li>
<li>使用原模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_prompt&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,   <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the sqrt of 101?&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    add_generation_prompt = <span class="literal">True</span>, <span class="comment"># Must add for generation</span></span><br><span class="line">    tokenize = <span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> SamplingParams</span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature = <span class="number">0.8</span>,</span><br><span class="line">    top_p = <span class="number">0.95</span>,</span><br><span class="line">    max_tokens = <span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line">output = model.fast_generate(</span><br><span class="line">    text,</span><br><span class="line">    sampling_params = sampling_params,</span><br><span class="line">    lora_request = <span class="literal">None</span>,</span><br><span class="line">)[<span class="number">0</span>].outputs[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">output</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&quot;&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;<span class="keyword">\nTo</span> find the square root of 101, we need to find a number that, when multiplied by itself, equals 101. This can be done through trial and error or using a calculator.<span class="keyword">\n</span><span class="keyword">\nOne</span> way to do this is to start with a number and multiply it by itself, then check if the result is close to 101. If it&#x27;s not, we can adjust our guess and try again.<span class="keyword">\n</span><span class="keyword">\nFor</span> example, we can start with 10:<span class="keyword">\n</span>10 * 10 = 100<span class="keyword">\n</span><span class="keyword">\nSince</span> 100 is less than 101, we can try a number larger than 10. Let&#x27;s try 11:<span class="keyword">\n</span>11 * 11 = 121<span class="keyword">\n</span><span class="keyword">\nSince</span> 121 is greater than 101, we know that the square root of 101 is between 10 and 11.<span class="keyword">\n</span><span class="keyword">\nNow</span>, we can try numbers in between 10 and 11, such as 10.5 or 10.9, to get a more accurate estimate.<span class="keyword">\n</span><span class="keyword">\n</span>10.5 * 10.5 = 110.25 (too high)<span class="keyword">\n</span>10.9 * 10.9 = 118.81 (too high)<span class="keyword">\n</span><span class="keyword">\nSo</span>, the square root of 101 is between 10.5 and 10.9.<span class="keyword">\n</span><span class="keyword">\nNext</span>, we can try numbers between 10.5 and 10.9, such as 10.6:<span class="keyword">\n</span>10.6 * 10.6 = 111.76 (too high)<span class="keyword">\n</span><span class="keyword">\nWe</span> can keep trying numbers between 10.5 and 10.9 to get a more accurate estimate. However, this can be a time-consuming process.<span class="keyword">\n</span><span class="keyword">\nA</span> more efficient way to find the square root of 101 is to use a calculator or a computer program.<span class="keyword">\n</span><span class="keyword">\n</span>&lt;SOLUTION&gt;<span class="keyword">\nThe</span> square root of 101 is approximately 10.0498756, which can be rounded to 10.05.&quot;</span><br><span class="line"></span><br><span class="line"><span class="params">#</span> 格式化后</span><br><span class="line">&lt;start<span class="built_in">_</span>working<span class="built_in">_</span>out&gt;</span><br><span class="line">To find the square root of 101, we need to find a number that, when multiplied by itself, equals 101. This can be done through trial and error or using a calculator.</span><br><span class="line"></span><br><span class="line">One way to do this is to start with a number and multiply it by itself, then check if the result is close to 101. If it&#x27;s not, we can adjust our guess and try again.</span><br><span class="line"></span><br><span class="line">For example, we can start with 10:</span><br><span class="line">10 * 10 = 100</span><br><span class="line"></span><br><span class="line">Since 100 is less than 101, we can try a number larger than 10. Let&#x27;s try 11:</span><br><span class="line">11 * 11 = 121</span><br><span class="line"></span><br><span class="line">Since 121 is greater than 101, we know that the square root of 101 is between 10 and 11.</span><br><span class="line"></span><br><span class="line">Now, we can try numbers in between 10 and 11, such as 10.5 or 10.9, to get a more accurate estimate.</span><br><span class="line"></span><br><span class="line">10.5 * 10.5 = 110.25 (too high)</span><br><span class="line">10.9 * 10.9 = 118.81 (too high)</span><br><span class="line"></span><br><span class="line">So, the square root of 101 is between 10.5 and 10.9.</span><br><span class="line"></span><br><span class="line">Next, we can try numbers between 10.5 and 10.9, such as 10.6:</span><br><span class="line">10.6 * 10.6 = 111.76 (too high)</span><br><span class="line"></span><br><span class="line">We can keep trying numbers between 10.5 and 10.9 to get a more accurate estimate. However, this can be a time-consuming process.</span><br><span class="line"></span><br><span class="line">A more efficient way to find the square root of 101 is to use a calculator or a computer program.</span><br><span class="line"></span><br><span class="line">&lt;SOLUTION&gt;</span><br><span class="line">The square root of 101 is approximately 10.0498756, which can be rounded to 10.05.</span><br></pre></td></tr></table></figure>

<ul>
<li>好像答得也有模有样</li>
<li>但并未完全按提示词走（只有推理和答案的开头一个标志，没有结束标志）</li>
</ul>
<h3 id="补充2：9-11和9-9谁大"><a href="#补充2：9-11和9-9谁大" class="headerlink" title="补充2：9.11和9.9谁大"></a><strong>补充2：9.11和9.9谁大</strong></h3><p>我觉得上述问题可能过于基础，换个更高级的问题并用双语各测试5次：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Which is bigger: 9.11 or 9.9?</span><br><span class="line">9.11和9.9哪个大？</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th></th>
<th>zh</th>
<th>en</th>
</tr>
</thead>
<tbody><tr>
<td>原模型+无提示词</td>
<td>1对，2错，2胡言乱语（说不能比较大小）</td>
<td>5错</td>
</tr>
<tr>
<td>微调模型+提示词</td>
<td>5错4</td>
<td>5错</td>
</tr>
<tr>
<td>原模型+无提示词</td>
<td>5错4</td>
<td>5错</td>
</tr>
<tr>
<td>微调模型+无提示词</td>
<td>5错4</td>
<td>5错</td>
</tr>
</tbody></table>
<blockquote>
<p>且微调模型+无提示词场景下，试了很多类型的问题都没有推理过程。</p>
</blockquote>
<p><strong>嗯？Excuse me? Are you kidding me? 🤷‍♂️</strong></p>
<p>难道是因为3B模型太小或者模型结构的事？（暂时没有答案）</p>
<hr>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="GSM8K"><a href="#GSM8K" class="headerlink" title="GSM8K"></a>GSM8K</h3><p>GSM8K (Grade School Math 8K, 小学数学8K)是一个包含8.5K道高质量、语言多样化（不是多语种，而是表达形式&#x2F;风格多样）的小学数学应用题的数据集。该数据集旨在支持需要多步骤推理的基本数学问题的问答任务。</p>
<ul>
<li>解决这些问题需要 2 到 8 个步骤。</li>
<li>解决方案主要涉及使用基本算术运算（+ − ×÷）执行一系列基本计算以得出最终答案。</li>
<li>一个聪明的中学生应该能够解决所有问题：从论文中可以看出，“问题不需要超出早期代数水平的概念，并且绝大多数问题可以在不明确定义变量的情况下解决。</li>
<li>解决方案以自然语言提供，而非纯数学表达式。论文中写道：“我们相信这是最通用的数据格式，并期望它能够揭示大型语言模型内部独白的属性。</li>
</ul>
<p><strong>数据实例</strong></p>
<p>对于<code>main</code>配置，每个实例包含一个小学水平数学问题的字符串和一个相应答案的字符串，其中包含多个推理步骤和计算器注释（<a target="_blank" rel="noopener" href="https://github.com/openai/grade-school-math#calculation-annotations">此处</a>解释）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;question&#x27;</span>: <span class="string">&#x27;Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May.\nNatalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May.\n#### 72&#x27;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="计算器机制"><a href="#计算器机制" class="headerlink" title="计算器机制"></a>计算器机制</h3><p>详见<a target="_blank" rel="noopener" href="https://github.com/openai/grade-school-math/blob/master/grade_school_math/calculator.py">calculator.py</a>，这里仅举例说明，</p>
<p><strong>示例场景</strong></p>
<p><strong>输入模型提示</strong>：<br><code>&quot;Natalia sold 48 clips in April. &lt;&lt;48/2=&quot;</code></p>
<p><strong>模型生成步骤</strong>：</p>
<ol>
<li>检测到 <code>&lt;&lt;48/2=</code> ，触发计算器。</li>
<li>计算 <code>48/2</code> 得到 <code>24</code>。</li>
<li>修改文本为 <code>&quot;Natalia sold 48 clips in April. &lt;&lt;48/2=24&gt;&gt;&quot;</code>。</li>
<li>继续生成后续内容（如 <code>&quot; She sold 24 in May.&quot;</code>）。</li>
</ol>
<h3 id="训练日志"><a href="#训练日志" class="headerlink" title="训练日志"></a>训练日志</h3><p>先写结论：</p>
<ul>
<li><p>训练了11h:21min:42s</p>
</li>
<li><p>比<a href="https://caihaoran-00.github.io/2025/04/08/Unsloth-Phi-4-GRPO%E5%B0%86%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B/">Phi-4</a>的微调的数据看着舒服多了</p>
</li>
<li><p>match_format_exactly和check_answer在150步左右才开始正常。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>Step</th>
<th>Training Loss</th>
<th>reward</th>
<th>reward_std</th>
<th>completion_length</th>
<th>kl</th>
<th>rewards &#x2F; match_format_exactly</th>
<th>rewards &#x2F; match_format_approximately</th>
<th>rewards &#x2F; check_answer</th>
<th>rewards &#x2F; check_numbers</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.679083</td>
<td>257.906250</td>
<td>0.000000</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.140625</td>
</tr>
<tr>
<td>2</td>
<td>-0.000000</td>
<td>-0.906250</td>
<td>0.590790</td>
<td>223.750000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.187500</td>
</tr>
<tr>
<td>3</td>
<td>0.000000</td>
<td>-0.796875</td>
<td>0.925001</td>
<td>204.218750</td>
<td>0.000339</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.203125</td>
</tr>
<tr>
<td>4</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>0.885618</td>
<td>214.781250</td>
<td>0.000319</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.000000</td>
<td>0.234375</td>
</tr>
<tr>
<td>5</td>
<td>0.000000</td>
<td>-0.796875</td>
<td>0.864609</td>
<td>262.750000</td>
<td>0.000344</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>6</td>
<td>0.000000</td>
<td>-1.125000</td>
<td>0.638588</td>
<td>203.687500</td>
<td>0.000307</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.000000</td>
<td>0.015625</td>
</tr>
<tr>
<td>7</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.585080</td>
<td>190.531250</td>
<td>0.000364</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.187500</td>
</tr>
<tr>
<td>8</td>
<td>0.000000</td>
<td>-0.609375</td>
<td>1.105576</td>
<td>206.593750</td>
<td>0.000285</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.343750</td>
</tr>
<tr>
<td>9</td>
<td>0.000000</td>
<td>-0.500000</td>
<td>0.974687</td>
<td>174.375000</td>
<td>0.000317</td>
<td>0.000000</td>
<td>-0.812500</td>
<td>0.000000</td>
<td>0.312500</td>
</tr>
<tr>
<td>10</td>
<td>0.000000</td>
<td>-0.968750</td>
<td>1.017919</td>
<td>245.843750</td>
<td>0.000342</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.218750</td>
</tr>
<tr>
<td>11</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>1.020435</td>
<td>228.343750</td>
<td>0.000307</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.375000</td>
</tr>
<tr>
<td>12</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>1.199693</td>
<td>230.250000</td>
<td>0.000305</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.328125</td>
</tr>
<tr>
<td>13</td>
<td>0.000000</td>
<td>-0.781250</td>
<td>0.803784</td>
<td>245.781250</td>
<td>0.000291</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.312500</td>
</tr>
<tr>
<td>14</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.949337</td>
<td>235.656250</td>
<td>0.000315</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.187500</td>
</tr>
<tr>
<td>15</td>
<td>0.000000</td>
<td>-0.812500</td>
<td>1.048837</td>
<td>225.031250</td>
<td>0.000293</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.281250</td>
</tr>
<tr>
<td>16</td>
<td>0.000000</td>
<td>-1.359375</td>
<td>0.987504</td>
<td>251.281250</td>
<td>0.000342</td>
<td>0.000000</td>
<td>-1.656250</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>17</td>
<td>0.000000</td>
<td>-1.015625</td>
<td>0.624389</td>
<td>226.531250</td>
<td>0.000454</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.031250</td>
</tr>
<tr>
<td>18</td>
<td>0.000000</td>
<td>-0.890625</td>
<td>0.861584</td>
<td>180.625000</td>
<td>0.000276</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>19</td>
<td>0.000000</td>
<td>-1.125000</td>
<td>0.916903</td>
<td>328.000000</td>
<td>0.000279</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.000000</td>
<td>0.015625</td>
</tr>
<tr>
<td>20</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.678592</td>
<td>252.843750</td>
<td>0.000354</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.187500</td>
</tr>
<tr>
<td>21</td>
<td>0.000000</td>
<td>-1.359375</td>
<td>1.062030</td>
<td>229.031250</td>
<td>0.000553</td>
<td>0.000000</td>
<td>-1.468750</td>
<td>0.000000</td>
<td>0.109375</td>
</tr>
<tr>
<td>22</td>
<td>0.000000</td>
<td>-0.968750</td>
<td>1.232316</td>
<td>202.625000</td>
<td>0.000422</td>
<td>0.000000</td>
<td>-1.281250</td>
<td>0.000000</td>
<td>0.312500</td>
</tr>
<tr>
<td>23</td>
<td>0.000000</td>
<td>-0.609375</td>
<td>1.146797</td>
<td>170.500000</td>
<td>0.000443</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.000000</td>
<td>0.531250</td>
</tr>
<tr>
<td>24</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.798529</td>
<td>227.812500</td>
<td>0.000416</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.046875</td>
</tr>
<tr>
<td>25</td>
<td>0.000000</td>
<td>-0.687500</td>
<td>1.169120</td>
<td>207.000000</td>
<td>0.000381</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.265625</td>
</tr>
<tr>
<td>26</td>
<td>0.000000</td>
<td>-0.937500</td>
<td>1.390742</td>
<td>193.812500</td>
<td>0.000440</td>
<td>0.093750</td>
<td>-1.187500</td>
<td>-0.046875</td>
<td>0.203125</td>
</tr>
<tr>
<td>27</td>
<td>0.000000</td>
<td>-0.453125</td>
<td>0.810799</td>
<td>179.562500</td>
<td>0.000437</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.000000</td>
<td>0.453125</td>
</tr>
<tr>
<td>28</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>0.864922</td>
<td>172.781250</td>
<td>0.000495</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.000000</td>
<td>0.093750</td>
</tr>
<tr>
<td>29</td>
<td>0.000000</td>
<td>-0.921875</td>
<td>1.147603</td>
<td>191.500000</td>
<td>0.000395</td>
<td>0.000000</td>
<td>-1.234375</td>
<td>0.000000</td>
<td>0.312500</td>
</tr>
<tr>
<td>30</td>
<td>0.000000</td>
<td>-0.703125</td>
<td>0.957798</td>
<td>175.843750</td>
<td>0.000645</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.343750</td>
</tr>
<tr>
<td>31</td>
<td>0.000000</td>
<td>-1.343750</td>
<td>0.901694</td>
<td>260.531250</td>
<td>0.000416</td>
<td>0.000000</td>
<td>-1.421875</td>
<td>0.000000</td>
<td>0.078125</td>
</tr>
<tr>
<td>32</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.868138</td>
<td>185.562500</td>
<td>0.000322</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.328125</td>
</tr>
<tr>
<td>33</td>
<td>0.000000</td>
<td>-1.203125</td>
<td>0.935517</td>
<td>303.968750</td>
<td>0.000383</td>
<td>0.000000</td>
<td>-1.234375</td>
<td>0.000000</td>
<td>0.031250</td>
</tr>
<tr>
<td>34</td>
<td>0.000000</td>
<td>-0.921875</td>
<td>0.908708</td>
<td>223.281250</td>
<td>0.000380</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.171875</td>
</tr>
<tr>
<td>35</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>0.925758</td>
<td>166.718750</td>
<td>0.000794</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.328125</td>
</tr>
<tr>
<td>36</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.922049</td>
<td>227.781250</td>
<td>0.000674</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.046875</td>
</tr>
<tr>
<td>37</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.857178</td>
<td>215.156250</td>
<td>0.000755</td>
<td>0.000000</td>
<td>-1.187500</td>
<td>0.000000</td>
<td>0.281250</td>
</tr>
<tr>
<td>38</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.859795</td>
<td>214.250000</td>
<td>0.000952</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.093750</td>
</tr>
<tr>
<td>39</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.916311</td>
<td>386.625000</td>
<td>0.000497</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.000000</td>
<td>0.187500</td>
</tr>
<tr>
<td>40</td>
<td>0.000000</td>
<td>-1.078125</td>
<td>0.916366</td>
<td>308.968750</td>
<td>0.000879</td>
<td>0.000000</td>
<td>-1.234375</td>
<td>0.000000</td>
<td>0.156250</td>
</tr>
<tr>
<td>41</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>0.464563</td>
<td>266.250000</td>
<td>0.000825</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.140625</td>
</tr>
<tr>
<td>42</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.499462</td>
<td>287.250000</td>
<td>0.000453</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>-0.046875</td>
</tr>
<tr>
<td>43</td>
<td>0.000000</td>
<td>-0.500000</td>
<td>0.679403</td>
<td>208.062500</td>
<td>0.000403</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.500000</td>
</tr>
<tr>
<td>44</td>
<td>0.000000</td>
<td>-0.515625</td>
<td>1.010345</td>
<td>256.781250</td>
<td>0.000640</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.437500</td>
</tr>
<tr>
<td>45</td>
<td>0.000000</td>
<td>-0.625000</td>
<td>1.067974</td>
<td>213.781250</td>
<td>0.000504</td>
<td>0.000000</td>
<td>-1.093750</td>
<td>0.000000</td>
<td>0.468750</td>
</tr>
<tr>
<td>46</td>
<td>0.000100</td>
<td>-0.562500</td>
<td>1.109565</td>
<td>233.718750</td>
<td>0.001255</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.484375</td>
</tr>
<tr>
<td>47</td>
<td>0.000000</td>
<td>-1.390625</td>
<td>1.049854</td>
<td>229.343750</td>
<td>0.000812</td>
<td>0.000000</td>
<td>-1.468750</td>
<td>0.000000</td>
<td>0.078125</td>
</tr>
<tr>
<td>48</td>
<td>0.000100</td>
<td>-0.828125</td>
<td>0.918515</td>
<td>226.843750</td>
<td>0.002188</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.171875</td>
</tr>
<tr>
<td>49</td>
<td>0.000000</td>
<td>-0.734375</td>
<td>0.961471</td>
<td>258.156250</td>
<td>0.001048</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.312500</td>
</tr>
<tr>
<td>50</td>
<td>0.000000</td>
<td>-0.593750</td>
<td>1.028425</td>
<td>209.031250</td>
<td>0.000938</td>
<td>0.093750</td>
<td>-0.812500</td>
<td>-0.046875</td>
<td>0.171875</td>
</tr>
<tr>
<td>51</td>
<td>0.000000</td>
<td>-0.343750</td>
<td>1.082000</td>
<td>183.468750</td>
<td>0.000924</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.609375</td>
</tr>
<tr>
<td>52</td>
<td>0.000000</td>
<td>-0.406250</td>
<td>0.805868</td>
<td>249.750000</td>
<td>0.000597</td>
<td>0.000000</td>
<td>-0.625000</td>
<td>0.000000</td>
<td>0.218750</td>
</tr>
<tr>
<td>53</td>
<td>0.000100</td>
<td>-0.921875</td>
<td>1.036722</td>
<td>191.125000</td>
<td>0.001668</td>
<td>0.000000</td>
<td>-1.328125</td>
<td>0.000000</td>
<td>0.406250</td>
</tr>
<tr>
<td>54</td>
<td>0.000100</td>
<td>-0.687500</td>
<td>1.058039</td>
<td>172.718750</td>
<td>0.001402</td>
<td>0.000000</td>
<td>-1.140625</td>
<td>0.000000</td>
<td>0.453125</td>
</tr>
<tr>
<td>55</td>
<td>0.000100</td>
<td>-0.640625</td>
<td>0.805069</td>
<td>210.156250</td>
<td>0.002699</td>
<td>0.000000</td>
<td>-0.812500</td>
<td>0.000000</td>
<td>0.171875</td>
</tr>
<tr>
<td>56</td>
<td>0.000000</td>
<td>-0.406250</td>
<td>0.868199</td>
<td>207.593750</td>
<td>0.000950</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.593750</td>
</tr>
<tr>
<td>57</td>
<td>0.000100</td>
<td>-0.484375</td>
<td>1.010269</td>
<td>218.000000</td>
<td>0.001974</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.000000</td>
<td>0.421875</td>
</tr>
<tr>
<td>58</td>
<td>0.000100</td>
<td>-0.671875</td>
<td>1.046235</td>
<td>255.750000</td>
<td>0.002728</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.328125</td>
</tr>
<tr>
<td>59</td>
<td>0.000100</td>
<td>-0.171875</td>
<td>1.008117</td>
<td>176.468750</td>
<td>0.002544</td>
<td>0.000000</td>
<td>-1.046875</td>
<td>0.000000</td>
<td>0.875000</td>
</tr>
<tr>
<td>60</td>
<td>0.000100</td>
<td>-0.656250</td>
<td>1.138962</td>
<td>189.625000</td>
<td>0.002432</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.343750</td>
</tr>
<tr>
<td>61</td>
<td>0.000200</td>
<td>-0.187500</td>
<td>1.310412</td>
<td>163.937500</td>
<td>0.004882</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>0.000000</td>
<td>0.671875</td>
</tr>
<tr>
<td>62</td>
<td>0.000100</td>
<td>-0.453125</td>
<td>0.838008</td>
<td>247.625000</td>
<td>0.003543</td>
<td>0.000000</td>
<td>-0.812500</td>
<td>0.000000</td>
<td>0.359375</td>
</tr>
<tr>
<td>63</td>
<td>0.000100</td>
<td>-0.609375</td>
<td>0.856011</td>
<td>302.343750</td>
<td>0.003246</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>64</td>
<td>0.002500</td>
<td>-0.390625</td>
<td>0.850635</td>
<td>229.843750</td>
<td>0.063149</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.000000</td>
<td>0.375000</td>
</tr>
<tr>
<td>65</td>
<td>0.000500</td>
<td>-0.328125</td>
<td>0.895767</td>
<td>224.125000</td>
<td>0.011753</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.625000</td>
</tr>
<tr>
<td>66</td>
<td>0.001000</td>
<td>-0.156250</td>
<td>0.875907</td>
<td>198.093750</td>
<td>0.024031</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>0.000000</td>
<td>0.703125</td>
</tr>
<tr>
<td>67</td>
<td>0.001500</td>
<td>-0.390625</td>
<td>1.233973</td>
<td>205.375000</td>
<td>0.037764</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>0.000000</td>
<td>0.468750</td>
</tr>
<tr>
<td>68</td>
<td>0.000500</td>
<td>-0.375000</td>
<td>1.013449</td>
<td>215.593750</td>
<td>0.012367</td>
<td>0.000000</td>
<td>-0.906250</td>
<td>0.000000</td>
<td>0.531250</td>
</tr>
<tr>
<td>69</td>
<td>0.000500</td>
<td>-0.468750</td>
<td>0.984997</td>
<td>227.500000</td>
<td>0.012935</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>70</td>
<td>0.000200</td>
<td>-0.484375</td>
<td>0.997699</td>
<td>229.875000</td>
<td>0.004841</td>
<td>0.000000</td>
<td>-0.953125</td>
<td>0.000000</td>
<td>0.468750</td>
</tr>
<tr>
<td>71</td>
<td>0.000100</td>
<td>-0.625000</td>
<td>1.067158</td>
<td>299.000000</td>
<td>0.002067</td>
<td>0.000000</td>
<td>-0.718750</td>
<td>0.000000</td>
<td>0.093750</td>
</tr>
<tr>
<td>72</td>
<td>0.000200</td>
<td>-0.390625</td>
<td>0.792165</td>
<td>267.218750</td>
<td>0.004823</td>
<td>0.000000</td>
<td>-1.000000</td>
<td>0.000000</td>
<td>0.609375</td>
</tr>
<tr>
<td>73</td>
<td>0.001500</td>
<td>-0.156250</td>
<td>0.869484</td>
<td>176.875000</td>
<td>0.038043</td>
<td>0.000000</td>
<td>-0.859375</td>
<td>0.000000</td>
<td>0.703125</td>
</tr>
<tr>
<td>74</td>
<td>0.000600</td>
<td>0.453125</td>
<td>1.180293</td>
<td>146.812500</td>
<td>0.013808</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>0.000000</td>
<td>1.125000</td>
</tr>
<tr>
<td>75</td>
<td>0.000500</td>
<td>-0.140625</td>
<td>1.220082</td>
<td>224.000000</td>
<td>0.011601</td>
<td>0.000000</td>
<td>-0.578125</td>
<td>0.000000</td>
<td>0.437500</td>
</tr>
<tr>
<td>76</td>
<td>0.001800</td>
<td>0.015625</td>
<td>1.187551</td>
<td>161.500000</td>
<td>0.044650</td>
<td>0.000000</td>
<td>-0.812500</td>
<td>0.000000</td>
<td>0.828125</td>
</tr>
<tr>
<td>77</td>
<td>0.001300</td>
<td>-0.015625</td>
<td>1.290225</td>
<td>225.250000</td>
<td>0.032133</td>
<td>0.000000</td>
<td>-0.578125</td>
<td>0.000000</td>
<td>0.562500</td>
</tr>
<tr>
<td>78</td>
<td>0.000800</td>
<td>-0.187500</td>
<td>0.950212</td>
<td>187.468750</td>
<td>0.019791</td>
<td>0.000000</td>
<td>-0.671875</td>
<td>0.000000</td>
<td>0.484375</td>
</tr>
<tr>
<td>79</td>
<td>0.000200</td>
<td>0.109375</td>
<td>1.230728</td>
<td>255.531250</td>
<td>0.003844</td>
<td>0.000000</td>
<td>-0.531250</td>
<td>0.000000</td>
<td>0.640625</td>
</tr>
<tr>
<td>80</td>
<td>0.000200</td>
<td>-0.046875</td>
<td>1.009551</td>
<td>192.156250</td>
<td>0.005587</td>
<td>0.000000</td>
<td>-0.625000</td>
<td>0.000000</td>
<td>0.578125</td>
</tr>
<tr>
<td>81</td>
<td>0.000500</td>
<td>0.328125</td>
<td>1.439251</td>
<td>205.875000</td>
<td>0.012380</td>
<td>0.000000</td>
<td>-0.484375</td>
<td>0.000000</td>
<td>0.812500</td>
</tr>
<tr>
<td>82</td>
<td>0.000300</td>
<td>-0.062500</td>
<td>0.829022</td>
<td>218.562500</td>
<td>0.006770</td>
<td>0.000000</td>
<td>-0.765625</td>
<td>0.000000</td>
<td>0.703125</td>
</tr>
<tr>
<td>83</td>
<td>0.000400</td>
<td>0.390625</td>
<td>1.217244</td>
<td>192.125000</td>
<td>0.010037</td>
<td>0.000000</td>
<td>-0.531250</td>
<td>0.000000</td>
<td>0.921875</td>
</tr>
<tr>
<td>84</td>
<td>0.000200</td>
<td>0.125000</td>
<td>1.083546</td>
<td>213.593750</td>
<td>0.006047</td>
<td>0.000000</td>
<td>-0.343750</td>
<td>0.000000</td>
<td>0.468750</td>
</tr>
<tr>
<td>85</td>
<td>0.000500</td>
<td>0.250000</td>
<td>1.329997</td>
<td>225.406250</td>
<td>0.012899</td>
<td>0.000000</td>
<td>-0.531250</td>
<td>0.000000</td>
<td>0.781250</td>
</tr>
<tr>
<td>86</td>
<td>0.000400</td>
<td>0.781250</td>
<td>1.129282</td>
<td>185.281250</td>
<td>0.009419</td>
<td>0.000000</td>
<td>-0.296875</td>
<td>0.000000</td>
<td>1.078125</td>
</tr>
<tr>
<td>87</td>
<td>0.001200</td>
<td>-0.140625</td>
<td>1.305537</td>
<td>263.750000</td>
<td>0.029129</td>
<td>0.093750</td>
<td>-0.625000</td>
<td>-0.046875</td>
<td>0.437500</td>
</tr>
<tr>
<td>88</td>
<td>0.000900</td>
<td>-0.031250</td>
<td>1.597338</td>
<td>224.218750</td>
<td>0.021730</td>
<td>0.093750</td>
<td>-0.484375</td>
<td>-0.046875</td>
<td>0.406250</td>
</tr>
<tr>
<td>89</td>
<td>0.000500</td>
<td>0.234375</td>
<td>1.120617</td>
<td>210.125000</td>
<td>0.012163</td>
<td>0.000000</td>
<td>-0.250000</td>
<td>0.000000</td>
<td>0.484375</td>
</tr>
<tr>
<td>90</td>
<td>0.000800</td>
<td>0.687500</td>
<td>1.133673</td>
<td>205.968750</td>
<td>0.019057</td>
<td>0.000000</td>
<td>-0.156250</td>
<td>0.000000</td>
<td>0.843750</td>
</tr>
<tr>
<td>91</td>
<td>0.000500</td>
<td>0.390625</td>
<td>1.246974</td>
<td>185.406250</td>
<td>0.012114</td>
<td>0.000000</td>
<td>-0.156250</td>
<td>0.000000</td>
<td>0.546875</td>
</tr>
<tr>
<td>92</td>
<td>0.000500</td>
<td>0.781250</td>
<td>1.404984</td>
<td>257.500000</td>
<td>0.011828</td>
<td>0.000000</td>
<td>-0.015625</td>
<td>0.000000</td>
<td>0.796875</td>
</tr>
<tr>
<td>93</td>
<td>0.000900</td>
<td>0.562500</td>
<td>1.055051</td>
<td>206.531250</td>
<td>0.022210</td>
<td>0.000000</td>
<td>-0.296875</td>
<td>0.000000</td>
<td>0.859375</td>
</tr>
<tr>
<td>94</td>
<td>0.001500</td>
<td>0.437500</td>
<td>1.188617</td>
<td>204.312500</td>
<td>0.036983</td>
<td>0.000000</td>
<td>-0.062500</td>
<td>0.000000</td>
<td>0.500000</td>
</tr>
<tr>
<td>95</td>
<td>0.000600</td>
<td>1.125000</td>
<td>1.179184</td>
<td>215.031250</td>
<td>0.013927</td>
<td>0.000000</td>
<td>0.078125</td>
<td>0.000000</td>
<td>1.046875</td>
</tr>
<tr>
<td>96</td>
<td>0.000600</td>
<td>0.562500</td>
<td>1.028679</td>
<td>258.593750</td>
<td>0.016106</td>
<td>0.000000</td>
<td>0.265625</td>
<td>0.000000</td>
<td>0.296875</td>
</tr>
<tr>
<td>97</td>
<td>0.001200</td>
<td>1.140625</td>
<td>0.947520</td>
<td>192.062500</td>
<td>0.030544</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>0.828125</td>
</tr>
<tr>
<td>98</td>
<td>0.001000</td>
<td>1.187500</td>
<td>1.241357</td>
<td>178.625000</td>
<td>0.025544</td>
<td>0.000000</td>
<td>0.218750</td>
<td>0.000000</td>
<td>0.968750</td>
</tr>
<tr>
<td>99</td>
<td>0.001100</td>
<td>1.703125</td>
<td>0.486136</td>
<td>207.468750</td>
<td>0.026573</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.296875</td>
</tr>
<tr>
<td>100</td>
<td>0.001500</td>
<td>0.984375</td>
<td>0.928457</td>
<td>185.281250</td>
<td>0.037967</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>0.671875</td>
</tr>
<tr>
<td>101</td>
<td>0.001500</td>
<td>0.984375</td>
<td>0.853727</td>
<td>228.781250</td>
<td>0.036708</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>0.671875</td>
</tr>
<tr>
<td>102</td>
<td>0.000600</td>
<td>1.312500</td>
<td>0.891995</td>
<td>218.906250</td>
<td>0.015497</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.906250</td>
</tr>
<tr>
<td>103</td>
<td>0.001000</td>
<td>1.375000</td>
<td>0.765978</td>
<td>222.750000</td>
<td>0.026233</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.875000</td>
</tr>
<tr>
<td>104</td>
<td>0.001500</td>
<td>1.406250</td>
<td>1.165558</td>
<td>266.062500</td>
<td>0.036789</td>
<td>0.093750</td>
<td>0.406250</td>
<td>-0.046875</td>
<td>0.953125</td>
</tr>
<tr>
<td>105</td>
<td>0.001700</td>
<td>1.343750</td>
<td>0.857756</td>
<td>227.343750</td>
<td>0.041906</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>0.984375</td>
</tr>
<tr>
<td>106</td>
<td>0.001400</td>
<td>0.984375</td>
<td>0.860658</td>
<td>189.531250</td>
<td>0.034120</td>
<td>0.000000</td>
<td>0.265625</td>
<td>0.000000</td>
<td>0.718750</td>
</tr>
<tr>
<td>107</td>
<td>0.001200</td>
<td>1.187500</td>
<td>1.046163</td>
<td>195.281250</td>
<td>0.031028</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.734375</td>
</tr>
<tr>
<td>108</td>
<td>0.000800</td>
<td>1.062500</td>
<td>0.811200</td>
<td>230.250000</td>
<td>0.018868</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.562500</td>
</tr>
<tr>
<td>109</td>
<td>0.001200</td>
<td>1.796875</td>
<td>0.482950</td>
<td>166.250000</td>
<td>0.029768</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.390625</td>
</tr>
<tr>
<td>110</td>
<td>0.001000</td>
<td>1.312500</td>
<td>0.942500</td>
<td>191.968750</td>
<td>0.024071</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.906250</td>
</tr>
<tr>
<td>111</td>
<td>0.001800</td>
<td>1.796875</td>
<td>0.378851</td>
<td>158.593750</td>
<td>0.044576</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.390625</td>
</tr>
<tr>
<td>112</td>
<td>0.001900</td>
<td>0.812500</td>
<td>0.878748</td>
<td>198.437500</td>
<td>0.047842</td>
<td>0.000000</td>
<td>0.265625</td>
<td>0.000000</td>
<td>0.546875</td>
</tr>
<tr>
<td>113</td>
<td>0.001800</td>
<td>1.375000</td>
<td>0.776981</td>
<td>236.000000</td>
<td>0.044439</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.968750</td>
</tr>
<tr>
<td>114</td>
<td>0.000500</td>
<td>1.234375</td>
<td>0.869200</td>
<td>227.343750</td>
<td>0.013377</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.734375</td>
</tr>
<tr>
<td>115</td>
<td>0.001200</td>
<td>1.296875</td>
<td>0.730539</td>
<td>266.500000</td>
<td>0.029172</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>0.937500</td>
</tr>
<tr>
<td>116</td>
<td>0.000700</td>
<td>0.812500</td>
<td>0.933436</td>
<td>237.468750</td>
<td>0.018362</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.359375</td>
</tr>
<tr>
<td>117</td>
<td>0.001600</td>
<td>1.312500</td>
<td>0.408232</td>
<td>201.656250</td>
<td>0.039018</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.812500</td>
</tr>
<tr>
<td>118</td>
<td>0.001400</td>
<td>0.515625</td>
<td>0.696807</td>
<td>301.312500</td>
<td>0.033776</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>0.203125</td>
</tr>
<tr>
<td>119</td>
<td>0.001400</td>
<td>1.406250</td>
<td>0.515019</td>
<td>203.562500</td>
<td>0.035745</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.953125</td>
</tr>
<tr>
<td>120</td>
<td>0.001000</td>
<td>1.437500</td>
<td>0.176777</td>
<td>181.718750</td>
<td>0.024595</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.937500</td>
</tr>
<tr>
<td>121</td>
<td>0.001000</td>
<td>1.437500</td>
<td>0.176777</td>
<td>175.812500</td>
<td>0.026220</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.937500</td>
</tr>
<tr>
<td>122</td>
<td>0.001400</td>
<td>1.734375</td>
<td>0.547634</td>
<td>148.187500</td>
<td>0.035160</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.328125</td>
</tr>
<tr>
<td>123</td>
<td>0.000600</td>
<td>1.328125</td>
<td>0.631299</td>
<td>221.125000</td>
<td>0.015376</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.875000</td>
</tr>
<tr>
<td>124</td>
<td>0.001300</td>
<td>1.515625</td>
<td>1.093026</td>
<td>188.750000</td>
<td>0.032095</td>
<td>0.093750</td>
<td>0.500000</td>
<td>-0.046875</td>
<td>0.968750</td>
</tr>
<tr>
<td>125</td>
<td>0.000800</td>
<td>1.234375</td>
<td>0.479745</td>
<td>210.093750</td>
<td>0.019243</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.828125</td>
</tr>
<tr>
<td>126</td>
<td>0.003200</td>
<td>1.625000</td>
<td>0.670366</td>
<td>191.593750</td>
<td>0.079061</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.218750</td>
</tr>
<tr>
<td>127</td>
<td>0.001300</td>
<td>1.500000</td>
<td>1.043960</td>
<td>204.718750</td>
<td>0.031953</td>
<td>0.093750</td>
<td>0.406250</td>
<td>-0.046875</td>
<td>1.046875</td>
</tr>
<tr>
<td>128</td>
<td>0.001300</td>
<td>1.609375</td>
<td>0.467538</td>
<td>175.968750</td>
<td>0.032042</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>1.156250</td>
</tr>
<tr>
<td>129</td>
<td>0.001000</td>
<td>1.500000</td>
<td>0.585008</td>
<td>183.968750</td>
<td>0.024249</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>1.000000</td>
</tr>
<tr>
<td>130</td>
<td>0.004600</td>
<td>1.328125</td>
<td>0.938505</td>
<td>164.875000</td>
<td>0.113943</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>1.015625</td>
</tr>
<tr>
<td>131</td>
<td>0.006000</td>
<td>1.640625</td>
<td>0.622812</td>
<td>188.250000</td>
<td>0.148817</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>1.187500</td>
</tr>
<tr>
<td>132</td>
<td>0.001000</td>
<td>1.281250</td>
<td>0.795312</td>
<td>277.781250</td>
<td>0.024185</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.828125</td>
</tr>
<tr>
<td>133</td>
<td>0.000800</td>
<td>1.171875</td>
<td>1.086910</td>
<td>261.875000</td>
<td>0.020210</td>
<td>0.000000</td>
<td>0.218750</td>
<td>0.000000</td>
<td>0.953125</td>
</tr>
<tr>
<td>134</td>
<td>0.000900</td>
<td>1.312500</td>
<td>0.408232</td>
<td>176.000000</td>
<td>0.023013</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>0.812500</td>
</tr>
<tr>
<td>135</td>
<td>0.000800</td>
<td>1.390625</td>
<td>0.813103</td>
<td>235.500000</td>
<td>0.020849</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.984375</td>
</tr>
<tr>
<td>136</td>
<td>0.001000</td>
<td>1.328125</td>
<td>1.096668</td>
<td>236.062500</td>
<td>0.024200</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>0.968750</td>
</tr>
<tr>
<td>137</td>
<td>0.001200</td>
<td>1.312500</td>
<td>1.005292</td>
<td>178.125000</td>
<td>0.029885</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>0.953125</td>
</tr>
<tr>
<td>138</td>
<td>0.001100</td>
<td>1.453125</td>
<td>0.545398</td>
<td>190.750000</td>
<td>0.027297</td>
<td>0.093750</td>
<td>0.453125</td>
<td>-0.046875</td>
<td>0.953125</td>
</tr>
<tr>
<td>139</td>
<td>0.000900</td>
<td>1.109375</td>
<td>0.764968</td>
<td>240.218750</td>
<td>0.022473</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.656250</td>
</tr>
<tr>
<td>140</td>
<td>0.001300</td>
<td>1.703125</td>
<td>0.595493</td>
<td>162.343750</td>
<td>0.033677</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>1.203125</td>
</tr>
<tr>
<td>141</td>
<td>0.002700</td>
<td>1.375000</td>
<td>0.584111</td>
<td>222.437500</td>
<td>0.068051</td>
<td>0.000000</td>
<td>0.312500</td>
<td>0.000000</td>
<td>1.062500</td>
</tr>
<tr>
<td>142</td>
<td>0.001100</td>
<td>1.609375</td>
<td>0.620965</td>
<td>149.593750</td>
<td>0.026268</td>
<td>0.093750</td>
<td>0.500000</td>
<td>-0.046875</td>
<td>1.062500</td>
</tr>
<tr>
<td>143</td>
<td>0.001000</td>
<td>1.578125</td>
<td>0.600920</td>
<td>180.031250</td>
<td>0.024581</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>1.125000</td>
</tr>
<tr>
<td>144</td>
<td>0.001400</td>
<td>1.484375</td>
<td>0.916169</td>
<td>264.343750</td>
<td>0.033957</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>1.031250</td>
</tr>
<tr>
<td>145</td>
<td>0.001000</td>
<td>1.781250</td>
<td>0.618718</td>
<td>206.625000</td>
<td>0.025896</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>1.375000</td>
</tr>
<tr>
<td>146</td>
<td>0.001000</td>
<td>1.218750</td>
<td>0.783080</td>
<td>206.125000</td>
<td>0.024213</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>0.765625</td>
</tr>
<tr>
<td>147</td>
<td>0.001700</td>
<td>0.656250</td>
<td>1.424720</td>
<td>235.937500</td>
<td>0.042147</td>
<td>0.000000</td>
<td>0.031250</td>
<td>0.000000</td>
<td>0.625000</td>
</tr>
<tr>
<td>148</td>
<td>0.002500</td>
<td>1.046875</td>
<td>1.278966</td>
<td>197.968750</td>
<td>0.062598</td>
<td>0.187500</td>
<td>0.312500</td>
<td>-0.093750</td>
<td>0.640625</td>
</tr>
<tr>
<td>149</td>
<td>0.000700</td>
<td>1.812500</td>
<td>0.408232</td>
<td>211.093750</td>
<td>0.017018</td>
<td>0.000000</td>
<td>0.500000</td>
<td>0.000000</td>
<td>1.312500</td>
</tr>
<tr>
<td>150</td>
<td>0.000600</td>
<td>1.234375</td>
<td>1.010477</td>
<td>230.593750</td>
<td>0.014464</td>
<td>0.187500</td>
<td>0.453125</td>
<td>-0.093750</td>
<td>0.687500</td>
</tr>
<tr>
<td>151</td>
<td>0.006400</td>
<td>1.296875</td>
<td>0.805979</td>
<td>169.187500</td>
<td>0.158935</td>
<td>0.000000</td>
<td>0.406250</td>
<td>0.000000</td>
<td>0.890625</td>
</tr>
<tr>
<td>152</td>
<td>0.000800</td>
<td>1.609375</td>
<td>0.797742</td>
<td>198.312500</td>
<td>0.020103</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>1.250000</td>
</tr>
<tr>
<td>153</td>
<td>0.002900</td>
<td>1.671875</td>
<td>1.251909</td>
<td>166.750000</td>
<td>0.073712</td>
<td>0.093750</td>
<td>0.265625</td>
<td>0.046875</td>
<td>1.265625</td>
</tr>
<tr>
<td>154</td>
<td>0.001300</td>
<td>1.453125</td>
<td>0.459246</td>
<td>145.593750</td>
<td>0.032790</td>
<td>0.093750</td>
<td>0.453125</td>
<td>-0.046875</td>
<td>0.953125</td>
</tr>
<tr>
<td>155</td>
<td>0.001300</td>
<td>1.578125</td>
<td>0.751779</td>
<td>182.906250</td>
<td>0.031981</td>
<td>0.000000</td>
<td>0.453125</td>
<td>0.000000</td>
<td>1.125000</td>
</tr>
<tr>
<td>156</td>
<td>0.000900</td>
<td>2.062500</td>
<td>0.678101</td>
<td>213.281250</td>
<td>0.022632</td>
<td>0.187500</td>
<td>0.593750</td>
<td>-0.093750</td>
<td>1.375000</td>
</tr>
<tr>
<td>157</td>
<td>0.001900</td>
<td>1.328125</td>
<td>0.981919</td>
<td>229.687500</td>
<td>0.046646</td>
<td>0.000000</td>
<td>0.359375</td>
<td>0.000000</td>
<td>0.968750</td>
</tr>
<tr>
<td>158</td>
<td>0.001200</td>
<td>1.656250</td>
<td>0.655990</td>
<td>214.375000</td>
<td>0.030503</td>
<td>0.093750</td>
<td>0.546875</td>
<td>-0.046875</td>
<td>1.062500</td>
</tr>
<tr>
<td>159</td>
<td>0.001300</td>
<td>1.718750</td>
<td>2.087155</td>
<td>188.125000</td>
<td>0.032338</td>
<td>0.375000</td>
<td>0.546875</td>
<td>0.046875</td>
<td>0.750000</td>
</tr>
<tr>
<td>160</td>
<td>0.001900</td>
<td>2.296875</td>
<td>1.705068</td>
<td>140.093750</td>
<td>0.048178</td>
<td>0.375000</td>
<td>0.593750</td>
<td>0.000000</td>
<td>1.328125</td>
</tr>
<tr>
<td>161</td>
<td>0.004200</td>
<td>2.640625</td>
<td>2.323636</td>
<td>228.031250</td>
<td>0.105646</td>
<td>0.937500</td>
<td>0.781250</td>
<td>-0.187500</td>
<td>1.109375</td>
</tr>
<tr>
<td>162</td>
<td>0.003700</td>
<td>1.500000</td>
<td>1.569054</td>
<td>229.250000</td>
<td>0.092943</td>
<td>0.281250</td>
<td>0.546875</td>
<td>-0.046875</td>
<td>0.718750</td>
</tr>
<tr>
<td>163</td>
<td>0.003100</td>
<td>3.046875</td>
<td>1.816729</td>
<td>180.000000</td>
<td>0.077299</td>
<td>1.406250</td>
<td>1.156250</td>
<td>-0.515625</td>
<td>1.000000</td>
</tr>
<tr>
<td>164</td>
<td>0.002200</td>
<td>2.781250</td>
<td>1.721653</td>
<td>202.093750</td>
<td>0.054294</td>
<td>1.125000</td>
<td>1.109375</td>
<td>-0.468750</td>
<td>1.015625</td>
</tr>
<tr>
<td>165</td>
<td>0.002600</td>
<td>2.953125</td>
<td>1.882202</td>
<td>185.906250</td>
<td>0.064815</td>
<td>1.218750</td>
<td>1.015625</td>
<td>-0.421875</td>
<td>1.140625</td>
</tr>
<tr>
<td>166</td>
<td>0.004200</td>
<td>3.546875</td>
<td>2.579256</td>
<td>188.656250</td>
<td>0.105926</td>
<td>1.218750</td>
<td>1.109375</td>
<td>-0.046875</td>
<td>1.265625</td>
</tr>
<tr>
<td>167</td>
<td>0.002700</td>
<td>4.796875</td>
<td>2.472211</td>
<td>186.468750</td>
<td>0.067445</td>
<td>1.500000</td>
<td>1.250000</td>
<td>0.609375</td>
<td>1.437500</td>
</tr>
<tr>
<td>168</td>
<td>0.003400</td>
<td>5.312500</td>
<td>2.838982</td>
<td>237.000000</td>
<td>0.084967</td>
<td>2.156250</td>
<td>1.484375</td>
<td>0.593750</td>
<td>1.078125</td>
</tr>
<tr>
<td>169</td>
<td>0.004400</td>
<td>5.687500</td>
<td>2.884149</td>
<td>153.906250</td>
<td>0.110844</td>
<td>2.250000</td>
<td>1.531250</td>
<td>0.562500</td>
<td>1.343750</td>
</tr>
<tr>
<td>170</td>
<td>0.003400</td>
<td>5.484375</td>
<td>3.178878</td>
<td>236.718750</td>
<td>0.085787</td>
<td>2.062500</td>
<td>1.437500</td>
<td>0.921875</td>
<td>1.062500</td>
</tr>
<tr>
<td>171</td>
<td>0.004100</td>
<td>5.000000</td>
<td>2.129701</td>
<td>194.343750</td>
<td>0.102202</td>
<td>2.250000</td>
<td>1.578125</td>
<td>-0.078125</td>
<td>1.250000</td>
</tr>
<tr>
<td>172</td>
<td>0.003700</td>
<td>5.890625</td>
<td>2.103432</td>
<td>172.750000</td>
<td>0.093451</td>
<td>2.437500</td>
<td>1.718750</td>
<td>0.609375</td>
<td>1.125000</td>
</tr>
<tr>
<td>173</td>
<td>0.004000</td>
<td>7.015625</td>
<td>1.445773</td>
<td>191.468750</td>
<td>0.099476</td>
<td>2.718750</td>
<td>1.859375</td>
<td>1.125000</td>
<td>1.312500</td>
</tr>
<tr>
<td>174</td>
<td>0.003400</td>
<td>6.609375</td>
<td>3.235784</td>
<td>207.437500</td>
<td>0.085591</td>
<td>2.625000</td>
<td>1.671875</td>
<td>1.046875</td>
<td>1.265625</td>
</tr>
<tr>
<td>175</td>
<td>0.004900</td>
<td>4.281250</td>
<td>2.483874</td>
<td>173.625000</td>
<td>0.122192</td>
<td>2.718750</td>
<td>1.718750</td>
<td>-0.750000</td>
<td>0.593750</td>
</tr>
<tr>
<td>176</td>
<td>0.005900</td>
<td>7.796875</td>
<td>2.549682</td>
<td>226.812500</td>
<td>0.146486</td>
<td>2.906250</td>
<td>1.953125</td>
<td>1.687500</td>
<td>1.250000</td>
</tr>
<tr>
<td>177</td>
<td>0.005500</td>
<td>7.703125</td>
<td>2.405153</td>
<td>175.250000</td>
<td>0.136918</td>
<td>2.625000</td>
<td>1.765625</td>
<td>1.921875</td>
<td>1.390625</td>
</tr>
<tr>
<td>178</td>
<td>0.003500</td>
<td>6.875000</td>
<td>1.509917</td>
<td>209.625000</td>
<td>0.088394</td>
<td>2.906250</td>
<td>1.953125</td>
<td>1.015625</td>
<td>1.000000</td>
</tr>
<tr>
<td>179</td>
<td>0.004700</td>
<td>5.281250</td>
<td>1.724031</td>
<td>252.000000</td>
<td>0.116413</td>
<td>2.718750</td>
<td>1.812500</td>
<td>-0.250000</td>
<td>1.000000</td>
</tr>
<tr>
<td>180</td>
<td>0.005100</td>
<td>6.718750</td>
<td>0.997137</td>
<td>161.156250</td>
<td>0.127866</td>
<td>2.906250</td>
<td>1.953125</td>
<td>0.468750</td>
<td>1.390625</td>
</tr>
<tr>
<td>181</td>
<td>0.003900</td>
<td>6.000000</td>
<td>1.986871</td>
<td>183.031250</td>
<td>0.097594</td>
<td>2.625000</td>
<td>1.812500</td>
<td>0.375000</td>
<td>1.187500</td>
</tr>
<tr>
<td>182</td>
<td>0.020000</td>
<td>4.968750</td>
<td>2.927116</td>
<td>249.968750</td>
<td>0.499361</td>
<td>2.718750</td>
<td>1.671875</td>
<td>-0.140625</td>
<td>0.718750</td>
</tr>
<tr>
<td>183</td>
<td>0.004400</td>
<td>6.984375</td>
<td>1.202613</td>
<td>202.781250</td>
<td>0.109274</td>
<td>3.000000</td>
<td>2.000000</td>
<td>1.046875</td>
<td>0.937500</td>
</tr>
<tr>
<td>184</td>
<td>0.005400</td>
<td>6.000000</td>
<td>2.385789</td>
<td>154.656250</td>
<td>0.135349</td>
<td>2.718750</td>
<td>1.812500</td>
<td>0.187500</td>
<td>1.281250</td>
</tr>
<tr>
<td>185</td>
<td>0.004800</td>
<td>7.234375</td>
<td>1.953614</td>
<td>224.187500</td>
<td>0.120685</td>
<td>2.718750</td>
<td>1.812500</td>
<td>1.406250</td>
<td>1.296875</td>
</tr>
<tr>
<td>186</td>
<td>0.020300</td>
<td>6.937500</td>
<td>0.883883</td>
<td>177.531250</td>
<td>0.507801</td>
<td>3.000000</td>
<td>2.000000</td>
<td>0.562500</td>
<td>1.375000</td>
</tr>
<tr>
<td>187</td>
<td>0.004100</td>
<td>6.515625</td>
<td>1.212687</td>
<td>168.500000</td>
<td>0.103705</td>
<td>2.906250</td>
<td>1.953125</td>
<td>0.468750</td>
<td>1.187500</td>
</tr>
<tr>
<td>188</td>
<td>0.004000</td>
<td>5.187500</td>
<td>1.122995</td>
<td>193.843750</td>
<td>0.100148</td>
<td>3.000000</td>
<td>2.000000</td>
<td>-0.812500</td>
<td>1.000000</td>
</tr>
<tr>
<td>189</td>
<td>0.004100</td>
<td>8.125000</td>
<td>1.308445</td>
<td>174.031250</td>
<td>0.102828</td>
<td>2.906250</td>
<td>1.953125</td>
<td>1.875000</td>
<td>1.390625</td>
</tr>
<tr>
<td>190</td>
<td>0.004100</td>
<td>6.937500</td>
<td>2.089403</td>
<td>195.593750</td>
<td>0.101901</td>
<td>2.718750</td>
<td>1.765625</td>
<td>1.312500</td>
<td>1.140625</td>
</tr>
<tr>
<td>191</td>
<td>0.005100</td>
<td>6.953125</td>
<td>1.416415</td>
<td>155.812500</td>
<td>0.126509</td>
<td>2.812500</td>
<td>1.906250</td>
<td>0.921875</td>
<td>1.312500</td>
</tr>
<tr>
<td>192</td>
<td>0.004300</td>
<td>5.875000</td>
<td>1.560049</td>
<td>156.406250</td>
<td>0.106396</td>
<td>2.906250</td>
<td>1.953125</td>
<td>-0.171875</td>
<td>1.187500</td>
</tr>
<tr>
<td>193</td>
<td>0.004100</td>
<td>6.906250</td>
<td>1.281434</td>
<td>194.406250</td>
<td>0.102739</td>
<td>2.906250</td>
<td>1.953125</td>
<td>1.359375</td>
<td>0.687500</td>
</tr>
<tr>
<td>194</td>
<td>0.007400</td>
<td>7.203125</td>
<td>2.318865</td>
<td>181.687500</td>
<td>0.185513</td>
<td>2.625000</td>
<td>1.812500</td>
<td>1.453125</td>
<td>1.312500</td>
</tr>
<tr>
<td>195</td>
<td>0.004100</td>
<td>8.781250</td>
<td>1.330866</td>
<td>166.250000</td>
<td>0.103291</td>
<td>2.812500</td>
<td>1.906250</td>
<td>2.687500</td>
<td>1.375000</td>
</tr>
<tr>
<td>196</td>
<td>0.003500</td>
<td>5.656250</td>
<td>2.358512</td>
<td>211.812500</td>
<td>0.087013</td>
<td>3.000000</td>
<td>2.000000</td>
<td>-0.093750</td>
<td>0.750000</td>
</tr>
<tr>
<td>197</td>
<td>0.004400</td>
<td>5.781250</td>
<td>1.350586</td>
<td>174.781250</td>
<td>0.109044</td>
<td>2.906250</td>
<td>1.953125</td>
<td>-0.328125</td>
<td>1.250000</td>
</tr>
<tr>
<td>198</td>
<td>0.003500</td>
<td>8.296875</td>
<td>1.729492</td>
<td>212.093750</td>
<td>0.086299</td>
<td>3.000000</td>
<td>2.000000</td>
<td>2.171875</td>
<td>1.125000</td>
</tr>
<tr>
<td>199</td>
<td>0.013300</td>
<td>5.250000</td>
<td>3.284425</td>
<td>210.468750</td>
<td>0.331680</td>
<td>2.812500</td>
<td>1.765625</td>
<td>0.203125</td>
<td>0.468750</td>
</tr>
<tr>
<td>200</td>
<td>0.003900</td>
<td>8.812500</td>
<td>1.409435</td>
<td>225.281250</td>
<td>0.098384</td>
<td>2.906250</td>
<td>1.953125</td>
<td>2.578125</td>
<td>1.375000</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>950</td>
<td>0.038400</td>
<td>3.390625</td>
<td>4.794105</td>
<td>739.437500</td>
<td>0.959228</td>
<td>1.781250</td>
<td>0.453125</td>
<td>0.671875</td>
<td>0.484375</td>
</tr>
<tr>
<td>951</td>
<td>0.026300</td>
<td>6.453125</td>
<td>3.471043</td>
<td>431.656250</td>
<td>0.656899</td>
<td>2.437500</td>
<td>1.156250</td>
<td>1.890625</td>
<td>0.968750</td>
</tr>
<tr>
<td>952</td>
<td>0.006600</td>
<td>5.281250</td>
<td>3.032026</td>
<td>601.281250</td>
<td>0.164812</td>
<td>1.968750</td>
<td>0.781250</td>
<td>1.703125</td>
<td>0.828125</td>
</tr>
<tr>
<td>953</td>
<td>0.020300</td>
<td>5.281250</td>
<td>5.468405</td>
<td>738.500000</td>
<td>0.507536</td>
<td>2.062500</td>
<td>0.593750</td>
<td>1.718750</td>
<td>0.906250</td>
</tr>
<tr>
<td>954</td>
<td>0.017300</td>
<td>6.046875</td>
<td>3.924325</td>
<td>572.656250</td>
<td>0.432236</td>
<td>2.250000</td>
<td>1.062500</td>
<td>1.687500</td>
<td>1.046875</td>
</tr>
<tr>
<td>955</td>
<td>0.006100</td>
<td>7.375000</td>
<td>2.560909</td>
<td>260.500000</td>
<td>0.151581</td>
<td>2.718750</td>
<td>1.625000</td>
<td>1.875000</td>
<td>1.156250</td>
</tr>
<tr>
<td>956</td>
<td>0.005200</td>
<td>6.718750</td>
<td>4.473347</td>
<td>468.906250</td>
<td>0.130771</td>
<td>2.531250</td>
<td>1.203125</td>
<td>1.968750</td>
<td>1.015625</td>
</tr>
<tr>
<td>957</td>
<td>0.007700</td>
<td>6.562500</td>
<td>3.947818</td>
<td>335.937500</td>
<td>0.191723</td>
<td>2.437500</td>
<td>1.390625</td>
<td>1.734375</td>
<td>1.000000</td>
</tr>
<tr>
<td>958</td>
<td>0.005400</td>
<td>8.421875</td>
<td>2.519068</td>
<td>259.281250</td>
<td>0.134572</td>
<td>2.718750</td>
<td>1.718750</td>
<td>2.578125</td>
<td>1.406250</td>
</tr>
<tr>
<td>959</td>
<td>0.004900</td>
<td>7.031250</td>
<td>4.792339</td>
<td>381.781250</td>
<td>0.122648</td>
<td>2.437500</td>
<td>1.250000</td>
<td>2.156250</td>
<td>1.187500</td>
</tr>
<tr>
<td>960</td>
<td>0.006900</td>
<td>6.312500</td>
<td>3.838946</td>
<td>431.875000</td>
<td>0.172965</td>
<td>2.343750</td>
<td>1.109375</td>
<td>1.843750</td>
<td>1.015625</td>
</tr>
<tr>
<td>961</td>
<td>0.005600</td>
<td>7.468750</td>
<td>1.149049</td>
<td>160.750000</td>
<td>0.140026</td>
<td>2.812500</td>
<td>1.812500</td>
<td>1.828125</td>
<td>1.015625</td>
</tr>
<tr>
<td>962</td>
<td>0.005400</td>
<td>8.265625</td>
<td>2.971786</td>
<td>260.843750</td>
<td>0.135317</td>
<td>2.718750</td>
<td>1.625000</td>
<td>2.578125</td>
<td>1.343750</td>
</tr>
<tr>
<td>963</td>
<td>0.005400</td>
<td>4.468750</td>
<td>3.997409</td>
<td>616.625000</td>
<td>0.133911</td>
<td>2.062500</td>
<td>0.734375</td>
<td>1.093750</td>
<td>0.578125</td>
</tr>
<tr>
<td>964</td>
<td>0.008400</td>
<td>5.390625</td>
<td>5.154791</td>
<td>533.406250</td>
<td>0.209105</td>
<td>1.968750</td>
<td>0.828125</td>
<td>1.718750</td>
<td>0.875000</td>
</tr>
<tr>
<td>965</td>
<td>0.011400</td>
<td>7.109375</td>
<td>2.650880</td>
<td>440.125000</td>
<td>0.286244</td>
<td>2.343750</td>
<td>1.203125</td>
<td>2.343750</td>
<td>1.218750</td>
</tr>
<tr>
<td>966</td>
<td>0.006400</td>
<td>8.593750</td>
<td>1.761913</td>
<td>245.218750</td>
<td>0.159059</td>
<td>2.906250</td>
<td>1.859375</td>
<td>2.562500</td>
<td>1.265625</td>
</tr>
<tr>
<td>967</td>
<td>0.005600</td>
<td>5.750000</td>
<td>3.698976</td>
<td>416.937500</td>
<td>0.140569</td>
<td>2.531250</td>
<td>1.343750</td>
<td>1.234375</td>
<td>0.640625</td>
</tr>
<tr>
<td>968</td>
<td>0.010700</td>
<td>5.890625</td>
<td>3.729024</td>
<td>445.937500</td>
<td>0.267731</td>
<td>2.156250</td>
<td>1.109375</td>
<td>1.609375</td>
<td>1.015625</td>
</tr>
<tr>
<td>969</td>
<td>0.007500</td>
<td>7.765625</td>
<td>2.266480</td>
<td>402.156250</td>
<td>0.187080</td>
<td>2.531250</td>
<td>1.390625</td>
<td>2.531250</td>
<td>1.312500</td>
</tr>
<tr>
<td>970</td>
<td>0.005200</td>
<td>8.062500</td>
<td>3.208148</td>
<td>278.343750</td>
<td>0.128951</td>
<td>2.812500</td>
<td>1.765625</td>
<td>2.328125</td>
<td>1.156250</td>
</tr>
<tr>
<td>971</td>
<td>0.004400</td>
<td>3.046875</td>
<td>4.904273</td>
<td>817.156250</td>
<td>0.108795</td>
<td>1.687500</td>
<td>0.265625</td>
<td>0.703125</td>
<td>0.390625</td>
</tr>
<tr>
<td>972</td>
<td>0.007400</td>
<td>6.625000</td>
<td>3.784479</td>
<td>436.281250</td>
<td>0.184524</td>
<td>2.343750</td>
<td>1.015625</td>
<td>2.109375</td>
<td>1.156250</td>
</tr>
<tr>
<td>973</td>
<td>0.012100</td>
<td>7.593750</td>
<td>2.933520</td>
<td>285.406250</td>
<td>0.303249</td>
<td>2.531250</td>
<td>1.625000</td>
<td>2.265625</td>
<td>1.171875</td>
</tr>
<tr>
<td>974</td>
<td>0.004700</td>
<td>5.437500</td>
<td>2.650628</td>
<td>622.156250</td>
<td>0.118285</td>
<td>2.156250</td>
<td>0.781250</td>
<td>1.546875</td>
<td>0.953125</td>
</tr>
<tr>
<td>975</td>
<td>0.005500</td>
<td>7.890625</td>
<td>2.330292</td>
<td>257.437500</td>
<td>0.136370</td>
<td>2.812500</td>
<td>1.625000</td>
<td>2.250000</td>
<td>1.203125</td>
</tr>
<tr>
<td>976</td>
<td>0.019800</td>
<td>7.906250</td>
<td>2.569942</td>
<td>234.718750</td>
<td>0.493939</td>
<td>2.531250</td>
<td>1.484375</td>
<td>2.531250</td>
<td>1.359375</td>
</tr>
<tr>
<td>977</td>
<td>0.021200</td>
<td>5.687500</td>
<td>4.039691</td>
<td>502.562500</td>
<td>0.529416</td>
<td>2.250000</td>
<td>1.062500</td>
<td>1.437500</td>
<td>0.937500</td>
</tr>
<tr>
<td>978</td>
<td>0.005700</td>
<td>7.671875</td>
<td>3.856081</td>
<td>308.375000</td>
<td>0.143650</td>
<td>2.437500</td>
<td>1.484375</td>
<td>2.390625</td>
<td>1.359375</td>
</tr>
<tr>
<td>979</td>
<td>0.006700</td>
<td>6.640625</td>
<td>2.014169</td>
<td>257.781250</td>
<td>0.167280</td>
<td>2.718750</td>
<td>1.671875</td>
<td>1.406250</td>
<td>0.843750</td>
</tr>
<tr>
<td>980</td>
<td>0.005800</td>
<td>7.296875</td>
<td>4.391503</td>
<td>358.843750</td>
<td>0.145540</td>
<td>2.343750</td>
<td>1.296875</td>
<td>2.343750</td>
<td>1.312500</td>
</tr>
<tr>
<td>981</td>
<td>0.014600</td>
<td>7.687500</td>
<td>3.170519</td>
<td>307.593750</td>
<td>0.364604</td>
<td>2.718750</td>
<td>1.578125</td>
<td>2.234375</td>
<td>1.156250</td>
</tr>
<tr>
<td>982</td>
<td>0.005100</td>
<td>4.515625</td>
<td>4.527711</td>
<td>662.531250</td>
<td>0.127984</td>
<td>2.062500</td>
<td>0.593750</td>
<td>1.328125</td>
<td>0.531250</td>
</tr>
<tr>
<td>983</td>
<td>0.020000</td>
<td>6.828125</td>
<td>3.359266</td>
<td>456.593750</td>
<td>0.501244</td>
<td>2.343750</td>
<td>1.109375</td>
<td>2.265625</td>
<td>1.109375</td>
</tr>
<tr>
<td>984</td>
<td>0.005600</td>
<td>7.906250</td>
<td>3.125799</td>
<td>311.125000</td>
<td>0.139092</td>
<td>2.531250</td>
<td>1.484375</td>
<td>2.531250</td>
<td>1.359375</td>
</tr>
<tr>
<td>985</td>
<td>0.020000</td>
<td>7.718750</td>
<td>3.967604</td>
<td>322.875000</td>
<td>0.500034</td>
<td>2.531250</td>
<td>1.484375</td>
<td>2.390625</td>
<td>1.312500</td>
</tr>
<tr>
<td>986</td>
<td>0.004900</td>
<td>6.375000</td>
<td>4.311140</td>
<td>431.562500</td>
<td>0.121487</td>
<td>2.343750</td>
<td>1.250000</td>
<td>1.890625</td>
<td>0.890625</td>
</tr>
<tr>
<td>987</td>
<td>0.004800</td>
<td>5.984375</td>
<td>3.908973</td>
<td>453.718750</td>
<td>0.119826</td>
<td>2.437500</td>
<td>1.156250</td>
<td>1.546875</td>
<td>0.843750</td>
</tr>
<tr>
<td>988</td>
<td>0.004000</td>
<td>6.421875</td>
<td>4.218211</td>
<td>495.031250</td>
<td>0.100046</td>
<td>2.437500</td>
<td>1.203125</td>
<td>1.812500</td>
<td>0.968750</td>
</tr>
<tr>
<td>989</td>
<td>0.004700</td>
<td>6.890625</td>
<td>3.777679</td>
<td>459.593750</td>
<td>0.118457</td>
<td>2.437500</td>
<td>1.203125</td>
<td>2.156250</td>
<td>1.093750</td>
</tr>
<tr>
<td>990</td>
<td>0.023000</td>
<td>7.687500</td>
<td>2.578946</td>
<td>244.218750</td>
<td>0.574621</td>
<td>2.625000</td>
<td>1.625000</td>
<td>2.203125</td>
<td>1.234375</td>
</tr>
<tr>
<td>991</td>
<td>0.059700</td>
<td>2.562500</td>
<td>3.699473</td>
<td>924.218750</td>
<td>1.491383</td>
<td>1.687500</td>
<td>0.031250</td>
<td>0.562500</td>
<td>0.281250</td>
</tr>
<tr>
<td>992</td>
<td>0.006300</td>
<td>7.218750</td>
<td>4.077323</td>
<td>456.187500</td>
<td>0.157262</td>
<td>2.531250</td>
<td>1.296875</td>
<td>2.250000</td>
<td>1.140625</td>
</tr>
<tr>
<td>993</td>
<td>0.004700</td>
<td>7.609375</td>
<td>3.041418</td>
<td>400.125000</td>
<td>0.117247</td>
<td>2.718750</td>
<td>1.578125</td>
<td>2.265625</td>
<td>1.046875</td>
</tr>
<tr>
<td>994</td>
<td>0.018800</td>
<td>7.531250</td>
<td>4.280019</td>
<td>353.843750</td>
<td>0.468879</td>
<td>2.437500</td>
<td>1.343750</td>
<td>2.437500</td>
<td>1.312500</td>
</tr>
<tr>
<td>995</td>
<td>0.004500</td>
<td>7.109375</td>
<td>3.185832</td>
<td>432.031250</td>
<td>0.112199</td>
<td>2.531250</td>
<td>1.390625</td>
<td>2.109375</td>
<td>1.078125</td>
</tr>
<tr>
<td>996</td>
<td>0.006500</td>
<td>5.000000</td>
<td>2.121320</td>
<td>597.406250</td>
<td>0.162493</td>
<td>2.062500</td>
<td>0.593750</td>
<td>1.312500</td>
<td>1.031250</td>
</tr>
<tr>
<td>997</td>
<td>0.005500</td>
<td>7.531250</td>
<td>3.559689</td>
<td>368.031250</td>
<td>0.138644</td>
<td>2.437500</td>
<td>1.390625</td>
<td>2.390625</td>
<td>1.312500</td>
</tr>
<tr>
<td>998</td>
<td>0.006600</td>
<td>5.812500</td>
<td>4.296125</td>
<td>364.437500</td>
<td>0.165292</td>
<td>2.250000</td>
<td>1.203125</td>
<td>1.515625</td>
<td>0.843750</td>
</tr>
<tr>
<td>999</td>
<td>0.004800</td>
<td>5.546875</td>
<td>4.128955</td>
<td>473.406250</td>
<td>0.118903</td>
<td>2.156250</td>
<td>1.156250</td>
<td>1.531250</td>
<td>0.703125</td>
</tr>
<tr>
<td>1000</td>
<td>0.027300</td>
<td>8.015625</td>
<td>2.569114</td>
<td>315.343750</td>
<td>0.681860</td>
<td>2.625000</td>
<td>1.531250</td>
<td>2.562500</td>
<td>1.296875</td>
</tr>
</tbody></table>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/unsloth/" rel="tag"># unsloth</a>
              <a href="/tags/llama-3-2/" rel="tag"># llama-3.2</a>
              <a href="/tags/grpo/" rel="tag"># grpo</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/04/11/gradio-cam%E6%90%AD%E5%BB%BAweb%E9%A1%B5%E9%9D%A2%E4%BD%93%E9%AA%8C/" rel="prev" title="gradio+cam搭建web页面体验">
                  <i class="fa fa-angle-left"></i> gradio+cam搭建web页面体验
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/04/17/Unsloth-Llama-3-2-3B-Instruct-SFT%E5%B0%86%E9%80%9A%E7%94%A8LLM%E5%BE%AE%E8%B0%83%E6%88%90%E6%8E%A8%E7%90%86LLM/" rel="next" title="Unsloth:Llama-3.2-3B-Instruct+SFT将通用LLM微调成推理LLM">
                  Unsloth:Llama-3.2-3B-Instruct+SFT将通用LLM微调成推理LLM <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">288k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">17:26</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
