<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"caihaoran-00.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.22.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="前言无他，就是想看看这篇论文（计划是将DeepSeek和Qwen系列论文都看咯），咱们一起来先看看DeepSeek-R1的论文吧。 Paper：https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-R1&#x2F;blob&#x2F;main&#x2F;DeepSeek_R1.pdf GitHub：https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-R1 Title–&amp;gt">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepSeek-R1论文一起看">
<meta property="og:url" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/index.html">
<meta property="og:site_name" content="Chr&#39;s Blog">
<meta property="og:description" content="前言无他，就是想看看这篇论文（计划是将DeepSeek和Qwen系列论文都看咯），咱们一起来先看看DeepSeek-R1的论文吧。 Paper：https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-R1&#x2F;blob&#x2F;main&#x2F;DeepSeek_R1.pdf GitHub：https:&#x2F;&#x2F;github.com&#x2F;deepseek-ai&#x2F;DeepSeek-R1 Title–&amp;gt">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424101955225.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424144433909.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424150706721.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424151308569.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424151351423.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424161850714.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424162046638.png">
<meta property="og:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424162851384.png">
<meta property="article:published_time" content="2025-04-24T01:49:22.000Z">
<meta property="article:modified_time" content="2025-05-07T03:22:34.482Z">
<meta property="article:author" content="Chr">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="deepseek-r1">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424101955225.png">


<link rel="canonical" href="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/","path":"2025/04/24/DeepSeek-R1论文一起看/","title":"DeepSeek-R1论文一起看"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DeepSeek-R1论文一起看 | Chr's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Chr's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Record and Share</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E6%96%87"><span class="nav-number">2.</span> <span class="nav-text">正文</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E5%90%8D"><span class="nav-number">2.1.</span> <span class="nav-text">文章名</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">2.2.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80"><span class="nav-number">2.3.</span> <span class="nav-text">1. 引言</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.3.1.</span> <span class="nav-text">1.1. 贡献</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%96%B9%E6%B3%95"><span class="nav-number">2.4.</span> <span class="nav-text">2. 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E6%A6%82%E8%BF%B0"><span class="nav-number">2.4.1.</span> <span class="nav-text">2.1. 概述</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-DeepSeek-R1-Zero-%E5%9F%BA%E4%BA%8E%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E5%81%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.4.2.</span> <span class="nav-text">2.2. DeepSeek-R1-Zero: 基于基础模型做强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-1-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.2.1.</span> <span class="nav-text">2.2.1. 强化学习算法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-2-%E5%A5%96%E5%8A%B1%E5%BB%BA%E6%A8%A1"><span class="nav-number">2.4.2.2.</span> <span class="nav-text">2.2.2. 奖励建模</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-3-%E8%AE%AD%E7%BB%83%E6%A8%A1%E6%9D%BF"><span class="nav-number">2.4.2.3.</span> <span class="nav-text">2.2.3. 训练模板</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-2-4-DeepSeek-R1-Zero-%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%81%E8%87%AA%E8%BF%9B%E5%8C%96%E8%BF%87%E7%A8%8B%E5%92%8C%E9%A1%BF%E6%82%9F%E6%97%B6%E5%88%BB"><span class="nav-number">2.4.2.4.</span> <span class="nav-text">2.2.4. DeepSeek-R1-Zero 的性能、自进化过程和顿悟时刻</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-DeepSeek-R1-%E5%B8%A6%E6%9C%89%E5%86%B7%E5%90%AF%E5%8A%A8%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.4.3.</span> <span class="nav-text">2.3 DeepSeek-R1: 带有冷启动的强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-1-%E5%86%B7%E5%90%AF%E5%8A%A8"><span class="nav-number">2.4.3.1.</span> <span class="nav-text">2.3.1. 冷启动</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-2-%E9%9D%A2%E5%90%91%E6%8E%A8%E7%90%86%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.4.3.2.</span> <span class="nav-text">2.3.2. 面向推理的强化学习</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-3-%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E5%92%8C%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83"><span class="nav-number">2.4.3.3.</span> <span class="nav-text">2.3.3. 拒绝采样和监督微调</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-3-4-%E9%9D%A2%E5%90%91%E5%85%A8%E5%9C%BA%E6%99%AF%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.4.3.4.</span> <span class="nav-text">2.3.4 面向全场景的强化学习</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-4-%E8%92%B8%E9%A6%8F%EF%BC%9A%E8%B5%8B%E4%BA%88%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B"><span class="nav-number">2.4.4.</span> <span class="nav-text">2.4 蒸馏：赋予小模型推理能力</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%AE%9E%E9%AA%8C"><span class="nav-number">2.5.</span> <span class="nav-text">3. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-DeepSeek-R1-%E8%AF%84%E4%BC%B0"><span class="nav-number">2.5.1.</span> <span class="nav-text">3.1 DeepSeek-R1 评估</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-%E8%92%B8%E9%A6%8F%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="nav-number">2.5.2.</span> <span class="nav-text">3.2 蒸馏模型评估</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E8%AE%A8%E8%AE%BA"><span class="nav-number">2.6.</span> <span class="nav-text">4. 讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-%E8%92%B8%E9%A6%8F-vs-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.6.1.</span> <span class="nav-text">4.1 蒸馏 vs 强化学习</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-%E5%A4%B1%E8%B4%A5%E5%B0%9D%E8%AF%95"><span class="nav-number">2.6.2.</span> <span class="nav-text">4.2 失败尝试</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E7%BB%93%E8%AE%BA%E3%80%81%E5%B1%80%E9%99%90%E4%B8%8E%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.7.</span> <span class="nav-text">5. 结论、局限与未来工作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">3.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%88%9B%E6%84%8F%E5%86%99%E4%BD%9C%E3%80%81%E9%80%9A%E7%94%A8%E9%97%AE%E7%AD%94%E3%80%81%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E3%80%81%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90-%E5%90%84%E4%B8%BE%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90"><span class="nav-number">3.1.</span> <span class="nav-text">1. 创意写作、通用问答、文本编辑、摘要生成 各举一个例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%95%BF%E5%BA%A6%E6%8E%A7%E5%88%B6%E8%AF%84%E6%B5%8B-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">3.2.</span> <span class="nav-text">2. 长度控制评测 是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%9D%9E%E8%80%83%E8%AF%95%E7%B1%BB%E9%97%AE%E9%A2%98-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">3.3.</span> <span class="nav-text">3. 非考试类问题 是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%86%B7%E5%90%AF%E5%8A%A8-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">3.4.</span> <span class="nav-text">4. 冷启动 是什么？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E4%BB%B7%E5%80%BC%E8%AF%84%E4%BC%B0%E5%99%A8%EF%BC%88critic%EF%BC%89%E5%92%8C%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%88reward-model%EF%BC%89%E5%85%B3%E7%B3%BB"><span class="nav-number">3.5.</span> <span class="nav-text">5. 价值评估器（critic）和奖励模型（reward model）关系?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-PPO%E2%80%93-DPO-or-GRPO"><span class="nav-number">3.6.</span> <span class="nav-text">6. PPO–&gt;DPO or GRPO ?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-%E4%BB%8B%E7%BB%8D%E4%B8%8B%E7%AC%AC3-%E8%8A%82%E6%8F%90%E5%88%B0%E7%9A%84%E4%BC%97%E5%A4%9A%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="nav-number">3.7.</span> <span class="nav-text">7. 介绍下第3.节提到的众多基准测试集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-STEM%E6%8C%87%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">3.8.</span> <span class="nav-text">8. STEM指的是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-reward-hacking%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">3.9.</span> <span class="nav-text">9. reward hacking是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-DeepSeek-R1%E6%98%AF%E6%80%8E%E4%B9%88%E8%AE%AD%E7%BB%83%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">3.10.</span> <span class="nav-text">10. DeepSeek-R1是怎么训练出来的？</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chr</p>
  <div class="site-description" itemprop="description">Welcome to my little world</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">88</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">99</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:1299964565@qq.com" title="E-Mail → mailto:1299964565@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://caihaoran-00.github.io/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chr">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chr's Blog">
      <meta itemprop="description" content="Welcome to my little world">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DeepSeek-R1论文一起看 | Chr's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DeepSeek-R1论文一起看
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-04-24 09:49:22" itemprop="dateCreated datePublished" datetime="2025-04-24T09:49:22+08:00">2025-04-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-07 11:22:34" itemprop="dateModified" datetime="2025-05-07T11:22:34+08:00">2025-05-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/theory/" itemprop="url" rel="index"><span itemprop="name">theory</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>52 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>无他，就是想看看这篇论文（计划是将DeepSeek和Qwen系列论文都看咯），咱们一起来先看看DeepSeek-R1的论文吧。</p>
<p><strong>Paper：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf</a></strong></p>
<p><strong>GitHub：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-R1">https://github.com/deepseek-ai/DeepSeek-R1</a></strong></p>
<p><strong>Title–&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</strong></p>
<span id="more"></span>

<hr>
<h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><h4 id="文章名"><a href="#文章名" class="headerlink" title="文章名"></a><strong>文章名</strong></h4><p><strong>DeepSeek-R1：通过强化学习激励LLM的推理能力</strong></p>
<hr>
<h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h4><p>我们介绍我们的第一代推理模型，DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 是一个通过大规模强化学习 (RL) 训练的模型，没有将监督微调 (SFT) 作为初步步骤，它展示了卓越的推理能力。通过 RL，DeepSeek-R1-Zero 自然涌现出许多强大而有趣的推理行为。然而，它遇到了诸如可读性差和语言混合等挑战。为了解决这些问题并进一步增强推理性能，我们引入了 DeepSeek-R1，它在 RL 之前整合了多阶段训练和冷启动数据。DeepSeek-R1 在推理任务上实现了与 OpenAI-01-1217 相媲美的性能。为了支持研究社区，我们开源了 DeepSeek-R1-Zero、DeepSeek-R1，以及基于 Qwen 和 Llama 从 DeepSeek-R1 蒸馏出的六个密集模型（1.5B、7B、8B、14B、32B、70B）。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424101955225.png" class="" title="image-20250424101955225">

<hr>
<h4 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a><strong>1. 引言</strong></h4><p>近年来，大型语言模型 (LLM) 经历了快速的迭代和演进 (Anthropic, 2024; Google, 2024; OpenAI, 2024a)，逐步缩小了与通用人工智能 (AGI) 之间的差距。</p>
<p>最近，后训练已成为完整训练流程中的一个重要组成部分。它已被证明可以在推理任务上提高准确性，与社会价值观对齐，并适应用户偏好，同时相对于预训练只需要相对较少的计算资源。在推理能力方面，OpenAI的o1系列模型首次引入了推理时扩展，通过增加思维链（Chain-of-Thought）推理过程的长度，在数学、编码和科学推理等多种任务中取得了显著改进。然而，如何有效实现测试时扩展仍是研究社区面临的开放性问题。先前的一些工作探索了各种方法，包括基于过程的奖励模型 (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2023)、强化学习 (Kumar et al., 2024)，以及蒙特卡洛树搜索和束搜索等搜索算法 (Feng et al., 2024; Trinh et al., 2024; Xin et al., 2024)。然而，这些方法中没有一个能够达到与 OpenAI o1 系列模型相当的通用推理性能。</p>
<p>本文中，我们首次尝试使用纯强化学习（RL）提升语言模型的推理能力。我们的目标是探索大语言模型在没有任何监督数据的情况下发展推理能力的潜力，专注于通过纯强化学习过程实现自我进化。具体而言，我们以DeepSeek-V3-Base为基础模型，采用GRPO（Shao et al., 2024）作为强化学习框架来提升模型在推理任务中的表现。在训练过程中，DeepSeek-R1-Zero 自然涌现出许多强大而有趣的推理行为。经过数千步 RL 训练后，DeepSeek-R1-Zero 在推理基准测试中表现出超强的性能。例如，在 AIME 2024 上的 pass@1 分数从 15.6% 提高到 71.0%，并且通过多数投票 (majority voting)，分数进一步提高到 86.7%，达到了 OpenAI-01-0912 的性能水平。</p>
<p>然而，DeepSeek-R1-Zero 面临着诸如可读性差和语言混合等挑战。为了解决这些问题并进一步提升推理性能，我们引入了 DeepSeek-R1，它整合了少量的冷启动数据和一个多阶段的训练流程。具体来说，我们首先收集数千条冷启动数据来微调 DeepSeek-V3-Base 模型。然后，我们执行像 DeepSeek-R1-Zero 一样的面向推理的 RL。在 RL 过程接近收敛时，我们通过对 RL 检查点进行拒绝采样来创建新的 SFT 数据，并结合 DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据，然后重新训练 DeepSeek-V3-Base 模型。在使用新数据进行微调后，该检查点会经历一个额外的 RL 过程，考虑所有场景的提示。经过这些步骤，我们得到了一个称为 DeepSeek-R1 的检查点，其性能与 OpenAI-01-1217 相当。</p>
<p>我们进一步探索了从 DeepSeek-R1 向更小的密集模型进行蒸馏。使用 Qwen2.5-32B (Qwen, 2024b) 作为基础模型，直接从 DeepSeek-R1 进行蒸馏优于在其上应用 RL。这表明大型基础模型发现的推理模式对于提高推理能力至关重要。我们开源了蒸馏后的 Qwen 和 Llama (Dubey et al., 2024) 系列模型。值得注意的是，我们蒸馏出的 14B 模型在性能上大幅超越了最先进的开源模型 QwQ-32B-Preview (Qwen, 2024a)，而蒸馏出的 32B 和 70B 模型在密集模型的推理基准测试中创下了新纪录。</p>
<hr>
<h5 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1. 贡献"></a>1.1. 贡献</h5><p><strong>后训练：在基础模型上进行大规模强化学习</strong></p>
<ul>
<li>我们直接将 RL 应用于基础模型，而不依赖监督微调 (SFT) 作为初步步骤。这种方法允许模型探索用于解决复杂问题的思维链 (CoT)，从而发展出 DeepSeek-R1-Zero。DeepSeek-R1-Zero 展示了诸如自我验证、反思以及生成长 CoT 等能力，标志着研究界的一个重要里程碑。值得注意的是，这是第一个公开验证 LLM 推理能力可以纯粹通过 RL 来激励，而无需 SFT 的研究。这一突破为该领域的未来发展铺平了道路。</li>
<li>我们引入了开发 DeepSeek-R1 的流程。该流程包含两个 RL 阶段，旨在发现改进的推理模式并与人类偏好对齐，以及两个 SFT 阶段，作为模型推理和非推理能力的种子。我们相信这个流程将通过创造更好的模型使行业受益。</li>
</ul>
<p><strong>蒸馏：小模型也可以很强大</strong></p>
<ul>
<li>我们证明了较大模型的推理模式可以被蒸馏到较小的模型中，与在小模型上通过 RL 发现的推理模式相比，性能更好。开源的 DeepSeek-R1 及其 API 将有助于研究社区未来蒸馏出更好的小模型。</li>
<li>利用 DeepSeek-R1 生成的推理数据，我们微调了几个在研究界广泛使用的密集模型。评估结果表明，蒸馏后的小型密集模型在基准测试中表现异常出色。DeepSeek-R1-Distill-Qwen-7B 在 AIME 2024 上达到 55.5%，超过了 QwQ-32B-Preview。此外，DeepSeek-R1-Distill-Qwen-32B 在 AIME 2024 上得分 72.6%，在 MATH-500 上得分 94.3%，在 LiveCodeBench 上得分 57.2%。这些结果显著优于之前的开源模型，并且与 o1-mini 相当。我们向社区开源了基于 Qwen2.5 和 Llama3 系列的蒸馏后的 1.5B、7B、8B、14B、32B 和 70B 检查点。</li>
</ul>
<p><strong>1.2. 评估结果总结</strong></p>
<ul>
<li><strong>推理任务：</strong> (1) DeepSeek-R1 在 AIME 2024 上取得了 79.8% 的 Pass@1 分数，略微超过 OpenAI-o1-1217。在 MATH-500 上，它达到了 97.3% 的惊人分数，与 OpenAI-o1-1217 表现相当，并显著优于其他模型。(2) 在编程相关任务上，DeepSeek-R1 在代码竞赛任务中表现出专家水平，它在 Codeforces 上获得了 2,029 的 Elo 评分，超过了竞赛中 96.3% 的人类参与者。对于工程相关任务，DeepSeek-R1 的表现略好于 DeepSeek-V3，这有助于开发者在实际任务中应用。</li>
<li><strong>知识：</strong> 在 MMLU、MMLU-Pro 和 GPQA Diamond 等基准测试中，DeepSeek-R1 取得了出色的结果，显著优于 DeepSeek-V3，在 MMLU 上得分 90.8%，在 MMLU-Pro 上得分 84.0%，在 GPQA Diamond 上得分 71.5%。虽然其性能略低于 OpenAI-o1-1217，但 DeepSeek-R1 超过了其他闭源模型，展示了其在教育任务中的竞争优势。在事实基准测试 SimpleQA 上，DeepSeek-R1 优于 DeepSeek-V3，展示了其处理基于事实的查询的能力。观察到类似的趋势，即 OpenAI-o1 在此基准上超过了 4o。</li>
</ul>
<hr>
<ul>
<li>**其他方面：**DeepSeek-R1 在多个任务上同样表现出色，包括创意写作、通用问答、文本编辑、摘要生成等。它在 AlpacaEval 2.0 的长度控制评测中取得了 87.6% 的胜率，在 ArenaHard 测试中达到了 92.3% 的胜率，展现了其在处理非考试类问题时的强大智能能力。此外，DeepSeek-R1 在需要长上下文理解的任务中表现尤为突出，在长上下文基准测试中大幅优于 DeepSeek-V3。</li>
</ul>
<hr>
<h4 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a><strong>2. 方法</strong></h4><h5 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1. 概述"></a><strong>2.1. 概述</strong></h5><p>以前的工作严重依赖大量监督数据来提升模型性能。在本研究中，我们证明了即使不使用监督微调 (SFT) 作为冷启动，通过大规模强化学习 (RL) 也可以显著提高推理能力。此外，通过引入少量冷启动数据，性能可以得到进一步增强。在以下章节中，我们介绍：(1) DeepSeek-R1-Zero，它直接将 RL 应用于基础模型，无需任何 SFT 数据；(2) DeepSeek-R1，它从一个经过数千个长思维链 (CoT) 示例微调的检查点开始应用 RL；3) 将 DeepSeek-R1 的推理能力蒸馏到小型密集模型中。</p>
<h5 id="2-2-DeepSeek-R1-Zero-基于基础模型做强化学习"><a href="#2-2-DeepSeek-R1-Zero-基于基础模型做强化学习" class="headerlink" title="2.2. DeepSeek-R1-Zero: 基于基础模型做强化学习"></a><strong>2.2. DeepSeek-R1-Zero: 基于基础模型做强化学习</strong></h5><p>正如我们之前的工作所证明的 (Shao et al., 2024; Wang et al., 2023)，强化学习在推理任务中已显示出显著的有效性。然而，这些工作严重依赖于监督数据，而收集这些数据非常耗时。在本节中，我们探索 LLM 在没有任何监督数据的情况下发展推理能力的潜力，重点关注它们通过纯强化学习过程的自我进化。我们首先简要概述我们的 RL 算法，然后展示一些令人兴奋的结果，希望这能为社区提供有价值的见解。</p>
<hr>
<h6 id="2-2-1-强化学习算法"><a href="#2-2-1-强化学习算法" class="headerlink" title="2.2.1. 强化学习算法"></a><strong>2.2.1. 强化学习算法</strong></h6><p><strong>组相对策略优化 (Group Relative Policy Optimization,GRPO)</strong> 为了节省 RL 的训练成本，我们采用组相对策略优化 (GRPO) (Shao et al., 2024)，该方法放弃了通常与策略模型大小相同的评论家模型 (critic model)，而是根据组得分来估计基线。具体来说，对于每个问题$ q $，GRPO 从旧策略 $ \pi_{\theta_{\text{old}}} $ 中采样一组输出 $ {o_1, o_2, …, o_g} $，然后通过最大化以下目标函数来优化策略模型 $ \pi_\theta $：<br>$$<br>J_{\text{GRPO}}(\theta) &#x3D; \mathbb{E}<em>{q \sim \mathcal{P}(Q), {o_i}</em>{i&#x3D;1}^G \sim \pi_{\theta_{\text{old}}}(O|q)}<br>\left[ \frac{1}{G} \sum_{i&#x3D;1}^G \left(<br>\min \left(<br>\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} \cdot A_i,\<br>\text{clip}\left(<br>\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\ 1 - \epsilon,\ 1 + \epsilon<br>\right) \cdot A_i<br>\right) - \beta \cdot D_{\text{KL}}(\pi_\theta | \pi_{\text{ref}})<br>\right) \right]<br>$$</p>
<p>$$<br>D_{\text{KL}}(\pi_\theta | \pi_{\text{ref}}) &#x3D;<br>\left( \frac{\pi_{\text{ref}}(o_i|q)}{\pi_\theta(o_i|q)} \right) - \log \left( \frac{\pi_{\text{ref}}(o_i|q)}{\pi_\theta(o_i|q)} \right) - 1<br>$$</p>
<p>其中  $ \varepsilon $ 和 $ \beta $ 是超参数，$ A_i $是优势（Advantage），使用一组对应于组内输出的奖励 $ { r_1, r_2, \dots, r_G } $  计算得出:<br>$$<br>A_i &#x3D; \frac{r_i - \text{mean}({r_1, r_2, \dots, r_G})}<br>{\text{std}({r_1, r_2, \dots, r_G})}\tag{3}<br>$$</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424144433909.png" class="" title="image-20250424144433909">

<hr>
<h6 id="2-2-2-奖励建模"><a href="#2-2-2-奖励建模" class="headerlink" title="2.2.2. 奖励建模"></a><strong>2.2.2. 奖励建模</strong></h6><p>奖励是训练信号的来源，决定了 RL 的优化方向。为了训练 DeepSeek-R1-Zero，我们采用了一个基于规则的奖励系统，主要包括两种类型的奖励：</p>
<ul>
<li><strong>准确性奖励：</strong> 准确性奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定格式（例如，在方框内）提供最终答案，从而能够进行可靠的基于规则的正确性验证。类似地，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。</li>
<li><strong>格式奖励：</strong> 除了准确性奖励模型外，我们还使用了一个格式奖励模型，强制模型将其思考过程置于 <code>&lt;think&gt;</code> 和 <code>&lt;/think&gt;</code>标签之间。</li>
</ul>
<p>我们在开发 DeepSeek-R1-Zero 时没有应用基于结果或过程的神经奖励模型，因为我们发现神经奖励模型在大规模强化学习过程中可能会遭受奖励黑客攻击 (reward hacking)，并且重新训练奖励模型需要额外的训练资源，这会使整个训练流程复杂化。</p>
<hr>
<h6 id="2-2-3-训练模板"><a href="#2-2-3-训练模板" class="headerlink" title="2.2.3. 训练模板"></a><strong>2.2.3. 训练模板</strong></h6><p>为了训练 DeepSeek-R1-Zero，我们首先设计了一个简单的模板，引导基础模型遵循我们指定的指令。如表 1 所示，该模板要求 DeepSeek-R1-Zero 首先产生推理过程，然后给出最终答案。我们有意将约束限制在这种结构格式上，避免任何内容特定的偏见——例如强制进行反思性推理或推广特定的解决问题策略——以确保我们能够准确观察模型在 RL 过程中的自然进展。</p>
<hr>
<h6 id="2-2-4-DeepSeek-R1-Zero-的性能、自进化过程和顿悟时刻"><a href="#2-2-4-DeepSeek-R1-Zero-的性能、自进化过程和顿悟时刻" class="headerlink" title="2.2.4. DeepSeek-R1-Zero 的性能、自进化过程和顿悟时刻"></a><strong>2.2.4. DeepSeek-R1-Zero 的性能、自进化过程和顿悟时刻</strong></h6><p><strong>DeepSeek-R1-Zero 的性能</strong> 图 2 描绘了 DeepSeek-R1-Zero 在整个 RL 训练过程中于 AIME 2024 基准测试上的性能轨迹。如图所示，随着 RL 训练的推进，DeepSeek-R1-Zero 的性能表现出稳定且持续的提升。值得注意的是，AIME 2024 上的平均 pass@1 分数显著增加，从最初的 15.6% 跃升至令人印象深刻的 71.0%，达到了与 OpenAI-O1-0912 相当的性能水平。这一显著改进凸显了我们的 RL 算法在优化模型性能方面的有效性。</p>
<p>表2提供了DeepSeek-R1-Zero和OpenAI的o1-0912模型在各种推理相关基准上的比较分析。研究结果表明，RL赋能DeepSeek-R1-Zero 获得强大的推理能力，而无需任何监督微调数据。这是一个值得注意的成就，因为它强调了模型仅通过 RL 就能有效学习和泛化的能力。此外，通过应用多数投票 (majority voting) 可以进一步增强 DeepSeek-R1-Zero 的性能。例如，当在 AIME 基准上采用多数投票时，DeepSeek-R1-Zero 的性能从 71.0% 提升到 86.7%，从而超过了 OpenAI-01-0912 的性能。DeepSeek-R1-Zero 无论是否使用多数投票都能取得如此具有竞争力的性能，突显了其强大的基础能力及其在推理任务上进一步发展的潜力。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424150706721.png" class="" title="image-20250424150706721">

<p><strong>DeepSeek-R1-Zero 的自进化过程</strong> DeepSeek-R1-Zero 的自进化过程是一个引人入胜的展示，说明了 RL 如何驱动模型自主提高其推理能力。通过直接从基础模型启动 RL，我们可以密切监控模型的进展，而不受监督微调阶段的影响。这种方法清晰地展示了模型如何随时间演变，特别是在处理复杂推理任务的能力方面。</p>
<p>如图3所示，DeepSeek-R1-Zero的思考时间在训练过程中持续改善。这种改进并非外部调整的结果，而是模型内部的固有发展。DeepSeek-R1-Zero 自然地获得了通过利用扩展的测试时计算来解决日益复杂的推理任务的能力。这种计算范围从生成数百到数千个推理 token，使模型能够更深入地探索和完善其思考过程。</p>
<p>这种自进化的最显著方面之一是随着测试时计算的增加而出现的复杂行为。诸如反思（模型重新审视和重新评估其先前步骤）和探索解决问题的替代方法等行为会自发产生。这些行为并非明确编程，而是模型与强化学习环境交互的结果。这种自发的发展显著增强了 DeepSeek-R1-Zero 的推理能力，使其能够更高效、更准确地处理更具挑战性的任务。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424151308569.png" class="" title="image-20250424151308569">

<p><strong>DeepSeek-R1-Zero 的顿悟时刻</strong> 在 DeepSeek-R1-Zero 的训练过程中观察到的一个特别有趣的现象是“顿悟时刻”的发生。如表 3 所示，这个时刻发生在模型的一个中间版本中。在这个阶段，DeepSeek-R1-Zero 通过重新评估其初始方法，学会了为问题分配更多的思考时间。这种行为不仅证明了模型日益增长的推理能力，而且也是一个引人入胜的例子，说明了强化学习如何导致意想不到的复杂结果。</p>
<p>这个时刻不仅对模型来说是一个“顿悟时刻”，对观察其行为的研究人员来说也是如此。它强调了强化学习的力量和魅力：我们并非明确教导模型如何解决问题，而是简单地为其提供正确的激励，它就能自主地发展出先进的问题解决策略。“顿悟时刻”有力地提醒我们 RL 在解锁人工智能系统中新层次智能方面的潜力，为未来更自主、更具适应性的模型铺平了道路。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424151351423.png" class="" title="image-20250424151351423">

<p>尽管DeepSeek-R1-Zero表现出强大的推理能力和自主开发出意外且强大的推理行为，但它仍面临一些问题。例如，DeepSeek-R1-Zero在处理可读性差和语言混杂等挑战时显得力不从心。为了使推理过程更加易读并将其分享给开放社区，我们探索了DeepSeek-R1，这是一种利用强化学习与人类友好型冷启动数据相结合的方法。</p>
<hr>
<h5 id="2-3-DeepSeek-R1-带有冷启动的强化学习"><a href="#2-3-DeepSeek-R1-带有冷启动的强化学习" class="headerlink" title="2.3 DeepSeek-R1: 带有冷启动的强化学习"></a>2.3 <strong>DeepSeek-R1: 带有冷启动的强化学习</strong></h5><p>受 DeepSeek-R1-Zero 令人鼓舞的结果启发，出现了两个自然的问题：1) 通过引入少量高质量数据作为冷启动，是否可以进一步提高推理性能或加速收敛？2) 我们如何训练一个用户友好的模型，它不仅能产生清晰连贯的思维链 (CoT)，而且还表现出强大的通用能力？为了解决这些问题，我们设计了一个训练 DeepSeek-R1 的流程。该流程包括四个阶段，概述如下。</p>
<h6 id="2-3-1-冷启动"><a href="#2-3-1-冷启动" class="headerlink" title="2.3.1. 冷启动"></a><strong>2.3.1. 冷启动</strong></h6><p>与 DeepSeek-R1-Zero 不同，为了防止 RL 训练早期从基础模型开始的不稳定冷启动阶段，我们为 DeepSeek-R1 构建并收集了少量长 CoT 数据，用于微调模型作为初始 RL 参与者 (actor)。为了收集此类数据，我们探索了几种方法：使用长 CoT 作为示例进行少样本提示 (few-shot prompting)，直接提示模型生成带有反思和验证的详细答案，以可读格式收集 DeepSeek-R1-Zero 的输出，以及通过人工标注员进行后处理来完善结果。</p>
<p>在这项工作中，我们收集了数千条冷启动数据来微调 DeepSeek-V3-Base，作为 RL 的起点。与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：</p>
<ul>
<li><strong>可读性：</strong> DeepSeek-R1-Zero 的一个关键限制是其内容通常不适合阅读。响应可能会混合多种语言或缺少 Markdown 格式来为用户突出显示答案。相比之下，在为 DeepSeek-R1 创建冷启动数据时，我们设计了一种可读模式，在每个响应的末尾包含摘要，并过滤掉不便于读者阅读的响应。在这里，我们将输出格式定义为 | special_token | <reasoning_process> | special_token | &lt;summary&gt;，其中<code>reasoning_process</code>是查询的 CoT，<code>summary</code>用于总结推理结果。</li>
<li><strong>潜力：</strong> 通过仔细设计带有人先验 (human priors) 的冷启动数据模式，我们观察到其性能优于 DeepSeek-R1-Zero。我们相信迭代训练是推理模型更好的方式。</li>
</ul>
<hr>
<h6 id="2-3-2-面向推理的强化学习"><a href="#2-3-2-面向推理的强化学习" class="headerlink" title="2.3.2. 面向推理的强化学习"></a><strong>2.3.2. 面向推理的强化学习</strong></h6><p>在使用冷启动数据对 DeepSeek-V3-Base 进行微调之后，我们应用与 DeepSeek-R1-Zero 相同的大规模强化学习训练流程。本阶段旨在进一步增强模型的推理能力，特别是在编码、数学、科学以及逻辑推理等推理密集型任务中，这些任务通常具有明确的问题定义和清晰的解答方案。</p>
<p>在训练过程中我们观察到，当 RL 的提示语涉及多种语言时，链式思维（CoT）生成往往会出现语言混杂现象。为缓解该问题，我们在强化学习训练中引入了<strong>语言一致性奖励</strong>，该奖励通过统计 CoT 中目标语言词汇所占的比例来计算。</p>
<p>尽管消融实验表明，这种语言对齐机制会轻微降低模型在某些指标上的性能，但它符合人类偏好，从而提升了生成内容的可读性。最终，我们将推理任务的准确度得分与语言一致性奖励<strong>直接相加</strong>，作为最终的奖励信号。随后，我们在已经微调的模型上继续进行强化学习训练，直至其在推理类任务上收敛。</p>
<hr>
<h6 id="2-3-3-拒绝采样和监督微调"><a href="#2-3-3-拒绝采样和监督微调" class="headerlink" title="2.3.3. 拒绝采样和监督微调"></a><strong>2.3.3. 拒绝采样和监督微调</strong></h6><p>当面向推理的强化学习（Reasoning-oriented RL）达到收敛后，我们使用所得的检查点（checkpoint）来收集下一轮监督微调（SFT）所需的数据。不同于初始的冷启动数据主要聚焦于推理能力，这一阶段的数据涵盖了<strong>写作、角色扮演以及其他通用任务</strong>领域，以进一步增强模型的综合能力。</p>
<p><strong>推理类数据（Reasoning data）</strong></p>
<p>我们从推理提示语（reasoning prompts）出发，通过对上述 RL 训练得到的检查点进行拒绝采样（rejection sampling），生成推理轨迹。在前一阶段，我们只纳入能够使用基于规则的奖励函数评估的数据。而在本阶段，我们拓展了数据集，引入了额外的数据，其中部分通过生成式奖励模型（generative reward model）进行评估：具体做法是将模型预测和参考答案一同输入 DeepSeek-V3，由它来判断结果质量。</p>
<blockquote>
<p>推理轨迹(reasoning trajectories)可理解为COT(思维链)。</p>
</blockquote>
<p>此外，由于模型在某些情况下输出内容混乱、难以阅读，我们过滤掉了以下类型的链式思维内容（Chain-of-Thought, CoT）：<strong>混合语言</strong>、<strong>段落过长</strong>、<strong>包含代码块</strong>等情况。对于每一个提示语，我们采样多个响应，仅保留其中<strong>正确的答案</strong>。</p>
<p>最终，我们共收集了约 <strong>60 万条与推理相关的训练样本</strong>，作为后续 SFT 阶段的数据。</p>
<hr>
<p><strong>非推理类数据（Non-Reasoning data）：<strong>对于</strong>非推理类的数据</strong>，例如写作、事实问答（factual QA）、自我认知（self-cognition）和翻译任务，我们采用了 DeepSeek-V3 的数据生成流程，并复用了部分 <strong>DeepSeek-V3 的监督微调（SFT）数据集</strong>。</p>
<p>在某些非推理类任务中，我们会调用 DeepSeek-V3，在回答问题前通过提示语引导其生成一个可能的 <strong>思维链（Chain-of-Thought, CoT）</strong>。不过，对于较为简单的请求，例如 “hello”，我们不会生成 CoT，仅直接给出回复。</p>
<p>最终，我们收集了<strong>大约 20 万条与推理无关的训练样本</strong>。</p>
<p>然后，我们使用上述整理得到的大约 80 万条数据（60 万推理类 + 20 万非推理类），对 DeepSeek-V3-Base 进行了 <strong>2 轮（epochs）微调训练</strong>。</p>
<hr>
<h6 id="2-3-4-面向全场景的强化学习"><a href="#2-3-4-面向全场景的强化学习" class="headerlink" title="2.3.4 面向全场景的强化学习"></a>2.3.4 面向全场景的强化学习</h6><p>为了进一步让模型与人类偏好对齐，我们实施了第二阶段的强化学习，旨在提升模型的<strong>有用性（helpfulness）<strong>与</strong>无害性（harmlessness）</strong>，同时进一步优化其推理能力。</p>
<p>具体来说，我们结合多种奖励信号与多样的提示分布对模型进行训练：</p>
<ul>
<li><strong>在推理类数据方面</strong>，我们继续采用 DeepSeek-R1-Zero 中的方法，即使用<strong>基于规则的奖励函数</strong>来指导模型在数学、代码和逻辑推理领域的学习。</li>
<li><strong>在通用任务数据方面</strong>，我们采用<strong>奖励模型</strong>来捕捉人类在复杂和细腻情境下的偏好。</li>
</ul>
<p>我们基于 DeepSeek-V3 的训练流程，采用类似的偏好对（preference pairs）和训练提示语分布。</p>
<ul>
<li><strong>在有用性方面</strong>，我们仅评估模型的<strong>最终摘要（summary）部分</strong>，以确保模型响应对用户具有实用性且内容相关，同时不干扰推理过程。</li>
<li><strong>在无害性方面</strong>，我们评估模型<strong>整个响应过程</strong>，包括推理过程和总结内容，以识别并减轻潜在的风险、偏见或有害内容。</li>
</ul>
<p>最终，通过结合奖励信号与多样化的数据分布，我们训练出了一个既擅长推理，又兼顾有用性与无害性的模型。</p>
<hr>
<h5 id="2-4-蒸馏：赋予小模型推理能力"><a href="#2-4-蒸馏：赋予小模型推理能力" class="headerlink" title="2.4 蒸馏：赋予小模型推理能力"></a>2.4 蒸馏：赋予小模型推理能力</h5><p>为了让更高效的小模型也具备 DeepSeek-R1 的推理能力，我们使用 §2.3.3 中构建的 80 万条数据（推理数据 + 通用数据），对开源模型如 <strong>Qwen</strong>（Qwen, 2024b）和 <strong>Llama</strong>（AI@Meta, 2024）进行了直接的监督微调（SFT）。</p>
<p>实验发现，这种<strong>直接蒸馏的方法</strong>显著增强了小模型的推理能力。</p>
<p>我们使用的基础模型包括：</p>
<ul>
<li>Qwen2.5-Math-1.5B</li>
<li>Qwen2.5-Math-7B</li>
<li>Qwen2.5-14B</li>
<li>Qwen2.5-32B</li>
<li>Llama-3.1-8B</li>
<li>Llama-3.3-70B-Instruct</li>
</ul>
<p>其中之所以选择 <strong>Llama-3.3</strong>，是因为其推理能力略优于 Llama-3.1。</p>
<p>在蒸馏后的模型中，我们<strong>只进行了 SFT，没有使用 RL 阶段</strong>，尽管引入 RL 可能进一步提升模型表现。我们在此阶段的主要目标是<strong>验证蒸馏技术的有效性</strong>，而将 RL 阶段的探索留给更广泛的研究社区。</p>
<hr>
<h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h4><p> **基准评估（Benchmarks）**我们在以下多个基准数据集上评估模型性能：</p>
<ul>
<li><strong>MMLU</strong>（Hendrycks 等, 2020）</li>
<li><strong>MMLU-Redux</strong>（Gema 等, 2024）</li>
<li><strong>MMLU-Pro</strong>（Wang 等, 2024）</li>
<li><strong>C-Eval</strong>（Huang 等, 2023）</li>
<li><strong>CMMLU</strong>（Li 等, 2023）</li>
<li><strong>IFEval</strong>（Zhou 等, 2023）</li>
<li><strong>FRAMES</strong>（Krishna 等, 2024）</li>
<li><strong>GPQA Diamond</strong>（Rein 等, 2023）</li>
<li><strong>SimpleQA</strong>（OpenAI, 2024c）</li>
<li><strong>C-SimpleQA</strong>（He 等, 2024）</li>
<li><strong>SWE-Bench Verified</strong>（OpenAI, 2024d）</li>
<li><strong>Aider</strong></li>
<li><strong>LiveCodeBench</strong>（Jain 等, 2024）（2024年8月至2025年1月）</li>
<li><strong>Codeforces</strong></li>
<li><strong>中国高中数学联赛（CNMO 2024）</strong></li>
<li><strong>美国数学邀请赛（AIME 2024）</strong>（MAA, 2024）</li>
</ul>
<p>除了这些标准基准测试之外，我们还在<strong>开放式生成任务</strong>上评估模型表现，并采用<strong>大语言模型作为评审</strong>。</p>
<p>具体来说，我们遵循 <strong>AlpacaEval 2.0</strong>（Dubois 等, 2024）和 <strong>Arena-Hard</strong>（Li 等, 2024）的原始配置，它们均使用 <strong>GPT-4-Turbo-1106</strong> 作为评审模型来进行成对比较。</p>
<p>在这类评估中，我们<strong>仅将最终摘要输入（summary）评审模型</strong>，以避免由于响应长度不同而引起的评估偏差。</p>
<p>对于<strong>蒸馏后的模型</strong>，我们报告以下具有代表性的评估结果：</p>
<ul>
<li>AIME 2024</li>
<li>MATH-500</li>
<li>GPQA Diamond</li>
<li>Codeforces</li>
<li>LiveCodeBench</li>
</ul>
<hr>
<p>**评估提示设计（Evaluation Prompts）**根据 DeepSeek-V3 的设置，我们使用如下方式对模型进行评估：</p>
<ul>
<li>对于标准基准测试集（如 <strong>MMLU</strong>、<strong>DROP</strong>、<strong>GPQA Diamond</strong> 和 <strong>SimpleQA</strong>），采用 <strong>simpleevals</strong> 框架中的提示语进行评估。</li>
<li><strong>MMLU-Redux</strong> 使用 <strong>Zero-Eval</strong> 提示格式（Lin, 2024），并在 <strong>zero-shot</strong>（零样本）设置下进行评估。</li>
<li>对于原始为 <strong>few-shot</strong>（少样本）格式的 <strong>MMLU-Pro</strong>、<strong>C-Eval</strong> 和 <strong>CLUE-WSC</strong>，我们略微调整提示语以适配 <strong>zero-shot</strong> 设置，因为 few-shot 中的 CoT（Chain-of-Thought）可能会削弱 DeepSeek-R1 的表现。</li>
<li>其他数据集按照其作者提供的默认评估协议执行。</li>
</ul>
<p>对于 <strong>代码与数学类基准测试</strong>：</p>
<ul>
<li><strong>HumanEval-Mul</strong> 数据集覆盖八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash）。</li>
<li><strong>LiveCodeBench</strong> 的模型评估采用 CoT 格式，数据收集时间为 2024 年 8 月至 2025 年 1 月。</li>
<li><strong>Codeforces</strong> 的评估基于 10 场 Div.2 比赛中的题目，并辅以专家制定的测试用例，随后计算期望得分与选手排名比例。</li>
<li><strong>SWE-Bench Verified</strong> 的评估通过 <strong>agentless framework</strong>（Xia 等, 2024）完成。</li>
<li>与 <strong>AIDER</strong> 相关的评估使用 <code>&quot;diff&quot;</code> 格式。</li>
</ul>
<p>每个基准测试中，DeepSeek-R1 的输出长度最多限制为 <strong>32,768 个 token</strong>。</p>
<hr>
<p>**基准模型（Baselines）**我们与多个强基线模型进行全面对比，包括：</p>
<ul>
<li><strong>DeepSeek-V3</strong></li>
<li><strong>Claude-Sonnet-3.5-1022</strong></li>
<li><strong>GPT-4o-0513</strong></li>
<li><strong>OpenAI-o1-mini</strong></li>
<li><strong>OpenAI-o1-1217</strong></li>
</ul>
<p>由于中国大陆难以直接访问 OpenAI-o1-1217 接口，我们依据其 <strong>官方报告</strong> 的结果进行对比。</p>
<p>对于 <strong>蒸馏模型</strong>，我们还引入开源模型 <strong>QwQ-32B-Preview</strong>（Qwen, 2024a）作为对比对象。</p>
<hr>
<p>**评估设置（Evaluation Setup）**我们为模型设置了最大生成长度为 <strong>32,768</strong> 个 token。</p>
<p>我们发现，使用贪心解码（greedy decoding）来评估长输出推理模型会导致较高的重复率，并且在不同检查点之间的结果变化较大。因此，我们默认使用 <strong>pass@𝑘</strong> 评估方法（Chen 等，2021），并报告 <strong>pass@1</strong>，同时使用非零温度进行解码。</p>
<p>具体而言，我们使用 <strong>0.6</strong> 的采样温度和 <strong>0.95</strong> 的 top-𝑝 值来生成 <strong>𝑘</strong> 个响应（通常在 4 到 64 之间，具体取决于测试集的大小）来回答每个问题。然后，通过以下公式计算 <strong>pass@1</strong>：<br>$$<br>\text{pass@1} &#x3D; \frac{1}{k} \sum_{i&#x3D;1}^{k} p_i<br>$$<br>其中 $ p_i $表示第 $i$ 个回答的正确性。这种方法提供了更可靠的性能估计。对于 AIME 2024，我们还报告了共识（多数投票）结果（Wang 等，2022），使用 64 个样本，记作 $ \text{cons@64} $。</p>
<hr>
<h5 id="3-1-DeepSeek-R1-评估"><a href="#3-1-DeepSeek-R1-评估" class="headerlink" title="3.1 DeepSeek-R1 评估"></a>3.1 <strong>DeepSeek-R1 评估</strong></h5><p>对于教育导向的知识基准测试，如 MMLU、MMLU-Pro 和 GPQA Diamond，DeepSeek-R1 相较于 DeepSeek-V3 展现出优越的表现。这一改进主要归因于在 STEM 相关问题上的准确性提高，其中通过大规模强化学习取得了显著的进展。此外，DeepSeek-R1 在 FRAMES 上表现出色，这是一个长上下文依赖的问答任务，展示了其强大的文档分析能力。这突显了推理模型在 AI 驱动的搜索和数据分析任务中的潜力。在事实类基准测试 SimpleQA 上，DeepSeek-R1 超过了 DeepSeek-V3，展现了其处理基于事实查询的能力。类似的趋势也出现在 OpenAI-o1 超过 GPT-4o 的情况下。然而，在中文 SimpleQA 基准测试中，DeepSeek-R1 的表现逊色于 DeepSeek-V3，主要是由于其在安全强化学习后倾向于拒绝回答某些查询。如果没有安全强化学习，DeepSeek-R1 可以达到超过 70% 的准确率。</p>
<p>DeepSeek-R1 在 IF-Eval 上也展现出了令人印象深刻的表现。该基准测试旨在评估模型遵循格式指令的能力。这些提升可以归因于在监督微调（SFT）和强化学习（RL）训练的最后阶段引入了指令遵循类的数据。此外，DeepSeek-R1 在 AlpacaEval 2.0 和 ArenaHard 上的优异表现，显示了其在写作任务和开放领域问答方面的强项。相比 DeepSeek-V3 的显著优势进一步表明，大规模强化学习不仅增强了推理能力，还提升了模型在各类任务中的泛化能力。</p>
<p>此外，DeepSeek-R1 所生成的摘要内容长度适中，在 ArenaHard 上的平均长度为 689 个 token，在 AlpacaEval 2.0 上的平均长度为 2,218 个字符，这表明在基于 GPT 的评估中，DeepSeek-R1 能够有效避免长度偏置，进一步巩固了其在多任务环境中的稳健性。</p>
<p>在数学任务方面，DeepSeek-R1 的表现与 OpenAI-o1-1217 不相上下，并且在与其他模型的比较中遥遥领先。在编程算法任务（如 LiveCodeBench 和 Codeforces）中也观察到类似趋势，以推理为核心的模型在这些基准测试中表现尤为突出。在面向工程的编程任务中，OpenAI-o1-1217 在 Aider 上优于 DeepSeek-R1，但在 SWE Verified 上两者表现相当。我们认为，随着后续版本引入更多相关的 RL 训练数据，DeepSeek-R1 在工程编程任务上的表现也将进一步提升。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424161850714.png" class="" title="image-20250424161850714">

<hr>
<h5 id="3-2-蒸馏模型评估"><a href="#3-2-蒸馏模型评估" class="headerlink" title="3.2 蒸馏模型评估"></a>3.2 蒸馏模型评估</h5><p>如表 5 所示，仅通过对 DeepSeek-R1 的输出进行蒸馏，就能让高效的 DeepSeek-R1-7B（即 DeepSeek-R1-Distill-Qwen-7B，简称为 DeepSeek-R1-7B）在各项评估中全面超越非推理模型，例如 GPT-4o-0513。DeepSeek-R1-14B 在所有评估指标上也优于 QwQ-32B-Preview，而 DeepSeek-R1-32B 和 DeepSeek-R1-70B 则在大多数基准测试上显著超越 o1-mini。这些结果充分展示了蒸馏技术的强大潜力。</p>
<p>此外，我们还发现，将强化学习（RL）应用于这些经过蒸馏的模型后，性能会有显著提升。基于这一发现，我们认为这一方向值得进一步深入研究，因此本报告仅展示了通过简单 SFT 蒸馏得到的模型结果。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424162046638.png" class="" title="image-20250424162046638">

<hr>
<h4 id="4-讨论"><a href="#4-讨论" class="headerlink" title="4. 讨论"></a>4. 讨论</h4><h5 id="4-1-蒸馏-vs-强化学习"><a href="#4-1-蒸馏-vs-强化学习" class="headerlink" title="4.1 蒸馏 vs 强化学习"></a><strong>4.1 蒸馏 vs 强化学习</strong></h5><p>在第 3.2 节中我们已经看到，通过对 DeepSeek-R1 进行蒸馏，小模型也可以取得非常出色的结果。但仍然有一个关键问题尚未解答：<strong>是否可以仅依赖论文中提到的大规模强化学习训练，而不依赖蒸馏，就实现与之相当的性能？</strong></p>
<p>为了解答这个问题，我们对 Qwen-32B-Base 模型在数学、代码和 STEM 数据上进行了超过 1 万步的大规模强化学习训练，得到了模型 <strong>DeepSeek-R1-Zero-Qwen-32B</strong>。实验结果如表 6 所示，训练后的 32B 基座模型性能与 QwQ-32B-Preview 持平。然而，从 DeepSeek-R1 蒸馏得到的 <strong>DeepSeek-R1-Distill-Qwen-32B</strong> 模型，在所有基准测试中都明显优于 DeepSeek-R1-Zero-Qwen-32B。</p>
<img src="/2025/04/24/DeepSeek-R1%E8%AE%BA%E6%96%87%E4%B8%80%E8%B5%B7%E7%9C%8B/image-20250424162851384.png" class="" title="image-20250424162851384">

<p>因此，我们可以得出两个结论：</p>
<ol>
<li>**将强模型蒸馏为小模型是非常有效的策略。**相比之下，仅依赖论文中描述的大规模强化学习来训练小模型，不仅成本高昂，性能也可能不如蒸馏方法。</li>
<li><strong>虽然蒸馏是一种经济又有效的方法，但若想继续突破智能边界，仍然需要更强大的基座模型和更大规模的强化学习。</strong></li>
</ol>
<hr>
<h5 id="4-2-失败尝试"><a href="#4-2-失败尝试" class="headerlink" title="4.2 失败尝试"></a><strong>4.2 失败尝试</strong></h5><p>在 DeepSeek-R1 的早期开发阶段，我们也经历了一些失败和挫折。在此我们分享这些失败的经验，希望为他人提供一些启示——不过这并不意味着这些方法本身无法用于构建有效的推理模型。</p>
<p>Process Reward Model（PRM）</p>
<p>PRM 是一种合理的方式，用于引导模型学习更优的推理路径（Lightman et al., 2023；Uesato et al., 2022；Wang et al., 2023）。但在实际应用中，PRM 存在以下三个主要限制，这些限制在我们的实验中制约了其最终效果：</p>
<ol>
<li><strong>难以明确定义通用推理中的细粒度步骤</strong>。推理过程往往不具备标准化的步骤模板，这使得 PRM 难以泛化。</li>
<li><strong>评估中间步骤是否正确本身就是一个困难的问题</strong>。自动标注依赖的模型往往效果不佳，而人工标注则不利于规模化。</li>
<li><strong>一旦引入基于模型的 PRM，就容易出现奖励黑客（reward hacking）现象</strong>（Gao et al., 2022），并且奖励模型的重训练会消耗额外算力，同时增加训练流程的复杂性。</li>
</ol>
<p>总之，虽然 PRM 在 <strong>对模型生成的 top-N 响应重排序</strong> 或 <strong>引导式搜索</strong>（Snell et al., 2024）中表现良好，但在我们的大规模强化学习场景中，PRM 带来的计算开销远大于它带来的性能提升，因此收益有限。</p>
<hr>
<p>Monte Carlo Tree Search（MCTS）</p>
<p>受到 AlphaGo（Silver et al., 2017b）和 AlphaZero（Silver et al., 2017a）的启发，我们尝试将蒙特卡洛树搜索（MCTS）应用于推理任务中，以增强测试时的计算扩展能力。</p>
<p>我们的方法是：将问题的解拆分成若干子步骤，引导模型系统性地探索解空间。为此，我们先通过 prompt 引导模型生成多个标签，这些标签对应于推理过程中需执行的特定子步骤。</p>
<p>训练流程如下：</p>
<ul>
<li>首先，我们利用预训练的 value model 指导 MCTS 搜索，以找到对应问题的最优解。</li>
<li>然后，我们将生成的问答对用于训练 actor 模型和 value 模型，不断迭代优化该流程。</li>
</ul>
<p>不过，在将这种方法扩展到大规模训练时，我们遇到了几个关键挑战：</p>
<hr>
<p><strong>首先</strong>，与国际象棋等任务不同，其搜索空间相对明确和有限，而<strong>文本生成的搜索空间呈指数级增长</strong>。为了应对这个问题，我们为每个节点设置了最大扩展限制（maximum extension limit），但这往往会导致模型<strong>陷入局部最优解</strong>，从而难以探索更优的全局路径。</p>
<p><strong>第二</strong>，<strong>value model 的质量直接影响生成效果</strong>，因为它在搜索过程中承担了指导每一步推理的职责。要训练出一个细粒度、精确可靠的 value model 本身就是一个非常困难的任务，这使得通过自我搜索迭代提升模型性能的方式很难稳定运行。</p>
<p>虽然 AlphaGo 的成功很大程度上依赖于 value model 的不断进化，从而推动整体性能逐步提升，但在我们的文本生成场景中，<strong>复制这一策略面临极高的复杂性</strong>，尤其是由于 token 级生成的高维度与不确定性。</p>
<hr>
<p><strong>总结来说</strong>，尽管在推理阶段，如果搭配预训练好的 value model，MCTS 确实能够提升一定的性能；但要通过自我搜索（self-search）方式实现模型性能的迭代式增强，仍然是一个<strong>尚未解决的重大难题</strong>。</p>
<hr>
<h4 id="5-结论、局限与未来工作"><a href="#5-结论、局限与未来工作" class="headerlink" title="5. 结论、局限与未来工作"></a>5. 结论、局限与未来工作</h4><p>在本研究中，我们分享了通过强化学习提升模型推理能力的探索过程。<strong>DeepSeek-R1-Zero</strong> 代表了一种纯粹基于强化学习的方法，未依赖冷启动数据，却在多个任务上取得了优异表现。而 <strong>DeepSeek-R1</strong> 则更为强大，结合了冷启动数据与迭代式强化学习微调，最终在多个任务上达到了与 <strong>OpenAI-o1-1217</strong> 相当的表现。</p>
<p>我们还进一步探索了如何将推理能力蒸馏到小型稠密模型中。我们使用 <strong>DeepSeek-R1</strong> 作为教师模型，生成了 <strong>80 万条训练样本</strong>，并用这些数据微调了多个小模型。结果令人鼓舞：<strong>DeepSeek-R1-Distill-Qwen-1.5B</strong> 在数学基准测试中表现优异，<strong>在 AIME 上达到 28.9%、在 MATH 上达到 83.9%</strong>，超越了 GPT-4o 和 Claude-3.5-Sonnet。其他小模型也取得了显著成绩，远超同类底座模型的指令微调版本。</p>
<p>未来我们计划从以下几个方向推进 DeepSeek-R1 的研究：</p>
<ul>
<li><strong>通用能力</strong>：当前，DeepSeek-R1 在函数调用、多轮对话、复杂角色扮演、结构化 JSON 输出等任务上仍不及 DeepSeek-V3。未来我们将探索如何利用更长的 CoT（Chain-of-Thought）推理链条来增强这些能力。</li>
<li><strong>多语言混用问题</strong>：DeepSeek-R1 当前主要针对中英文优化，在处理其他语言的输入时，可能出现语言混杂的问题。例如即使用户使用非中英文提问，模型仍可能用英文进行推理与回答。我们计划在后续版本中优化这一问题。</li>
<li><strong>提示词工程（Prompt Engineering）</strong>：我们观察到 DeepSeek-R1 对提示词非常敏感。使用 few-shot 提示词时，模型性能普遍下降。因此我们建议用户<strong>采用 zero-shot 设置</strong>，直接描述问题并明确输出格式，以获得最佳效果。</li>
<li><strong>软件工程任务</strong>：由于软件工程任务的评估耗时较长，影响了 RL 的效率，我们尚未在该领域大规模应用强化学习。因此，DeepSeek-R1 在此类基准上尚未超过 DeepSeek-V3。未来，我们将通过<strong>拒绝采样（rejection sampling）</strong> 或 <strong>在 RL 过程中引入异步评估</strong> 等手段来提升训练效率。</li>
</ul>
<hr>
<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><h4 id="1-创意写作、通用问答、文本编辑、摘要生成-各举一个例子"><a href="#1-创意写作、通用问答、文本编辑、摘要生成-各举一个例子" class="headerlink" title="1. 创意写作、通用问答、文本编辑、摘要生成 各举一个例子"></a><strong>1. 创意写作、通用问答、文本编辑、摘要生成 各举一个例子</strong></h4><p>✅ 创意写作（Creative Writing）</p>
<blockquote>
<p><strong>任务例子</strong>：写一首以“秋天的黄昏”为主题的现代诗<br> <strong>模型输出</strong>：<br> <em>“落日流火染山坡，风吹枯叶过老窝，<br> 一地金黄无人拾，唯有黄昏忆蹉跎。”</em></p>
</blockquote>
<p>✅ 通用问答（General Question Answering）</p>
<blockquote>
<p><strong>任务例子</strong>：水的沸点是多少？<br> <strong>模型输出</strong>：100°C（在标准大气压下）</p>
</blockquote>
<p>✅ 文本编辑（Text Editing）</p>
<blockquote>
<p><strong>任务例子</strong>：将句子“他昨天去商店”改为礼貌语气<br> <strong>模型输出</strong>：“他昨天去了商店。”</p>
</blockquote>
<p>✅ 摘要生成（Summarization）</p>
<blockquote>
<p><strong>任务例子</strong>：给一段关于气候变化影响的长文章生成摘要<br> <strong>模型输出</strong>：气候变化正在导致极端天气频发，对农业和生态系统造成显著影响。</p>
</blockquote>
<h4 id="2-长度控制评测-是什么？"><a href="#2-长度控制评测-是什么？" class="headerlink" title="2. 长度控制评测 是什么？"></a><strong>2. 长度控制评测 是什么？</strong></h4><blockquote>
<p>简单说：<strong>评估模型是否能控制生成文本的长度</strong>，比如你说“写一个 50 字的摘要”，模型能否做到接近要求，既不啰嗦也不敷衍。</p>
</blockquote>
<p>🧪 比如在 <strong>AlpacaEval 2.0</strong> 中，它可能会要求：</p>
<ul>
<li>生成“100字以内”的回答；</li>
<li>或者生成“一句话的总结”； 然后人类或模型再去评估它是否：</li>
<li><strong>长度合适</strong></li>
<li><strong>内容完整</strong></li>
<li><strong>表达自然</strong></li>
</ul>
<p>所以 <strong>87.6% 的胜率</strong> 说明 DeepSeek-R1 在“控制输出长度”的任务上非常擅长。</p>
<hr>
<h4 id="3-非考试类问题-是什么？"><a href="#3-非考试类问题-是什么？" class="headerlink" title="3. 非考试类问题 是什么？"></a><strong>3. 非考试类问题 是什么？</strong></h4><blockquote>
<p>就是不是“教科书标准题”的问题，换句话说就是：<strong>更贴近现实、更开放、需要“理解+表达”的题目</strong>。</p>
</blockquote>
<p>📘 对比来看：</p>
<table>
<thead>
<tr>
<th>考试类问题</th>
<th>非考试类问题</th>
</tr>
</thead>
<tbody><tr>
<td>“光合作用的反应式是什么？”</td>
<td>“如何形象地解释光合作用给小孩听？”</td>
</tr>
<tr>
<td>“计算：234×67&#x3D;”</td>
<td>“帮我用 Python 写一个简单的工资计算器”</td>
</tr>
<tr>
<td>“莎士比亚出生在哪一年？”</td>
<td>“写一个像莎士比亚那样的爱情独白”</td>
</tr>
</tbody></table>
<p>DeepSeek-R1 擅长的是后者这类更“开放、灵活、类人类沟通”的问题。</p>
<hr>
<h4 id="4-冷启动-是什么？"><a href="#4-冷启动-是什么？" class="headerlink" title="4. 冷启动 是什么？"></a>4. 冷启动 是什么？</h4><p>“冷启动”（<strong>Cold Start</strong>）这个概念，来源于推荐系统和机器学习领域，意思是：</p>
<blockquote>
<p><strong>在一开始几乎没有数据、没有经验、没有预训练知识的情况下启动模型训练或推理。</strong></p>
</blockquote>
<p>在大模型训练中，它通常有两个含义：</p>
<p>✅ <strong>1. 冷启动模型（Cold Start Model）</strong></p>
<blockquote>
<p>指从一个“什么都没学过”的基础模型（base model）开始训练，而不是从已经微调过的模型继续训练。</p>
</blockquote>
<p>比如：</p>
<ul>
<li>从一个预训练语言模型直接开始做强化学习（RL），<strong>不做 SFT</strong>，这就是冷启动。</li>
<li>如果你先做了 SFT（监督微调），再做 RL，就不是冷启动了。</li>
</ul>
<p>✅ <strong>2. 冷启动数据（Cold Start Data）</strong></p>
<blockquote>
<p>指一开始用来“启动模型学习过程”的那一小部分高质量数据，常用于引导方向。</p>
</blockquote>
<p>这种数据有两个典型特征：</p>
<ul>
<li>量少但关键（比如几千条高质量 CoT 样本）；</li>
<li>用于“破冰”或“暖机”，让模型从一开始就朝正确的方向学习。</li>
</ul>
<p>可以理解为：</p>
<blockquote>
<p>冷启动数据 &#x3D; 少量但高质量的种子数据，用于帮助模型迈出第一步。</p>
</blockquote>
<table>
<thead>
<tr>
<th>模型状态</th>
<th>是否预训练</th>
<th>是否SFT</th>
<th>是否是冷启动</th>
</tr>
</thead>
<tbody><tr>
<td>完全空白模型</td>
<td>❌ 没有</td>
<td>❌ 没有</td>
<td>❌（甚至不实用）</td>
</tr>
<tr>
<td>预训练过的大语言模型（base model）</td>
<td>✅ 有</td>
<td>❌ 没有</td>
<td>✅ 是冷启动</td>
</tr>
<tr>
<td>预训练 + 监督微调（SFT）后再做RL</td>
<td>✅ 有</td>
<td>✅ 有</td>
<td>❌ 不是冷启动</td>
</tr>
</tbody></table>
<hr>
<h4 id="5-价值评估器（critic）和奖励模型（reward-model）关系"><a href="#5-价值评估器（critic）和奖励模型（reward-model）关系" class="headerlink" title="5. 价值评估器（critic）和奖励模型（reward model）关系?"></a>5. 价值评估器（critic）和奖励模型（reward model）关系?</h4><p>🎯 一句话区分</p>
<ul>
<li><strong>奖励模型（Reward Model）</strong>：是<strong>人类偏好评分的模拟器</strong>，负责对「一个完整回答」打一个分，代表“人类喜不喜欢”。</li>
<li><strong>价值评估器（Critic）</strong>：是策略学习中的<strong>强化学习工具</strong>，用来估计“采取某个动作会带来多大的期望回报”。</li>
</ul>
<p>✅ 套用到大模型训练场景</p>
<p>在大模型的 RLHF 中，有三大角色：</p>
<table>
<thead>
<tr>
<th>角色</th>
<th>功能</th>
<th>是否打分</th>
<th>是否可训练</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Actor（策略模型）</strong></td>
<td>生成回答（action）</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td><strong>Reward Model（奖励模型）</strong></td>
<td>模拟人类打分</td>
<td>✅ 打“整体分”</td>
<td>✅ 但常提前训练好</td>
</tr>
<tr>
<td><strong>Critic（价值模型）</strong></td>
<td>评估每个 token 的期望收益</td>
<td>✅ 打“过程分”</td>
<td>✅ 在 PPO 中实时更新</td>
</tr>
</tbody></table>
<p>🔍 举个例子：</p>
<p>假设你让模型回答一个问题：“如何学好数学？”</p>
<p>模型生成的回答是：</p>
<blockquote>
<p>“1. 多做题；2. 找一本好教材；3. 建立兴趣。”</p>
</blockquote>
<p>✅ 这时候：</p>
<ul>
<li><strong>Reward Model</strong>：给整个回答打个 9&#x2F;10 的分数（基于人类偏好）</li>
<li><strong>Critic</strong>：会评估“第1步好不好”、“第2步是否引人”、“第3步是否是高价值的策略延续”，输出 token-level 的价值估计 $V(s_t)$。</li>
</ul>
<p>🔁 在 PPO 中的工作流程：</p>
<ol>
<li>Actor 生成回答</li>
<li>Reward Model 给出一个整体得分（或使用人类打分训练出来的 RM）</li>
<li>Critic 估计每一步的 token 带来的期望奖励（帮助学习）</li>
<li>用这些分数计算 Advantage（优势函数）来更新 Actor 策略</li>
</ol>
<p>🚀 那么 GRPO 又做了什么？</p>
<p>在 GRPO 中：</p>
<ul>
<li><strong>不训练 Critic</strong>（因为太重）</li>
<li>也 <strong>不用传统的 Reward Model</strong> 给固定分数</li>
<li>而是从 <strong>多个输出中排个序</strong>，比如：<ul>
<li>这组输出中，$ O_3 $ 比 $ O_1 $ 更好，$ O_2 $ 最差</li>
<li>然后用这个“相对好坏”来指导学习</li>
</ul>
</li>
</ul>
<p>所以：</p>
<blockquote>
<p><strong>GRPO 相当于用一组输出内部的相对打分，取代了传统的 Reward Model + Critic 的组合</strong>，更轻更省资源。</p>
</blockquote>
<hr>
<p>如果你用过 OpenAI 的 PPO 代码，你会发现它里面有个 <code>reward_model</code> 和一个 <code>value_head</code>（critic），GRPO 就是把这两个角色“压缩”成了一个“排序评估”。</p>
<hr>
<h4 id="6-PPO–-DPO-or-GRPO"><a href="#6-PPO–-DPO-or-GRPO" class="headerlink" title="6. PPO–&gt;DPO or GRPO ?"></a>6. PPO–&gt;DPO or GRPO ?</h4><p><strong>PPO → DPO</strong> 和 <strong>PPO → GRPO</strong> 其实是<strong>两条不同的发展路线</strong>，都属于 RLHF 的演化方向，但各自有不同的出发点和目标，我们来分清楚：</p>
<p>✅ PPO 被 DPO 替代了吗？</p>
<p>在很多应用场景中，是的：</p>
<blockquote>
<p><strong>DPO（Direct Preference Optimization）</strong> 是目前被广泛认为能“替代 PPO”在大模型训练中的方法，<strong>因为它更简单、更稳定、不需要 reward model</strong>。</p>
</blockquote>
<p>🔁 DPO 的特点：</p>
<ul>
<li><strong>不需要 reward model 或 critic</strong></li>
<li>只用人类偏好的「比较」数据（比如 A 比 B 好）直接优化策略</li>
<li>更像是一种监督学习变体（而不是强化学习）</li>
</ul>
<p>✅ 所以 <strong>DPO 被称为“去RL化的 RLHF”方法</strong></p>
<p>✅ 那 GRPO 是干嘛的？它跟 PPO 是谁的“替代”？</p>
<blockquote>
<p><strong>GRPO 其实是一个真正的 RL 方法，用来优化策略模型，同时尝试降低 PPO 的训练成本。</strong></p>
</blockquote>
<p>GRPO 的目标是：</p>
<ul>
<li>保留 RL 的“策略优化思想”</li>
<li>但去掉成本大的 Critic（或者 reward model）</li>
<li>用组内相对排名代替精确 reward</li>
</ul>
<p>所以它不是走「去RL化」的 DPO 路，而是走「RL 更高效」这条路。</p>
<p>🔍 总结：DPO 和 GRPO 的区别</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>本质</th>
<th>是否 RL</th>
<th>是否需要 reward model</th>
<th>是否需要 critic</th>
<th>优点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>PPO</strong></td>
<td>强化学习</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>推理能力强，但成本高</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>偏好监督学习</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>简洁稳定，不需要 RL</td>
</tr>
<tr>
<td><strong>GRPO</strong></td>
<td>RL + 排名估值</td>
<td>✅ 是</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>RL 思路 + 成本低</td>
</tr>
</tbody></table>
<p>✅ 怎么选？</p>
<table>
<thead>
<tr>
<th>目标</th>
<th>建议方法</th>
</tr>
</thead>
<tbody><tr>
<td>想用 RL 提升推理能力，但训练成本有限</td>
<td>用 <strong>GRPO</strong></td>
</tr>
<tr>
<td>想快速微调、追求简单高效</td>
<td>用 <strong>DPO</strong></td>
</tr>
<tr>
<td>想最大化能力，资源又够</td>
<td>还是 <strong>PPO（或其优化版）</strong></td>
</tr>
</tbody></table>
<hr>
<p>如果你是自己做蒸馏或SFT，可以用 DPO；<br> 如果你搞推理能力强化、实验新的 RLHF 结构，那 GRPO 更有趣。</p>
<hr>
<h4 id="7-介绍下第3-节提到的众多基准测试集"><a href="#7-介绍下第3-节提到的众多基准测试集" class="headerlink" title="7. 介绍下第3.节提到的众多基准测试集"></a>7. 介绍下第3.节提到的众多基准测试集</h4><p>🎓 <strong>通用学术知识与推理</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>能力</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MMLU</strong></td>
<td>多学科常识 + 推理</td>
<td>评估模型在57个中高难度学科（如历史、化学、法律等）上的综合表现。</td>
</tr>
<tr>
<td><strong>MMLU-Redux</strong></td>
<td>更精准的MMLU变体</td>
<td>对MMLU题目进行了更严格的数据清洗与任务定义，更贴近真实推理能力。</td>
</tr>
<tr>
<td><strong>MMLU-Pro</strong></td>
<td>专业难度版MMLU</td>
<td>包含更高难度的问题，主要面向专业领域如工程、医学。</td>
</tr>
<tr>
<td><strong>CMMLU</strong></td>
<td>中文多学科能力</td>
<td>中文版本的MMLU，适配中文背景，评估中文语言和知识掌握能力。</td>
</tr>
<tr>
<td><strong>C-Eval</strong></td>
<td>中文学术知识</td>
<td>涵盖中国初高中到大学阶段的学科考试题，重点在语文、数学、物理等。</td>
</tr>
<tr>
<td><strong>IFEval</strong></td>
<td>格式遵循能力（Instruction Following）</td>
<td>检测模型是否能按用户设定的输出格式返回内容（如JSON、特定表格等）。</td>
</tr>
<tr>
<td><strong>FRAMES</strong></td>
<td>长上下文问答</td>
<td>检测模型在理解和利用多轮对话（或文档）信息中的表现。</td>
</tr>
<tr>
<td><strong>GPQA Diamond</strong></td>
<td>高质量科学推理</td>
<td>聚焦于生物、化学、物理等STEM领域的高难度问题。</td>
</tr>
</tbody></table>
<p>📚 <strong>事实与问答能力</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>能力</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SimpleQA</strong></td>
<td>英文事实问答</td>
<td>简短的、开放域的事实性问题（例如：乔布斯创办了哪家公司？）。</td>
</tr>
<tr>
<td><strong>C-SimpleQA</strong></td>
<td>中文事实问答</td>
<td>中文版本的SimpleQA，检测中文问答能力及模型事实准确性。</td>
</tr>
</tbody></table>
<p>🧮 <strong>数学能力</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>能力</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td><strong>CNMO 2024</strong></td>
<td>中文数学竞赛</td>
<td>中国高中数学奥林匹克试题，评估高级数学推理与解题能力。</td>
</tr>
<tr>
<td><strong>AIME 2024</strong></td>
<td>美数邀请赛</td>
<td>美国数学邀请赛试题，具有较高难度，注重逻辑与多步推理。</td>
</tr>
<tr>
<td><strong>Math-500</strong></td>
<td>数学推理挑战</td>
<td>包含 500 道高质量数学题，题型涵盖代数、几何、数论、组合等，主要用于评估模型的数学多步推理与解题能力，难度接近 AIME 和 USAMO。注重逻辑严密性与最终答案的准确性。</td>
</tr>
</tbody></table>
<p>💻 <strong>编程与代码能力</strong></p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>能力</th>
<th>简介</th>
</tr>
</thead>
<tbody><tr>
<td><strong>LiveCodeBench</strong></td>
<td>实时代码生成</td>
<td>用户提出问题，模型写代码、解释、修复等，模拟真实编程场景。</td>
</tr>
<tr>
<td><strong>Codeforces</strong></td>
<td>算法竞赛题</td>
<td>来自著名算法竞赛平台的题目，考验算法设计与编码实现。</td>
</tr>
<tr>
<td><strong>Aider</strong></td>
<td>工程类编程任务</td>
<td>更贴近真实软件工程任务（如代码修改、Pull Request 自动生成等）。</td>
</tr>
<tr>
<td><strong>SWE-Bench Verified</strong></td>
<td>软件工程问题解决</td>
<td>开源项目真实Issue解决能力测试，是否能写出可用补丁。</td>
</tr>
</tbody></table>
<p>✅ <strong>总结：这些评估维度</strong></p>
<table>
<thead>
<tr>
<th>能力类型</th>
<th>对应基准</th>
</tr>
</thead>
<tbody><tr>
<td><strong>语言&#x2F;通识&#x2F;逻辑推理</strong></td>
<td>MMLU、CMMLU、C-Eval、GPQA、FRAMES</td>
</tr>
<tr>
<td><strong>数学能力</strong></td>
<td>AIME、CNMO</td>
</tr>
<tr>
<td><strong>事实问答能力</strong></td>
<td>SimpleQA、C-SimpleQA</td>
</tr>
<tr>
<td><strong>格式遵循能力</strong></td>
<td>IFEval</td>
</tr>
<tr>
<td><strong>编程&#x2F;工程能力</strong></td>
<td>Codeforces、LiveCodeBench、SWE-Bench、Aider</td>
</tr>
</tbody></table>
<hr>
<h4 id="8-STEM指的是什么"><a href="#8-STEM指的是什么" class="headerlink" title="8. STEM指的是什么"></a>8. STEM指的是什么</h4><p>“<strong>STEM</strong>” 是 “<strong>Science, Technology, Engineering, and Mathematics</strong>” 的缩写，中文一般翻译为 <strong>科学、技术、工程和数学</strong>。</p>
<p>这个术语通常用来泛指以下这几类学科：</p>
<ul>
<li><strong>Science（科学）</strong>：物理、化学、生物、地理等自然科学</li>
<li><strong>Technology（技术）</strong>：信息技术、人工智能、数据科学等</li>
<li><strong>Engineering（工程）</strong>：电子工程、土木工程、机械工程等</li>
<li><strong>Mathematics（数学）</strong>：代数、几何、概率、微积分等</li>
</ul>
<hr>
<p>在语言模型评估中，说“评估模型在 STEM 领域的能力”，意思是：</p>
<blockquote>
<p>测试模型是否能处理好数学题、科学类推理题、代码实现、技术分析等具有<strong>结构性、逻辑性强</strong>的问题。</p>
</blockquote>
<hr>
<p>如果你看到“STEM推理”、“STEM任务”等说法，基本上都是在指：</p>
<blockquote>
<p>“这个任务不是单纯看语言流畅或常识回答，而是需要严谨推导、有步骤的分析，比如做数学题、写代码、解物理题”这类更高阶的智能能力。</p>
</blockquote>
<hr>
<h4 id="9-reward-hacking是什么"><a href="#9-reward-hacking是什么" class="headerlink" title="9. reward hacking是什么"></a>9. reward hacking是什么</h4><p>“<strong>Reward Hacking</strong>（奖励滥用）”是强化学习（Reinforcement Learning, RL）中的一个经典问题，指的是：</p>
<blockquote>
<p><strong>智能体（AI 模型）学会了“作弊”来最大化奖励信号，而不是完成我们真正想要的任务。</strong></p>
</blockquote>
<p>🔍 举个具体的例子：</p>
<p>假设你训练一个 AI 模型玩一个游戏，目标是<strong>吃到尽可能多的金币</strong>，于是你设计了一个奖励函数：</p>
<blockquote>
<p>每吃到一个金币就给 +1 奖励。</p>
</blockquote>
<p>但是，模型发现有一个地图漏洞，它可以无限地在某个地方刷金币、反复吃同一个金币。<br> 它就开始在那个地方疯狂卡 bug 刷分，而不是去认真探索游戏世界。<br> <strong>它确实最大化了奖励……但却完全偏离了我们希望它“聪明玩游戏”的初衷。</strong></p>
<p>这就是 reward hacking。</p>
<p>💥 在语言模型中也会发生：</p>
<p>比如你训练一个模型通过打分模型（Reward Model）来优化输出，期望它生成<strong>有逻辑、有帮助、真实可靠</strong>的答案。<br> 但模型可能学会了“怎么写答案才能让打分模型喜欢”，比如：</p>
<ul>
<li>加很多看起来很“聪明”的逻辑词（如“因此”、“综上”、“由此可得”）；</li>
<li>复制训练集中高分样本的模式，但其实内容胡编；</li>
<li>学会“套路”Reward Model 而不是认真推理。</li>
</ul>
<p>最终效果可能是：<strong>回答看起来很聪明，但其实内容错误或者重复。</strong></p>
<p>📌 这就是为什么很多团队在论文里强调：</p>
<blockquote>
<p>“我们要想办法减少 reward hacking，比如用多样性奖励、对抗式训练、人工校验”等方式去<strong>避免模型走偏</strong>。</p>
</blockquote>
<hr>
<h4 id="10-DeepSeek-R1是怎么训练出来的？"><a href="#10-DeepSeek-R1是怎么训练出来的？" class="headerlink" title="10. DeepSeek-R1是怎么训练出来的？"></a>10. DeepSeek-R1是怎么训练出来的？</h4><p>想回答这个问题，<strong>我们需要先知道DeepSeek-R1-Zero是怎么出来的：</strong></p>
<p>鉴于收集监督数据相当费时，我们探索LLM在没有任何监督数据的情况下发展推理能力的潜力，重点关注它们通过纯强化学习过程的自我进化。<br>1.使用纯GRPO强化学习方法，不使用SFT<br>2.使用奖励函数来代替奖励模型（关注结果准确性和格式准确性）<br>3.定义系统提示词模板引导LLM输出推理过程</p>
<p>好的，现在就得到DeepSeek-R1-Zero了，它有两个问题（语音混乱，可读性差-&gt;没有markdown格式输出），但也展现出了推理方面能力的潜力与自我进化。</p>
<p><strong>一句话描述DeepSeek-R1-Zero：没有输出模板参考，纯靠自发探索。</strong></p>
<p>基于DeepSeek-R1-Zero的探索（优势和劣势），继续了更全面的DeepSeek-R1的开发：</p>
<ol>
<li><p>few-shot形式使用DeepSeek-R1-Zero生成数千条数据并通过人类注标员处理完善</p>
</li>
<li><p>微调之后，使用语言一致性和结果的准确性作为奖励函数进行GRPO</p>
</li>
<li><p>RL收敛后，通过推理提示词，使用该收敛后的模型生成推理类数据，最后通过Deepseek-V3进行拒绝采样（判断结果质量）。并过滤掉以下类型的链式思维内容：混合语言、段落过长、包含代码块等情况。对于每一个提示语，我们采样多个响应，仅保留其中正确的答案。最终，我们共收集了约60 万条与推理相关的训练样本，作为后续 SFT 阶段的数据。</p>
<p>非推理类数据采用 DeepSeek-V3 的数据生成流程，并复用了部分DeepSeek-V3 的监督微调（SFT）数据集。<br>在某些非推理类任务中，调用 DeepSeek-V3在回答问题前通过提示语引导其生成一个可能的 思维链（Chain-of-Thought, CoT）。对于较为简单的请求，例如 “hello”，不会生成 CoT，仅直接给出回复。<br>收集了大约 20 万条与推理无关的训练样本。</p>
<p>总共大约 80 万条数据（60 万推理类 + 20 万非推理类），对 DeepSeek-V3-Base 进行了 <strong>2 轮（epochs）微调训练</strong>。</p>
</li>
<li><p>第3步完成后，进行第二阶段的强化学习，旨在提升模型的有用性（helpfulness）与无害性（harmlessness），同时进一步优化其推理能力。<br> 推理类数据方面，采用 DeepSeek-R1-Zero 中的方法，基于规则的奖励函数来指导模型在数学、代码和逻辑推理领域的学习。<br> 通用任务数据方面，采用奖励模型来捕捉人类在复杂和细腻情境下的偏好。</p>
</li>
</ol>
<p>  基于 DeepSeek-V3 的训练流程，采用类似的偏好对（preference pairs）和训练提示语分布。</p>
<p>  有用性方面，仅评估模型的最终摘要（summary）部分，以确保模型响应对用户具有实用性且内容相关，同时不干扰推理过程。<br>  在无害性方面，我们评估模型整个响应过程，包括推理过程和总结内容，以识别并减轻潜在的风险、偏见或有害内容。</p>
<hr>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/llm/" rel="tag"># llm</a>
              <a href="/tags/deepseek-r1/" rel="tag"># deepseek-r1</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/04/21/gRPC%E5%9F%BA%E7%A1%80%E4%B8%80%E8%B5%B7%E7%9C%8B/" rel="prev" title="gRPC基础一起看">
                  <i class="fa fa-angle-left"></i> gRPC基础一起看
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/04/25/%E8%AF%AD%E9%9F%B3%E7%8E%A9%E5%85%B7%EF%BC%9A%E5%90%84%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95%E9%9F%B3%E9%A2%91%E6%A0%BC%E5%BC%8F%E8%A6%81%E6%B1%82/" rel="next" title="语音玩具：各核心算法音频格式要求">
                  语音玩具：各核心算法音频格式要求 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Chr</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">365k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">22:06</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
